{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Models Using Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages (from imblearn) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.17 in /Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in /Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.13.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION:\n",
    "\n",
    "def ridgereg_cv(df, target, method):\n",
    "    # Logistic regression with ridge 'l2' penalty. Hyperparameter tuning.\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # training and testing sets\n",
    "    X_sample, X_test, y_sample, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42)\n",
    "    \n",
    "    if method == 'ros':\n",
    "        X_train, y_train = RandomOverSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'ADASYN':\n",
    "        X_train, y_train = ADASYN(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'SMOTE':\n",
    "        X_train, y_train = SMOTE(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'rus':\n",
    "        X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'cc':\n",
    "        X_train, y_train = ClusterCentroids(random_state=0).fit_resample(X_sample, y_sample)   \n",
    "    if method == 'NearMiss':\n",
    "        X_train, y_train = NearMiss(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    \n",
    "    param_grid = {'solver': ['lbfgs', 'sag', 'saga'],\n",
    "                'C': [int(x) for x in np.logspace(0, 1, num = 10)]}\n",
    "    \n",
    "    # Instantiate model and grid search\n",
    "    lgr = LogisticRegression(penalty='l2', random_state=30)\n",
    "    gm_cv = RandomizedSearchCV(lgr, param_grid, n_iter = 15, cv = 3, random_state=30)\n",
    "    gm_cv.fit(X_train, y_train)\n",
    "\n",
    "    # Scores for training and testing\n",
    "    y_predict_train = gm_cv.predict(X_train)\n",
    "    print(\"Train accuracy score:\", accuracy_score(y_predict_train, y_train))\n",
    "\n",
    "    y_predict_test = gm_cv.predict(X_test)\n",
    "    print(\"Test accuracy score\",accuracy_score(y_predict_test, y_test))\n",
    "\n",
    "    # Classification reports\n",
    "    print(\"\\n Training Classification Report:\")\n",
    "    print(classification_report(y_train, y_predict_train))\n",
    "\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "    # Best Model\n",
    "    print(gm_cv.best_estimator_)\n",
    "    \n",
    "def lassoreg_cv(df, target, method):\n",
    "    # Logistic regression with lasso 'l1' penalty. Hyperparameter tuning\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    # training and testing sets\n",
    "    X_sample, X_test, y_sample, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42)\n",
    "    \n",
    "    if method == 'ros':\n",
    "        X_train, y_train = RandomOverSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'ADASYN':\n",
    "        X_train, y_train = ADASYN(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'SMOTE':\n",
    "        X_train, y_train = SMOTE(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'rus':\n",
    "        X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'cc':\n",
    "        X_train, y_train = ClusterCentroids(random_state=0).fit_resample(X_sample, y_sample)   \n",
    "    if method == 'NearMiss':\n",
    "        X_train, y_train = NearMiss(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    \n",
    "    param_grid = {'solver': ['liblinear', 'saga'],\n",
    "              'C': [int(x) for x in np.logspace(0, 1, num = 10)]} \n",
    "    \n",
    "    # Instantiate model and grid search\n",
    "    lgr = LogisticRegression(penalty='l1', class_weight = \"balanced\", random_state=30)\n",
    "    gm_cv = RandomizedSearchCV(lgr, param_grid, n_iter = 15, cv = 3, random_state=30)\n",
    "    gm_cv.fit(X_train, y_train)\n",
    "\n",
    "    # Scores for training and testing\n",
    "    y_predict_train = gm_cv.predict(X_train)\n",
    "    print(\"Train accuracy score:\", round(accuracy_score(y_predict_train, y_train), 3))\n",
    "\n",
    "    y_predict_test = gm_cv.predict(X_test)\n",
    "    print(\"Test accuracy score\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "\n",
    "    # Classification reports\n",
    "    print(\"\\n Training Classification Report:\")\n",
    "    print(classification_report(y_train, y_predict_train))\n",
    "\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "    # Best estimator\n",
    "    print(gm_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_data = pd.read_csv('/Users/Julia/Documents/bootcamp/fraud_capstone/data_out/train_final_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChronicCond_Alzheimer</th>\n",
       "      <th>ChronicCond_Cancer</th>\n",
       "      <th>ChronicCond_Depression</th>\n",
       "      <th>ChronicCond_Diabetes</th>\n",
       "      <th>ChronicCond_Heartfailure</th>\n",
       "      <th>ChronicCond_IschemicHeart</th>\n",
       "      <th>ChronicCond_KidneyDisease</th>\n",
       "      <th>ChronicCond_ObstrPulmonary</th>\n",
       "      <th>ChronicCond_Osteoporasis</th>\n",
       "      <th>ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>ChronicCond_stroke</th>\n",
       "      <th>County_0</th>\n",
       "      <th>County_1</th>\n",
       "      <th>County_10</th>\n",
       "      <th>County_100</th>\n",
       "      <th>County_11</th>\n",
       "      <th>County_110</th>\n",
       "      <th>County_111</th>\n",
       "      <th>County_113</th>\n",
       "      <th>County_117</th>\n",
       "      <th>County_120</th>\n",
       "      <th>County_130</th>\n",
       "      <th>County_131</th>\n",
       "      <th>County_14</th>\n",
       "      <th>County_140</th>\n",
       "      <th>County_141</th>\n",
       "      <th>County_150</th>\n",
       "      <th>County_160</th>\n",
       "      <th>County_161</th>\n",
       "      <th>County_170</th>\n",
       "      <th>County_180</th>\n",
       "      <th>County_190</th>\n",
       "      <th>County_191</th>\n",
       "      <th>County_194</th>\n",
       "      <th>County_20</th>\n",
       "      <th>County_200</th>\n",
       "      <th>County_210</th>\n",
       "      <th>County_211</th>\n",
       "      <th>County_212</th>\n",
       "      <th>County_213</th>\n",
       "      <th>County_220</th>\n",
       "      <th>County_221</th>\n",
       "      <th>County_222</th>\n",
       "      <th>County_223</th>\n",
       "      <th>County_224</th>\n",
       "      <th>County_230</th>\n",
       "      <th>County_240</th>\n",
       "      <th>County_241</th>\n",
       "      <th>County_25</th>\n",
       "      <th>County_250</th>\n",
       "      <th>...</th>\n",
       "      <th>proc_9764.0</th>\n",
       "      <th>proc_9784.0</th>\n",
       "      <th>proc_9787.0</th>\n",
       "      <th>proc_9789.0</th>\n",
       "      <th>proc_9805.0</th>\n",
       "      <th>proc_9815.0</th>\n",
       "      <th>proc_9851.0</th>\n",
       "      <th>proc_9903.0</th>\n",
       "      <th>proc_9904.0</th>\n",
       "      <th>proc_9905.0</th>\n",
       "      <th>proc_9906.0</th>\n",
       "      <th>proc_9907.0</th>\n",
       "      <th>proc_9910.0</th>\n",
       "      <th>proc_9914.0</th>\n",
       "      <th>proc_9915.0</th>\n",
       "      <th>proc_9916.0</th>\n",
       "      <th>proc_9917.0</th>\n",
       "      <th>proc_9918.0</th>\n",
       "      <th>proc_9919.0</th>\n",
       "      <th>proc_9920.0</th>\n",
       "      <th>proc_9921.0</th>\n",
       "      <th>proc_9922.0</th>\n",
       "      <th>proc_9923.0</th>\n",
       "      <th>proc_9925.0</th>\n",
       "      <th>proc_9926.0</th>\n",
       "      <th>proc_9928.0</th>\n",
       "      <th>proc_9929.0</th>\n",
       "      <th>proc_9938.0</th>\n",
       "      <th>proc_9939.0</th>\n",
       "      <th>proc_9952.0</th>\n",
       "      <th>proc_9955.0</th>\n",
       "      <th>proc_9959.0</th>\n",
       "      <th>proc_9960.0</th>\n",
       "      <th>proc_9961.0</th>\n",
       "      <th>proc_9962.0</th>\n",
       "      <th>proc_9969.0</th>\n",
       "      <th>proc_9971.0</th>\n",
       "      <th>proc_9972.0</th>\n",
       "      <th>proc_9973.0</th>\n",
       "      <th>proc_9974.0</th>\n",
       "      <th>proc_9975.0</th>\n",
       "      <th>proc_9978.0</th>\n",
       "      <th>proc_9979.0</th>\n",
       "      <th>proc_9982.0</th>\n",
       "      <th>proc_9984.0</th>\n",
       "      <th>proc_9986.0</th>\n",
       "      <th>proc_9992.0</th>\n",
       "      <th>proc_9995.0</th>\n",
       "      <th>proc_9998.0</th>\n",
       "      <th>proc_9999.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.365759</td>\n",
       "      <td>0.233463</td>\n",
       "      <td>0.451362</td>\n",
       "      <td>0.754864</td>\n",
       "      <td>0.564202</td>\n",
       "      <td>0.762646</td>\n",
       "      <td>0.474708</td>\n",
       "      <td>0.400778</td>\n",
       "      <td>0.272374</td>\n",
       "      <td>0.330739</td>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.426901</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.730994</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.380117</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.345029</td>\n",
       "      <td>0.076023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429515</td>\n",
       "      <td>0.229075</td>\n",
       "      <td>0.451542</td>\n",
       "      <td>0.685022</td>\n",
       "      <td>0.596916</td>\n",
       "      <td>0.799559</td>\n",
       "      <td>0.398678</td>\n",
       "      <td>0.341410</td>\n",
       "      <td>0.370044</td>\n",
       "      <td>0.290749</td>\n",
       "      <td>0.063877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.496454</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.773050</td>\n",
       "      <td>0.624113</td>\n",
       "      <td>0.794326</td>\n",
       "      <td>0.460993</td>\n",
       "      <td>0.304965</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.099291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322917</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 16888 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ChronicCond_Alzheimer  ChronicCond_Cancer  ChronicCond_Depression  ChronicCond_Diabetes  ChronicCond_Heartfailure  ChronicCond_IschemicHeart  ChronicCond_KidneyDisease  ChronicCond_ObstrPulmonary  ChronicCond_Osteoporasis  ChronicCond_rheumatoidarthritis  ChronicCond_stroke  County_0  County_1  County_10  County_100  County_11  County_110  County_111  County_113  County_117  County_120  County_130  County_131  County_14  County_140  County_141  County_150  County_160  County_161  \\\n",
       "0               0.365759            0.233463                0.451362              0.754864                  0.564202                   0.762646                   0.474708                    0.400778                  0.272374                         0.330739            0.105058  0.011673       0.0   0.011673    0.011673        0.0         0.0         0.0         0.0         0.0         0.0    0.015564         0.0        0.0    0.003891         0.0     0.07393    0.000000         0.0   \n",
       "1               0.426901            0.175439                0.444444              0.730994                  0.649123                   0.807018                   0.473684                    0.380117                  0.280702                         0.345029            0.076023  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.023392         0.0        0.0    0.005848         0.0     0.00000    0.000000         0.0   \n",
       "2               0.429515            0.229075                0.451542              0.685022                  0.596916                   0.799559                   0.398678                    0.341410                  0.370044                         0.290749            0.063877  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.000000         0.0   \n",
       "3               0.496454            0.191489                0.446809              0.773050                  0.624113                   0.794326                   0.460993                    0.304965                  0.326241                         0.326241            0.099291  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.014184         0.0   \n",
       "4               0.322917            0.156250                0.385417              0.645833                  0.645833                   0.687500                   0.395833                    0.302083                  0.291667                         0.270833            0.104167  0.000000       0.0   0.031250    0.031250        0.0         0.0         0.0         0.0         0.0         0.0    0.020833         0.0        0.0    0.000000         0.0     0.00000    0.135417         0.0   \n",
       "\n",
       "   County_170  County_180  County_190  County_191  County_194  County_20  County_200  County_210  County_211  County_212  County_213  County_220  County_221  County_222  County_223  County_224  County_230  County_240  County_241  County_25  County_250     ...       proc_9764.0  proc_9784.0  proc_9787.0  proc_9789.0  proc_9805.0  proc_9815.0  proc_9851.0  proc_9903.0  proc_9904.0  proc_9905.0  proc_9906.0  proc_9907.0  proc_9910.0  proc_9914.0  proc_9915.0  proc_9916.0  proc_9917.0  \\\n",
       "0         0.0    0.003891    0.011673         0.0         0.0   0.003891         0.0    0.000000         0.0         0.0         0.0    0.011673         0.0         0.0         0.0         0.0    0.007782    0.011673         0.0        0.0    0.054475     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "1         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.070175         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.070175         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          2.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "2         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.000000         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.156388         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "3         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.014184         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "4         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.010417         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   proc_9918.0  proc_9919.0  proc_9920.0  proc_9921.0  proc_9922.0  proc_9923.0  proc_9925.0  proc_9926.0  proc_9928.0  proc_9929.0  proc_9938.0  proc_9939.0  proc_9952.0  proc_9955.0  proc_9959.0  proc_9960.0  proc_9961.0  proc_9962.0  proc_9969.0  proc_9971.0  proc_9972.0  proc_9973.0  proc_9974.0  proc_9975.0  proc_9978.0  proc_9979.0  proc_9982.0  proc_9984.0  proc_9986.0  proc_9992.0  proc_9995.0  proc_9998.0  proc_9999.0  \n",
       "0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "1          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "2          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "3          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "4          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "\n",
       "[5 rows x 16888 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Provider</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PotentialFraud</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Provider\n",
       "PotentialFraud          \n",
       "0                   4904\n",
       "1                    506"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are dealing with an imbalanced data set.\n",
    "train_final_data[['PotentialFraud', 'Provider']].groupby('PotentialFraud').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with varied resampling methods, to help with class imbalanced. Under-sampling methods worked best.\n",
    "##### Key models  \n",
    "\n",
    "\n",
    "###### Key models:  \n",
    "\n",
    "* our best models, which implemented random undersampling, had a .20 increase in class 1 recall, consistent class 1 precision, and marginal decrease in overall accuracy.\n",
    "* near miss undersampling resulted in a higher class 1 recall of .98, but overall accuracy decreased .331. \n",
    "\n",
    "type, undersampling method | Class 1 Precision | Class 1 Recall | Accuracy Score |\n",
    "------- | ------------ | -------------- | -------------- | \n",
    "BEST: lasso tuned, random under sampling | 0.46 | 0.75 | 0.885\n",
    "BEST: ridge tuned, random under sampling | 0.46 | 0.75 | 0.885\n",
    "ridge tuned, near miss | 0.18 | 0.98 | 0.554|\n",
    "* lasso logistic tuned | .46   | .55 | .897\n",
    "* ridge logistic tuned | .46 | .55 | .897"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Provider</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PotentialFraud</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Provider\n",
       "PotentialFraud          \n",
       "0                   4904\n",
       "1                    506"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_data[['PotentialFraud', 'Provider']].groupby('PotentialFraud').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 1.0\n",
      "Test accuracy score 0.869\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3688\n",
      "           1       1.00      1.00      1.00      3688\n",
      "\n",
      "    accuracy                           1.00      7376\n",
      "   macro avg       1.00      1.00      1.00      7376\n",
      "weighted avg       1.00      1.00      1.00      7376\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      1216\n",
      "           1       0.33      0.28      0.31       137\n",
      "\n",
      "    accuracy                           0.87      1353\n",
      "   macro avg       0.63      0.61      0.62      1353\n",
      "weighted avg       0.86      0.87      0.86      1353\n",
      "\n",
      "LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=30, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lassoreg_cv(train_final_data, 'PotentialFraud', 'ros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 1.0\n",
      "Test accuracy score 0.86\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3688\n",
      "           1       1.00      1.00      1.00      3799\n",
      "\n",
      "    accuracy                           1.00      7487\n",
      "   macro avg       1.00      1.00      1.00      7487\n",
      "weighted avg       1.00      1.00      1.00      7487\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92      1216\n",
      "           1       0.29      0.26      0.27       137\n",
      "\n",
      "    accuracy                           0.86      1353\n",
      "   macro avg       0.60      0.59      0.60      1353\n",
      "weighted avg       0.85      0.86      0.86      1353\n",
      "\n",
      "LogisticRegression(C=5, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=30, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lassoreg_cv(train_final_data, 'PotentialFraud', 'ADASYN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 1.0\n",
      "Test accuracy score 0.866\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3688\n",
      "           1       1.00      1.00      1.00      3688\n",
      "\n",
      "    accuracy                           1.00      7376\n",
      "   macro avg       1.00      1.00      1.00      7376\n",
      "weighted avg       1.00      1.00      1.00      7376\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93      1216\n",
      "           1       0.33      0.32      0.33       137\n",
      "\n",
      "    accuracy                           0.87      1353\n",
      "   macro avg       0.63      0.62      0.63      1353\n",
      "weighted avg       0.86      0.87      0.87      1353\n",
      "\n",
      "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=30, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lassoreg_cv(train_final_data, 'PotentialFraud', 'SMOTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 0.942\n",
      "Test accuracy score 0.885\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       369\n",
      "           1       0.99      0.89      0.94       369\n",
      "\n",
      "    accuracy                           0.94       738\n",
      "   macro avg       0.95      0.94      0.94       738\n",
      "weighted avg       0.95      0.94      0.94       738\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93      1216\n",
      "           1       0.46      0.75      0.57       137\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.71      0.83      0.75      1353\n",
      "weighted avg       0.92      0.88      0.90      1353\n",
      "\n",
      "LogisticRegression(C=3, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=30, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lassoreg_cv(train_final_data, 'PotentialFraud', 'rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 0.927\n",
      "Test accuracy score 0.806\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       369\n",
      "           1       0.95      0.91      0.93       369\n",
      "\n",
      "    accuracy                           0.93       738\n",
      "   macro avg       0.93      0.93      0.93       738\n",
      "weighted avg       0.93      0.93      0.93       738\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.88      1216\n",
      "           1       0.28      0.59      0.38       137\n",
      "\n",
      "    accuracy                           0.81      1353\n",
      "   macro avg       0.61      0.71      0.63      1353\n",
      "weighted avg       0.88      0.81      0.83      1353\n",
      "\n",
      "LogisticRegression(C=3, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=30, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lassoreg_cv(train_final_data, \"PotentialFraud\", \"cc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 0.977\n",
      "Test accuracy score 0.664\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       369\n",
      "           1       1.00      0.95      0.98       369\n",
      "\n",
      "    accuracy                           0.98       738\n",
      "   macro avg       0.98      0.98      0.98       738\n",
      "weighted avg       0.98      0.98      0.98       738\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.63      0.77      1216\n",
      "           1       0.22      0.93      0.36       137\n",
      "\n",
      "    accuracy                           0.66      1353\n",
      "   macro avg       0.60      0.78      0.57      1353\n",
      "weighted avg       0.91      0.66      0.73      1353\n",
      "\n",
      "LogisticRegression(C=3, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=30, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lassoreg_cv(train_final_data, \"PotentialFraud\", \"NearMiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 0.9417344173441734\n",
      "Test accuracy score 0.88470066518847\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       369\n",
      "           1       0.99      0.89      0.94       369\n",
      "\n",
      "    accuracy                           0.94       738\n",
      "   macro avg       0.95      0.94      0.94       738\n",
      "weighted avg       0.95      0.94      0.94       738\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93      1216\n",
      "           1       0.46      0.75      0.57       137\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.71      0.83      0.75      1353\n",
      "weighted avg       0.92      0.88      0.90      1353\n",
      "\n",
      "LogisticRegression(C=7, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=30, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "ridgereg_cv(train_final_data, 'PotentialFraud', 'rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 0.926829268292683\n",
      "Test accuracy score 0.8056171470805618\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       369\n",
      "           1       0.95      0.91      0.93       369\n",
      "\n",
      "    accuracy                           0.93       738\n",
      "   macro avg       0.93      0.93      0.93       738\n",
      "weighted avg       0.93      0.93      0.93       738\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.88      1216\n",
      "           1       0.28      0.59      0.38       137\n",
      "\n",
      "    accuracy                           0.81      1353\n",
      "   macro avg       0.61      0.71      0.63      1353\n",
      "weighted avg       0.88      0.81      0.83      1353\n",
      "\n",
      "LogisticRegression(C=7, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=30, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#top performing model\n",
    "ridgereg_cv(train_final_data, 'PotentialFraud', 'cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 1.0\n",
      "Test accuracy score 0.5543237250554324\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       369\n",
      "           1       1.00      1.00      1.00       369\n",
      "\n",
      "    accuracy                           1.00       738\n",
      "   macro avg       1.00      1.00      1.00       738\n",
      "weighted avg       1.00      1.00      1.00       738\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.51      0.67      1216\n",
      "           1       0.18      0.98      0.31       137\n",
      "\n",
      "    accuracy                           0.55      1353\n",
      "   macro avg       0.59      0.74      0.49      1353\n",
      "weighted avg       0.91      0.55      0.63      1353\n",
      "\n",
      "LogisticRegression(C=7, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=30, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "ridgereg_cv(train_final_data, 'PotentialFraud', 'NearMiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Importance                       Variable\n",
      "12067    0.336890                       in_c_854\n",
      "11663    0.299544                       in_c_309\n",
      "12077    0.232808                       in_c_867\n",
      "12148    0.228298                       in_c_983\n",
      "11620    0.225878                       in_c_251\n",
      "335      0.213491    Mean_InscClaimAmtReimbursed\n",
      "11612    0.206222                       in_c_243\n",
      "333      0.204890  Mean_IPAnnualReimbursementAmt\n",
      "11640    0.204276                       in_c_286\n",
      "11591    0.196688                       in_c_222\n",
      "11588    0.192800                       in_c_219\n",
      "11567    0.192328                       in_c_192\n",
      "11594    0.182433                       in_c_225\n",
      "11653    0.179927                       in_c_299\n",
      "12130    0.177822                       in_c_950\n",
      "11423    0.175631                       in_c_001\n",
      "11430    0.173405                       in_c_008\n",
      "11562    0.173224                       in_c_187\n",
      "11424    0.172189                       in_c_002\n",
      "11613    0.170132                       in_c_244\n",
      "11558    0.169803                       in_c_183\n",
      "12075    0.167966                       in_c_865\n",
      "12122    0.166835                       in_c_939\n",
      "11435    0.161920                       in_c_013\n",
      "11570    0.161757                       in_c_195\n",
      "11667    0.160553                       in_c_313\n",
      "12081    0.160383                       in_c_871\n",
      "12125    0.157131                       in_c_945\n",
      "11571    0.155427                       in_c_196\n",
      "11561    0.155304                       in_c_186\n",
      "11568    0.154676                       in_c_193\n",
      "11668    0.152830                       in_c_314\n",
      "12070    0.152214                       in_c_857\n",
      "11646    0.149941                       in_c_292\n",
      "11670    0.149534                       in_c_316\n",
      "11589    0.148855                       in_c_220\n",
      "11623    0.146329                       in_c_254\n",
      "11576    0.145374                       in_c_201\n",
      "11644    0.145233                       in_c_290\n",
      "12069    0.142173                       in_c_856\n",
      "11600    0.140229                       in_c_231\n",
      "11584    0.138781                       in_c_215\n",
      "11648    0.137865                       in_c_294\n",
      "11731    0.136339                       in_c_395\n",
      "11638    0.136073                       in_c_284\n",
      "11573    0.135002                       in_c_198\n",
      "11578    0.133795                       in_c_203\n",
      "11843    0.129602                       in_c_543\n",
      "11626    0.129294                       in_c_257\n",
      "11607    0.128913                       in_c_238\n",
      "11802    0.126959                       in_c_487\n",
      "12128    0.126178                       in_c_948\n",
      "11606    0.124938                       in_c_237\n",
      "11818    0.124847                       in_c_503\n",
      "12151    0.124444                       in_c_986\n",
      "11425    0.123306                       in_c_003\n",
      "11587    0.122017                       in_c_218\n",
      "11628    0.121206                       in_c_259\n",
      "11572    0.120035                       in_c_197\n",
      "12090    0.119945                       in_c_886\n",
      "11602    0.119519                       in_c_233\n",
      "11654    0.118989                       in_c_300\n",
      "12073    0.117063                       in_c_863\n",
      "11657    0.116821                       in_c_303\n",
      "11605    0.116740                       in_c_236\n",
      "11604    0.116442                       in_c_235\n",
      "11934    0.116200                       in_c_664\n",
      "11611    0.115589                       in_c_242\n",
      "11590    0.112307                       in_c_221\n",
      "11544    0.111203                       in_c_163\n",
      "11857    0.109991                       in_c_557\n",
      "12072    0.109702                       in_c_862\n",
      "11577    0.109016                       in_c_202\n",
      "11582    0.108977                       in_c_207\n",
      "11492    0.108544                       in_c_085\n",
      "11585    0.108294                       in_c_216\n",
      "11639    0.107109                       in_c_285\n",
      "12153    0.106246                       in_c_988\n",
      "11618    0.106098                       in_c_249\n",
      "12034    0.106087                       in_c_812\n",
      "11549    0.104926                       in_c_168\n",
      "11632    0.103380                       in_c_263\n",
      "11712    0.101914                       in_c_376\n",
      "11672    0.101366                       in_c_327\n",
      "11560    0.100981                       in_c_185\n",
      "11664    0.100509                       in_c_310\n",
      "11559    0.098724                       in_c_184\n",
      "11649    0.096518                       in_c_295\n",
      "12091    0.096023                       in_c_887\n",
      "12088    0.095332                       in_c_884\n",
      "11581    0.094696                       in_c_206\n",
      "12087    0.094140                       in_c_883\n",
      "11552    0.093802                       in_c_177\n",
      "11679    0.093039                       in_c_334\n",
      "11599    0.092385                       in_c_230\n",
      "11808    0.092316                       in_c_493\n",
      "11813    0.091369                       in_c_498\n",
      "12066    0.091006                       in_c_853\n",
      "11627    0.089719                       in_c_258\n",
      "11434    0.089675                       in_c_012\n",
      "12138    0.088601                       in_c_964\n",
      "11580    0.087960                       in_c_205\n",
      "12080    0.087253                       in_c_870\n",
      "11429    0.086083                       in_c_007\n",
      "11794    0.084556                       in_c_479\n",
      "11566    0.084360                       in_c_191\n",
      "12155    0.082065                       in_c_998\n",
      "11858    0.081980                       in_c_558\n",
      "11859    0.081558                       in_c_559\n",
      "11660    0.081214                       in_c_306\n",
      "11746    0.079798                       in_c_419\n",
      "11556    0.077755                       in_c_181\n",
      "11615    0.076659                       in_c_246\n",
      "11630    0.076265                       in_c_261\n",
      "11666    0.075826                       in_c_312\n",
      "12127    0.075769                       in_c_947\n",
      "11681    0.075323                       in_c_336\n",
      "11932    0.075178                       in_c_662\n",
      "11960    0.074281                       in_c_696\n",
      "11683    0.074105                       in_c_338\n",
      "11853    0.072377                       in_c_553\n",
      "11609    0.070850                       in_c_240\n",
      "11729    0.070592                       in_c_393\n",
      "11592    0.069398                       in_c_223\n",
      "12085    0.065249                       in_c_881\n",
      "11554    0.064971                       in_c_179\n",
      "11851    0.064971                       in_c_551\n",
      "11752    0.064891                       in_c_425\n",
      "12078    0.064360                       in_c_868\n",
      "11490    0.063806                       in_c_083\n",
      "11496    0.063626                       in_c_089\n",
      "11911    0.062569                       in_c_629\n",
      "11651    0.060763                       in_c_297\n",
      "11770    0.060536                       in_c_455\n",
      "13838    0.059866                     out_c_5859\n",
      "11838    0.059693                       in_c_538\n",
      "11545    0.057654                       in_c_164\n",
      "11815    0.057332                       in_c_500\n",
      "11810    0.057224                       in_c_495\n",
      "12076    0.056397                       in_c_866\n",
      "11841    0.056205                       in_c_541\n",
      "11564    0.056026                       in_c_189\n",
      "11603    0.055886                       in_c_234\n",
      "11819    0.055747                       in_c_504\n",
      "11689    0.055652                       in_c_344\n",
      "11551    0.055517                       in_c_176\n",
      "11575    0.055497                       in_c_200\n",
      "11768    0.054424                       in_c_453\n",
      "11579    0.053786                       in_c_204\n",
      "15332    0.053589                     out_c_V562\n",
      "11760    0.053332                       in_c_439\n",
      "11621    0.053280                       in_c_252\n",
      "11785    0.053129                       in_c_470\n",
      "12126    0.052848                       in_c_946\n",
      "11804    0.051774                       in_c_489\n",
      "12083    0.051451                       in_c_876\n",
      "11634    0.051338                       in_c_280\n",
      "15195    0.051018                     out_c_V420\n",
      "11796    0.050940                       in_c_481\n",
      "13835    0.050849                     out_c_5854\n",
      "11557    0.050356                       in_c_182\n",
      "11691    0.050333                       in_c_346\n",
      "12079    0.049977                       in_c_869\n",
      "15331    0.049227                     out_c_V561\n",
      "11784    0.049072                       in_c_469\n",
      "11732    0.048858                       in_c_405\n",
      "11940    0.048841                       in_c_670\n",
      "11883    0.048757                       in_c_595\n",
      "11788    0.048572                       in_c_473\n",
      "11952    0.047934                       in_c_688\n",
      "11879    0.047821                       in_c_585\n",
      "11477    0.047552                       in_c_070\n",
      "11617    0.047540                       in_c_248\n",
      "11950    0.047037                       in_c_686\n",
      "12146    0.046921                       in_c_981\n",
      "11786    0.046848                       in_c_471\n",
      "11460    0.046333                       in_c_053\n",
      "11817    0.046268                       in_c_502\n",
      "11474    0.046167                       in_c_067\n",
      "12131    0.046079                       in_c_951\n",
      "12113    0.045947                       in_c_921\n",
      "11743    0.045903                       in_c_416\n",
      "13834    0.045557                     out_c_5853\n",
      "11619    0.044803                       in_c_250\n",
      "11655    0.044782                       in_c_301\n",
      "11844    0.044169                       in_c_544\n",
      "11741    0.044025                       in_c_414\n",
      "11616    0.043980                       in_c_247\n",
      "12086    0.043781                       in_c_882\n",
      "11597    0.043696                       in_c_228\n",
      "11828    0.043545                       in_c_513\n",
      "15333    0.043466                    out_c_V5631\n",
      "11797    0.043292                       in_c_482\n",
      "11826    0.043100                       in_c_511\n",
      "11569    0.042928                       in_c_194\n",
      "11622    0.042773                       in_c_253\n",
      "11693    0.042688                       in_c_348\n",
      "12150    0.042671                       in_c_985\n",
      "11595    0.042603                       in_c_226\n",
      "11937    0.042547                       in_c_667\n",
      "11673    0.042470                       in_c_328\n",
      "11922    0.041867                       in_c_652\n",
      "11629    0.041613                       in_c_260\n",
      "14789    0.041603                     out_c_7925\n",
      "12097    0.041281                       in_c_902\n",
      "11856    0.040674                       in_c_556\n",
      "11787    0.040539                       in_c_472\n",
      "11801    0.040453                       in_c_486\n",
      "11704    0.040447                       in_c_368\n",
      "15238    0.040255                    out_c_V4512\n",
      "12047    0.040242                       in_c_828\n",
      "11669    0.040241                       in_c_315\n",
      "11485    0.040200                       in_c_078\n",
      "11658    0.039869                       in_c_304\n",
      "15330    0.039514                     out_c_V560\n",
      "12084    0.039492                       in_c_880\n",
      "15334    0.039442                    out_c_V5632\n",
      "13618    0.039425                    out_c_51884\n",
      "11506    0.039338                       in_c_099\n",
      "11438    0.039284                       in_c_022\n",
      "11913    0.038904                       in_c_637\n",
      "11951    0.038415                       in_c_687\n",
      "11694    0.038115                       in_c_349\n",
      "11832    0.037805                       in_c_517\n",
      "15335    0.037349                     out_c_V568\n",
      "11700    0.036712                       in_c_355\n",
      "11550    0.036535                       in_c_175\n",
      "11662    0.036499                       in_c_308\n",
      "11458    0.036292                       in_c_042\n",
      "11678    0.036185                       in_c_333\n",
      "11586    0.036174                       in_c_217\n",
      "11936    0.036016                       in_c_666\n",
      "11426    0.035836                       in_c_004\n",
      "11665    0.035485                       in_c_311\n",
      "11698    0.035331                       in_c_353\n",
      "11437    0.035086                       in_c_021\n",
      "11722    0.034891                       in_c_386\n",
      "11761    0.034848                       in_c_440\n",
      "13836    0.034656                     out_c_5855\n",
      "11718    0.034571                       in_c_382\n",
      "11909    0.033765                       in_c_627\n",
      "11753    0.033639                       in_c_432\n",
      "11647    0.033634                       in_c_293\n",
      "11772    0.033573                       in_c_457\n",
      "11715    0.033472                       in_c_379\n",
      "11574    0.033421                       in_c_199\n",
      "11433    0.033416                       in_c_011\n",
      "12089    0.033342                       in_c_885\n",
      "11637    0.033260                       in_c_283\n",
      "15237    0.033228                    out_c_V4511\n",
      "11831    0.032306                       in_c_516\n",
      "11839    0.031853                       in_c_539\n",
      "11456    0.031671                       in_c_040\n",
      "11781    0.031610                       in_c_466\n",
      "12074    0.031259                       in_c_864\n",
      "11867    0.031046                       in_c_573\n",
      "11439    0.030972                       in_c_023\n",
      "11799    0.030869                       in_c_484\n",
      "15347    0.030643                    out_c_V5812\n",
      "11800    0.030415                       in_c_485\n",
      "13837    0.030402                     out_c_5856\n",
      "11725    0.030345                       in_c_389\n",
      "13842    0.030317                    out_c_58881\n",
      "11855    0.030285                       in_c_555\n",
      "15393    0.030167                     out_c_V662\n",
      "11834    0.029679                       in_c_534\n",
      "11555    0.029517                       in_c_180\n",
      "11713    0.029423                       in_c_377\n",
      "12105    0.028865                       in_c_913\n",
      "11803    0.028849                       in_c_488\n",
      "11811    0.028503                       in_c_496\n",
      "11962    0.028410                       in_c_698\n",
      "11845    0.028151                       in_c_545\n",
      "337      0.028070  Mean_OPAnnualReimbursementAmt\n",
      "11903    0.027716                       in_c_621\n",
      "11596    0.027493                       in_c_227\n",
      "11563    0.027252                       in_c_188\n",
      "15346    0.027156                    out_c_V5811\n",
      "11869    0.027140                       in_c_575\n",
      "14675    0.026985                    out_c_78659\n",
      "11860    0.026567                       in_c_560\n",
      "11933    0.026558                       in_c_663\n",
      "11642    0.026451                       in_c_288\n",
      "11821    0.025846                       in_c_506\n",
      "11898    0.025719                       in_c_616\n",
      "12036    0.025648                       in_c_814\n",
      "11635    0.025253                       in_c_281\n",
      "11766    0.025142                       in_c_445\n",
      "12103    0.024978                       in_c_908\n",
      "11701    0.024930                       in_c_356\n",
      "11865    0.024698                       in_c_565\n",
      "11779    0.024486                       in_c_464\n",
      "12609    0.023833                    out_c_28521\n",
      "11918    0.023282                       in_c_642\n",
      "11771    0.023171                       in_c_456\n",
      "11650    0.022835                       in_c_296\n",
      "11702    0.022753                       in_c_357\n",
      "12049    0.022683                       in_c_830\n",
      "11680    0.022514                       in_c_335\n",
      "11687    0.022164                       in_c_342\n",
      "11907    0.022110                       in_c_625\n",
      "11503    0.022096                       in_c_096\n",
      "11789    0.021925                       in_c_474\n",
      "11916    0.021895                       in_c_640\n",
      "15345    0.021855                     out_c_V580\n",
      "11958    0.021812                       in_c_694\n",
      "11816    0.021745                       in_c_501\n",
      "12043    0.021635                       in_c_824\n",
      "11726    0.021562                       in_c_390\n",
      "11717    0.021471                       in_c_381\n",
      "15402    0.021117                     out_c_V672\n",
      "11546    0.021033                       in_c_165\n",
      "15251    0.020972                    out_c_V4582\n",
      "11915    0.020843                       in_c_639\n",
      "11835    0.020813                       in_c_535\n",
      "11751    0.020586                       in_c_424\n",
      "15401    0.020559                     out_c_V671\n",
      "11938    0.020464                       in_c_668\n",
      "12098    0.020375                       in_c_903\n",
      "11703    0.020345                       in_c_358\n",
      "11747    0.020198                       in_c_420\n",
      "11824    0.020130                       in_c_509\n",
      "11682    0.019967                       in_c_337\n",
      "11777    0.019869                       in_c_462\n",
      "11805    0.019579                       in_c_490\n",
      "11593    0.018782                       in_c_224\n",
      "11847    0.017820                       in_c_547\n",
      "12050    0.017774                       in_c_834\n",
      "11993    0.017659                       in_c_741\n",
      "11442    0.017325                       in_c_026\n",
      "11904    0.017249                       in_c_622\n",
      "12046    0.017231                       in_c_827\n",
      "11943    0.017114                       in_c_673\n",
      "12124    0.016544                       in_c_941\n",
      "11641    0.016347                       in_c_287\n",
      "11778    0.016330                       in_c_463\n",
      "11776    0.016167                       in_c_461\n",
      "11850    0.015880                       in_c_550\n",
      "11443    0.015848                       in_c_027\n",
      "11901    0.015717                       in_c_619\n",
      "11986    0.015580                       in_c_734\n",
      "15392    0.015404                     out_c_V661\n",
      "11450    0.015294                       in_c_034\n",
      "11481    0.015158                       in_c_074\n",
      "11871    0.015138                       in_c_577\n",
      "12154    0.015034                       in_c_989\n",
      "14673    0.014961                    out_c_78651\n",
      "11921    0.014898                       in_c_645\n",
      "11685    0.014721                       in_c_340\n",
      "11692    0.014598                       in_c_347\n",
      "11895    0.014588                       in_c_607\n",
      "11833    0.014481                       in_c_533\n",
      "11671    0.014359                       in_c_326\n",
      "11697    0.014346                       in_c_352\n",
      "11780    0.014337                       in_c_465\n",
      "11444    0.014204                       in_c_028\n",
      "11688    0.014072                       in_c_343\n",
      "11902    0.014008                       in_c_620\n",
      "11471    0.013843                       in_c_064\n",
      "11480    0.013577                       in_c_073\n",
      "11690    0.013550                       in_c_345\n",
      "11957    0.013548                       in_c_693\n",
      "11848    0.013321                       in_c_548\n",
      "14587    0.013247                     out_c_7802\n",
      "12109    0.012997                       in_c_917\n",
      "12104    0.012869                       in_c_909\n",
      "11479    0.012767                       in_c_072\n",
      "15098    0.012672                     out_c_V103\n",
      "11464    0.012632                       in_c_057\n",
      "11881    0.012632                       in_c_593\n",
      "11448    0.012610                       in_c_032\n",
      "11829    0.012549                       in_c_514\n",
      "11849    0.012411                       in_c_549\n",
      "11769    0.012075                       in_c_454\n",
      "11765    0.011989                       in_c_444\n",
      "12009    0.011979                       in_c_760\n",
      "11756    0.011955                       in_c_435\n",
      "11470    0.011923                       in_c_063\n",
      "11486    0.011906                       in_c_079\n",
      "12059    0.011883                       in_c_843\n",
      "11994    0.011855                       in_c_742\n",
      "11884    0.011841                       in_c_596\n",
      "11488    0.011805                       in_c_081\n",
      "11948    0.011683                       in_c_684\n",
      "11720    0.011669                       in_c_384\n",
      "11608    0.011614                       in_c_239\n",
      "11745    0.011604                       in_c_418\n",
      "13181    0.011439                      out_c_412\n",
      "12108    0.011344                       in_c_916\n",
      "11854    0.011297                       in_c_554\n",
      "11975    0.010769                       in_c_717\n",
      "11699    0.010703                       in_c_354\n",
      "11893    0.010578                       in_c_605\n",
      "11926    0.010383                       in_c_656\n",
      "11827    0.010218                       in_c_512\n",
      "12142    0.010156                       in_c_974\n",
      "11870    0.010154                       in_c_576\n",
      "11887    0.009985                       in_c_599\n",
      "12027    0.009907                       in_c_802\n",
      "11920    0.009798                       in_c_644\n",
      "11510    0.009768                       in_c_103\n",
      "12266    0.009732                      out_c_185\n",
      "12029    0.009719                       in_c_804\n",
      "11944    0.009602                       in_c_674\n",
      "11912    0.009422                       in_c_630\n",
      "11928    0.009410                       in_c_658\n",
      "11742    0.009346                       in_c_415\n",
      "12578    0.009335                     out_c_2809\n",
      "13185    0.009285                    out_c_41400\n",
      "11440    0.009220                       in_c_024\n",
      "11476    0.009198                       in_c_069\n",
      "11917    0.009165                       in_c_641\n",
      "12062    0.009122                       in_c_846\n",
      "11734    0.008969                       in_c_407\n",
      "11454    0.008827                       in_c_038\n",
      "14672    0.008819                    out_c_78650\n",
      "13246    0.008810                    out_c_42789\n",
      "11472    0.008808                       in_c_065\n",
      "13276    0.008736                      out_c_431\n",
      "12230    0.008727                     out_c_1625\n",
      "15099    0.008667                    out_c_V1042\n",
      "12044    0.008662                       in_c_825\n",
      "11923    0.008498                       in_c_653\n",
      "11910    0.008472                       in_c_628\n",
      "11947    0.008433                       in_c_683\n",
      "13186    0.008335                    out_c_41401\n",
      "12033    0.008265                       in_c_811\n",
      "11457    0.008100                       in_c_041\n",
      "11466    0.008068                       in_c_059\n",
      "13194    0.007969                     out_c_4148\n",
      "11427    0.007944                       in_c_005\n",
      "13184    0.007942                     out_c_4139\n",
      "12107    0.007910                       in_c_915\n",
      "11949    0.007871                       in_c_685\n",
      "11504    0.007682                       in_c_097\n",
      "11677    0.007677                       in_c_332\n",
      "11520    0.007563                       in_c_130\n",
      "11499    0.007561                       in_c_092\n",
      "15101    0.007460                    out_c_V1046\n",
      "11894    0.007382                       in_c_606\n",
      "12052    0.007298                       in_c_836\n",
      "11478    0.007275                       in_c_071\n",
      "11866    0.007219                       in_c_566\n",
      "11762    0.007176                       in_c_441\n",
      "11939    0.007155                       in_c_669\n",
      "11941    0.007135                       in_c_671\n",
      "11875    0.007115                       in_c_581\n",
      "15250    0.007053                    out_c_V4581\n",
      "11710    0.007042                       in_c_374\n",
      "13179    0.007003                    out_c_41181\n",
      "11840    0.007000                       in_c_540\n",
      "11601    0.006985                       in_c_232\n",
      "12140    0.006858                       in_c_969\n",
      "12137    0.006856                       in_c_963\n",
      "11452    0.006837                       in_c_036\n",
      "11982    0.006834                       in_c_727\n",
      "11999    0.006587                       in_c_747\n",
      "11812    0.006432                       in_c_497\n",
      "11896    0.006427                       in_c_614\n",
      "12328    0.006367                     out_c_2334\n",
      "13247    0.006345                     out_c_4279\n",
      "11730    0.006252                       in_c_394\n",
      "11806    0.006238                       in_c_491\n",
      "13183    0.006233                     out_c_4131\n",
      "11900    0.006181                       in_c_618\n",
      "11491    0.006145                       in_c_084\n",
      "11441    0.005939                       in_c_025\n",
      "11469    0.005907                       in_c_062\n",
      "11767    0.005903                       in_c_446\n",
      "12326    0.005900                     out_c_2330\n",
      "11878    0.005785                       in_c_584\n",
      "11892    0.005774                       in_c_604\n",
      "13013    0.005733                     out_c_3555\n",
      "11931    0.005676                       in_c_661\n",
      "13069    0.005502                    out_c_36616\n",
      "13665    0.005462                    out_c_53361\n",
      "11449    0.005287                       in_c_033\n",
      "11461    0.005273                       in_c_054\n",
      "11890    0.005270                       in_c_602\n",
      "11953    0.005204                       in_c_689\n",
      "11820    0.005188                       in_c_505\n",
      "11489    0.005129                       in_c_082\n",
      "13193    0.005060                     out_c_4143\n",
      "12055    0.005009                       in_c_839\n",
      "12302    0.004983                    out_c_20921\n",
      "12232    0.004974                     out_c_1629\n",
      "11723    0.004967                       in_c_387\n",
      "15363    0.004934                    out_c_V5869\n",
      "13180    0.004933                    out_c_41189\n",
      "12038    0.004893                       in_c_816\n",
      "12030    0.004886                       in_c_808\n",
      "12251    0.004884                     out_c_1741\n",
      "12045    0.004882                       in_c_826\n",
      "11930    0.004869                       in_c_660\n",
      "13178    0.004844                     out_c_4111\n",
      "11431    0.004841                       in_c_009\n",
      "11422    0.004796                       in_c_000\n",
      "11971    0.004783                       in_c_713\n",
      "12231    0.004762                     out_c_1628\n",
      "13878    0.004741                     out_c_5942\n",
      "12093    0.004694                       in_c_895\n",
      "13244    0.004643                    out_c_42769\n",
      "11889    0.004624                       in_c_601\n",
      "13177    0.004601                     out_c_4110\n",
      "11754    0.004590                       in_c_433\n",
      "11885    0.004521                       in_c_597\n",
      "11791    0.004446                       in_c_476\n",
      "13379    0.004424                     out_c_4439\n",
      "13182    0.004407                     out_c_4130\n",
      "11880    0.004371                       in_c_592\n",
      "11467    0.004209                       in_c_060\n",
      "12265    0.004087                     out_c_1830\n",
      "11493    0.003981                       in_c_086\n",
      "11995    0.003970                       in_c_743\n",
      "11744    0.003964                       in_c_417\n",
      "11468    0.003853                       in_c_061\n",
      "11728    0.003835                       in_c_392\n",
      "13732    0.003818                     out_c_5570\n",
      "12135    0.003810                       in_c_958\n",
      "11997    0.003797                       in_c_745\n",
      "13195    0.003796                     out_c_4149\n",
      "12253    0.003767                     out_c_1743\n",
      "11888    0.003612                       in_c_600\n",
      "13339    0.003607                    out_c_44020\n",
      "12558    0.003591                     out_c_2766\n",
      "12024    0.003578                       in_c_799\n",
      "13212    0.003571                    out_c_42491\n",
      "12115    0.003568                       in_c_923\n",
      "12256    0.003501                     out_c_1746\n",
      "13145    0.003498                     out_c_4019\n",
      "14089    0.003401                     out_c_7144\n",
      "332      0.003314     Mean_IPAnnualDeductibleAmt\n",
      "12134    0.003286                       in_c_957\n",
      "11748    0.003271                       in_c_421\n",
      "12112    0.003264                       in_c_920\n",
      "12866    0.003255                     out_c_3320\n",
      "11793    0.003250                       in_c_478\n",
      "14534    0.003225                    out_c_72992\n",
      "14697    0.003193                     out_c_7880\n",
      "12095    0.003187                       in_c_897\n",
      "12252    0.003156                     out_c_1742\n",
      "11530    0.003148                       in_c_146\n",
      "15306    0.003071                     out_c_V529\n",
      "13877    0.003068                     out_c_5941\n",
      "11906    0.003056                       in_c_624\n",
      "11963    0.003041                       in_c_699\n",
      "13564    0.003029                      out_c_490\n",
      "11500    0.003008                       in_c_093\n",
      "12004    0.002969                       in_c_755\n",
      "12905    0.002964                      out_c_340\n",
      "11733    0.002915                       in_c_406\n",
      "12101    0.002890                       in_c_906\n",
      "12000    0.002888                       in_c_748\n",
      "14088    0.002813                    out_c_71433\n",
      "12106    0.002786                       in_c_914\n",
      "12613    0.002786                     out_c_2859\n",
      "14604    0.002772                    out_c_78079\n",
      "12149    0.002756                       in_c_984\n",
      "13219    0.002732                    out_c_42613\n",
      "12065    0.002705                       in_c_849\n",
      "14731    0.002691                    out_c_78903\n",
      "13346    0.002649                     out_c_4404\n",
      "12259    0.002644                     out_c_1750\n",
      "11977    0.002631                       in_c_722\n",
      "12003    0.002612                       in_c_754\n",
      "11882    0.002599                       in_c_594\n",
      "13057    0.002582                    out_c_36600\n",
      "11432    0.002579                       in_c_010\n",
      "13238    0.002575                    out_c_42732\n",
      "11987    0.002569                       in_c_735\n",
      "13733    0.002537                     out_c_5571\n",
      "11501    0.002502                       in_c_094\n",
      "14390    0.002444                     out_c_7224\n",
      "13738    0.002442                    out_c_55842\n",
      "13691    0.002438                    out_c_55001\n",
      "13880    0.002339                     out_c_5949\n",
      "13070    0.002323                    out_c_36617\n",
      "11484    0.002311                       in_c_077\n",
      "15304    0.002311                     out_c_V524\n",
      "15097    0.002302                    out_c_V1011\n",
      "11528    0.002289                       in_c_138\n",
      "14426    0.002285                    out_c_72470\n",
      "14425    0.002276                     out_c_7246\n",
      "12260    0.002261                     out_c_1759\n",
      "12099    0.002252                       in_c_904\n",
      "15068    0.002244                    out_c_V0481\n",
      "11877    0.002235                       in_c_583\n",
      "12222    0.002233                     out_c_1573\n",
      "13860    0.002217                     out_c_5929\n",
      "15303    0.002205                     out_c_V521\n",
      "12228    0.002169                     out_c_1623\n",
      "13340    0.002168                    out_c_44021\n",
      "11521    0.002162                       in_c_131\n",
      "12042    0.002135                       in_c_823\n",
      "12110    0.002110                       in_c_918\n",
      "13243    0.002098                    out_c_42761\n",
      "12114    0.002097                       in_c_922\n",
      "13915    0.002079                     out_c_6100\n",
      "12281    0.002069                     out_c_1970\n",
      "12258    0.002065                     out_c_1749\n",
      "13237    0.002060                    out_c_42731\n",
      "13858    0.002058                     out_c_5920\n",
      "12255    0.002043                     out_c_1745\n",
      "13061    0.002012                    out_c_36604\n",
      "13144    0.002000                     out_c_4011\n",
      "14653    0.001990                     out_c_7851\n",
      "13245    0.001990                    out_c_42781\n",
      "14864    0.001989                     out_c_8409\n",
      "14730    0.001952                    out_c_78902\n",
      "13093    0.001947                     out_c_3669\n",
      "12445    0.001929                    out_c_25000\n",
      "11988    0.001923                       in_c_736\n",
      "15344    0.001920                     out_c_V579\n",
      "15341    0.001903                     out_c_V574\n",
      "12133    0.001899                       in_c_956\n",
      "12139    0.001880                       in_c_965\n",
      "14370    0.001870                     out_c_7210\n",
      "15337    0.001865                     out_c_V571\n",
      "11946    0.001859                       in_c_682\n",
      "15339    0.001851                    out_c_V5722\n",
      "13063    0.001845                    out_c_36610\n",
      "14085    0.001841                    out_c_71430\n",
      "12121    0.001833                       in_c_935\n",
      "11998    0.001831                       in_c_746\n",
      "14734    0.001821                    out_c_78906\n",
      "15308    0.001803                    out_c_V5331\n",
      "12152    0.001802                       in_c_987\n",
      "14735    0.001759                    out_c_78907\n",
      "14759    0.001756                    out_c_78961\n",
      "12288    0.001753                    out_c_20300\n",
      "12610    0.001745                    out_c_28522\n",
      "12026    0.001742                       in_c_801\n",
      "11534    0.001739                       in_c_150\n",
      "11976    0.001725                       in_c_718\n",
      "13280    0.001723                    out_c_43300\n",
      "13694    0.001713                    out_c_55010\n",
      "12250    0.001701                     out_c_1740\n",
      "14664    0.001697                    out_c_78605\n",
      "12037    0.001675                       in_c_815\n",
      "13590    0.001667                      out_c_496\n",
      "12559    0.001662                     out_c_2767\n",
      "11954    0.001648                       in_c_690\n",
      "14383    0.001648                    out_c_72210\n",
      "14084    0.001648                     out_c_7142\n",
      "14551    0.001647                    out_c_73303\n",
      "13485    0.001641                    out_c_46431\n",
      "12293    0.001635                    out_c_20382\n",
      "13089    0.001631                    out_c_36651\n",
      "11676    0.001631                       in_c_331\n",
      "14764    0.001621                    out_c_78966\n",
      "13218    0.001595                    out_c_42612\n",
      "12533    0.001577                     out_c_2724\n",
      "13425    0.001539                     out_c_4548\n",
      "12575    0.001538                     out_c_2800\n",
      "13091    0.001537                    out_c_36653\n",
      "15336    0.001533                     out_c_V570\n",
      "15305    0.001532                     out_c_V528\n",
      "13718    0.001529                    out_c_55321\n",
      "15340    0.001526                     out_c_V573\n",
      "12092    0.001524                       in_c_894\n",
      "13719    0.001499                     out_c_5533\n",
      "14130    0.001479                    out_c_71595\n",
      "13601    0.001465                     out_c_5131\n",
      "13075    0.001463                    out_c_36622\n",
      "15074    0.001462                     out_c_V053\n",
      "14603    0.001459                    out_c_78071\n",
      "11737    0.001437                       in_c_410\n",
      "13341    0.001435                    out_c_44022\n",
      "14499    0.001434                     out_c_7282\n",
      "14549    0.001432                    out_c_73301\n",
      "11807    0.001427                       in_c_492\n",
      "13696    0.001418                    out_c_55013\n",
      "12041    0.001405                       in_c_822\n",
      "12229    0.001394                     out_c_1624\n",
      "340      0.001388   Median_IPAnnualDeductibleAmt\n",
      "12254    0.001386                     out_c_1744\n",
      "13068    0.001384                    out_c_36615\n",
      "12284    0.001371                     out_c_1983\n",
      "12599    0.001358                     out_c_2839\n",
      "12058    0.001352                       in_c_842\n",
      "11874    0.001343                       in_c_580\n",
      "12306    0.001341                     out_c_2113\n",
      "14729    0.001341                    out_c_78901\n",
      "14086    0.001337                    out_c_71431\n",
      "12275    0.001328                     out_c_1888\n",
      "14393    0.001325                     out_c_7226\n",
      "14758    0.001324                    out_c_78960\n",
      "11533    0.001323                       in_c_149\n",
      "13064    0.001319                    out_c_36611\n",
      "13253    0.001303                    out_c_42823\n",
      "15010    0.001303                    out_c_99674\n",
      "13734    0.001289                     out_c_5579\n",
      "13248    0.001281                     out_c_4280\n",
      "12972    0.001271                     out_c_3481\n",
      "12227    0.001268                     out_c_1622\n",
      "15373    0.001263                    out_c_V5882\n",
      "14400    0.001244                    out_c_72282\n",
      "13722    0.001243                     out_c_5551\n",
      "13072    0.001243                    out_c_36619\n",
      "12321    0.001240                     out_c_2325\n",
      "13909    0.001236                    out_c_60020\n",
      "13699    0.001232                    out_c_55102\n",
      "14763    0.001231                    out_c_78965\n",
      "14813    0.001229                     out_c_7942\n",
      "13344    0.001221                    out_c_44029\n",
      "11905    0.001219                       in_c_623\n",
      "14663    0.001213                    out_c_78604\n",
      "13337    0.001208                     out_c_4400\n",
      "14526    0.001201                    out_c_72972\n",
      "13809    0.001201                     out_c_5763\n",
      "12257    0.001200                     out_c_1748\n",
      "14520    0.001199                    out_c_72931\n",
      "11782    0.001198                       in_c_467\n",
      "12601    0.001196                    out_c_28409\n",
      "13077    0.001192                    out_c_36630\n",
      "11527    0.001189                       in_c_137\n",
      "13073    0.001188                    out_c_36620\n",
      "14851    0.001182                    out_c_82021\n",
      "11757    0.001182                       in_c_436\n",
      "13749    0.001181                    out_c_56210\n",
      "14728    0.001180                    out_c_78900\n",
      "13301    0.001179                     out_c_4353\n",
      "13066    0.001179                    out_c_36613\n",
      "14736    0.001177                    out_c_78909\n",
      "13644    0.001174                     out_c_5309\n",
      "13086    0.001172                    out_c_36645\n",
      "14387    0.001170                    out_c_72231\n",
      "14092    0.001170                     out_c_7149\n",
      "12276    0.001164                     out_c_1889\n",
      "13863    0.001154                     out_c_5932\n",
      "12141    0.001153                       in_c_970\n",
      "14379    0.001152                     out_c_7218\n",
      "15302    0.001135                     out_c_V520\n",
      "13796    0.001134                    out_c_57420\n",
      "13234    0.001131                     out_c_4270\n",
      "13236    0.001117                     out_c_4272\n",
      "14399    0.001115                    out_c_72281\n",
      "13223    0.001114                    out_c_42650\n",
      "13840    0.001112                     out_c_5880\n",
      "12051    0.001108                       in_c_835\n",
      "13298    0.001107                     out_c_4350\n",
      "14761    0.001102                    out_c_78963\n",
      "14413    0.001098                     out_c_7237\n",
      "12411    0.001096                     out_c_2449\n",
      "11483    0.001088                       in_c_076\n",
      "13354    0.001087                     out_c_4412\n",
      "13220    0.001085                     out_c_4262\n",
      "15342    0.001077                    out_c_V5781\n",
      "14391    0.001073                    out_c_72251\n",
      "15310    0.001071                    out_c_V5339\n",
      "15343    0.001050                    out_c_V5789\n",
      "15338    0.001049                    out_c_V5721\n",
      "11531    0.001028                       in_c_147\n",
      "13080    0.001027                    out_c_36633\n",
      "12246    0.001026                     out_c_1736\n",
      "14429    0.001020                     out_c_7248\n",
      "15009    0.001020                    out_c_99673\n",
      "13058    0.001018                    out_c_36601\n",
      "11487    0.001016                       in_c_080\n",
      "12771    0.001015                    out_c_29661\n",
      "336      0.001009     Mean_OPAnnualDeductibleAmt\n",
      "13285    0.001001                    out_c_43321\n",
      "12061    0.000999                       in_c_845\n",
      "14760    0.000997                    out_c_78962\n",
      "15120    0.000996                    out_c_V1269\n",
      "14871    0.000994                    out_c_84200\n",
      "13748    0.000976                    out_c_56203\n",
      "15012    0.000970                    out_c_99682\n",
      "13136    0.000970                     out_c_3963\n",
      "14733    0.000969                    out_c_78905\n",
      "14401    0.000962                    out_c_72283\n",
      "13571    0.000953                     out_c_4919\n",
      "13081    0.000953                    out_c_36634\n",
      "13258    0.000944                    out_c_42840\n",
      "14921    0.000934                     out_c_8489\n",
      "11774    0.000933                       in_c_459\n",
      "13079    0.000932                    out_c_36632\n",
      "14408    0.000932                     out_c_7232\n",
      "15002    0.000930                     out_c_9961\n",
      "13876    0.000926                     out_c_5940\n",
      "12955    0.000925                    out_c_34663\n",
      "14097    0.000922                    out_c_71511\n",
      "14548    0.000919                    out_c_73300\n",
      "14762    0.000916                    out_c_78964\n",
      "13698    0.000916                    out_c_55091\n",
      "11966    0.000906                       in_c_708\n",
      "13705    0.000903                    out_c_55202\n",
      "13575    0.000902                    out_c_49301\n",
      "14619    0.000898                     out_c_7819\n",
      "12290    0.000889                    out_c_20302\n",
      "13605    0.000887                     out_c_5161\n",
      "14732    0.000885                    out_c_78904\n",
      "14397    0.000879                    out_c_72273\n",
      "14406    0.000879                     out_c_7230\n",
      "15356    0.000878                    out_c_V5861\n",
      "12516    0.000867                     out_c_2682\n",
      "13566    0.000864                     out_c_4911\n",
      "12298    0.000864                    out_c_20914\n",
      "13256    0.000850                    out_c_42832\n",
      "13065    0.000842                    out_c_36612\n",
      "13235    0.000839                     out_c_4271\n",
      "14417    0.000836                    out_c_72401\n",
      "13992    0.000831                     out_c_7012\n",
      "13062    0.000830                    out_c_36609\n",
      "13347    0.000824                     out_c_4408\n",
      "12271    0.000820                     out_c_1884\n",
      "13752    0.000820                    out_c_56213\n",
      "15004    0.000818                    out_c_99645\n",
      "15102    0.000818                    out_c_V1051\n",
      "12100    0.000816                       in_c_905\n",
      "15125    0.000815                    out_c_V1301\n",
      "13913    0.000813                    out_c_60091\n",
      "13348    0.000812                     out_c_4409\n",
      "13852    0.000812                     out_c_5902\n",
      "14766    0.000809                    out_c_78969\n",
      "13879    0.000809                     out_c_5948\n",
      "13187    0.000808                    out_c_41402\n",
      "15013    0.000806                    out_c_99685\n",
      "13746    0.000805                    out_c_56201\n",
      "15003    0.000804                    out_c_99630\n",
      "13908    0.000802                    out_c_60011\n",
      "12859    0.000802                     out_c_3310\n",
      "13739    0.000800                     out_c_5589\n",
      "14091    0.000800                    out_c_71489\n",
      "13824    0.000793                     out_c_5794\n",
      "13303    0.000788                     out_c_4359\n",
      "14652    0.000788                     out_c_7850\n",
      "11886    0.000786                       in_c_598\n",
      "13328    0.000780                     out_c_4386\n",
      "13299    0.000778                     out_c_4351\n",
      "12576    0.000773                     out_c_2801\n",
      "13078    0.000773                    out_c_36631\n",
      "14087    0.000772                    out_c_71432\n",
      "11536    0.000772                       in_c_152\n",
      "13342    0.000771                    out_c_44023\n",
      "13612    0.000768                     out_c_5178\n",
      "13697    0.000766                    out_c_55090\n",
      "14375    0.000759                    out_c_72142\n",
      "13808    0.000758                     out_c_5761\n",
      "14428    0.000757                    out_c_72479\n",
      "13730    0.000757                     out_c_5566\n",
      "13801    0.000757                    out_c_57480\n",
      "14750    0.000756                    out_c_78942\n",
      "13859    0.000756                     out_c_5921\n",
      "12739    0.000755                    out_c_29614\n",
      "14398    0.000755                    out_c_72280\n",
      "15205    0.000753                     out_c_V431\n",
      "12580    0.000751                     out_c_2811\n",
      "14082    0.000750                     out_c_7140\n",
      "13230    0.000749                    out_c_42681\n",
      "13693    0.000748                    out_c_55003\n",
      "13082    0.000748                    out_c_36641\n",
      "14411    0.000745                     out_c_7235\n",
      "12225    0.000745                     out_c_1579\n",
      "13910    0.000744                    out_c_60021\n",
      "12747    0.000740                    out_c_29625\n",
      "13087    0.000726                    out_c_36646\n",
      "14409    0.000716                     out_c_7233\n",
      "14550    0.000713                    out_c_73302\n",
      "12006    0.000713                       in_c_757\n",
      "14125    0.000712                    out_c_71590\n",
      "13602    0.000709                      out_c_514\n",
      "12529    0.000708                     out_c_2720\n",
      "12264    0.000707                     out_c_1828\n",
      "15100    0.000705                    out_c_V1043\n",
      "15057    0.000704                    out_c_V0382\n",
      "13242    0.000703                    out_c_42760\n",
      "14395    0.000701                    out_c_72271\n",
      "15374    0.000701                    out_c_V5883\n",
      "14366    0.000701                     out_c_7202\n",
      "15236    0.000700                    out_c_V4509\n",
      "14384    0.000699                    out_c_72211\n",
      "14519    0.000696                    out_c_72930\n",
      "14415    0.000696                     out_c_7239\n",
      "14794    0.000695                     out_c_7934\n",
      "15014    0.000693                    out_c_99690\n",
      "13589    0.000691                     out_c_4941\n",
      "13060    0.000688                    out_c_36603\n",
      "14591    0.000686                     out_c_7804\n",
      "13241    0.000682                     out_c_4275\n",
      "11740    0.000681                       in_c_413\n",
      "13905    0.000681                    out_c_60000\n",
      "11675    0.000681                       in_c_330\n",
      "13231    0.000680                    out_c_42682\n",
      "12605    0.000680                    out_c_28489\n",
      "13882    0.000680                     out_c_5951\n",
      "13239    0.000679                    out_c_42741\n",
      "14368    0.000674                    out_c_72089\n",
      "13278    0.000671                     out_c_4321\n",
      "14109    0.000669                    out_c_71524\n",
      "13260    0.000668                    out_c_42842\n",
      "14377    0.000667                     out_c_7216\n",
      "14765    0.000667                    out_c_78967\n",
      "14319    0.000667                    out_c_71946\n",
      "13424    0.000666                     out_c_4542\n",
      "12443    0.000666                    out_c_24990\n",
      "13292    0.000665                    out_c_43400\n",
      "14808    0.000665                    out_c_79412\n",
      "12591    0.000664                     out_c_2827\n",
      "12586    0.000664                     out_c_2820\n",
      "14857    0.000663                     out_c_8402\n",
      "12376    0.000662                     out_c_2390\n",
      "14414    0.000661                     out_c_7238\n",
      "12362    0.000658                     out_c_2382\n",
      "14682    0.000655                    out_c_78703\n",
      "14681    0.000653                    out_c_78702\n",
      "13572    0.000651                     out_c_4920\n",
      "13607    0.000648                     out_c_5163\n",
      "13866    0.000647                     out_c_5935\n",
      "14416    0.000647                    out_c_72400\n",
      "13894    0.000646                    out_c_59800\n",
      "12584    0.000646                     out_c_2818\n",
      "14403    0.000644                    out_c_72291\n",
      "14371    0.000643                     out_c_7211\n",
      "12329    0.000640                     out_c_2337\n",
      "13604    0.000636                     out_c_5160\n",
      "14878    0.000636                    out_c_84213\n",
      "14396    0.000632                    out_c_72272\n",
      "14424    0.000631                     out_c_7245\n",
      "13154    0.000630                    out_c_40310\n",
      "13160    0.000630                    out_c_40402\n",
      "12579    0.000630                     out_c_2810\n",
      "12851    0.000629                    out_c_32735\n",
      "15119    0.000626                    out_c_V1261\n",
      "15213    0.000626                    out_c_V4362\n",
      "13345    0.000626                    out_c_44031\n",
      "12248    0.000624                     out_c_1738\n",
      "14600    0.000622                    out_c_78061\n",
      "12589    0.000622                     out_c_2823\n",
      "12249    0.000621                     out_c_1739\n",
      "12748    0.000620                    out_c_29626\n",
      "11978    0.000619                       in_c_723\n",
      "14771    0.000617                    out_c_79029\n",
      "14450    0.000616                    out_c_72665\n",
      "14791    0.000613                     out_c_7931\n",
      "13386    0.000612                     out_c_4464\n",
      "14695    0.000610                    out_c_78791\n",
      "13074    0.000610                    out_c_36621\n",
      "14532    0.000609                    out_c_72990\n",
      "13634    0.000608                     out_c_5303\n",
      "14667    0.000607                    out_c_78609\n",
      "12439    0.000607                    out_c_24970\n",
      "12014    0.000606                       in_c_769\n",
      "14680    0.000605                    out_c_78701\n",
      "12588    0.000601                     out_c_2822\n",
      "12592    0.000599                     out_c_2828\n",
      "14457    0.000599                     out_c_7268\n",
      "13800    0.000596                    out_c_57471\n",
      "14032    0.000596                    out_c_70701\n",
      "14394    0.000596                    out_c_72270\n",
      "14552    0.000596                    out_c_73309\n",
      "13097    0.000595                    out_c_38603\n",
      "12201    0.000593                     out_c_1530\n",
      "12825    0.000592                      out_c_311\n",
      "12587    0.000590                     out_c_2821\n",
      "13936    0.000587                     out_c_6119\n",
      "12480    0.000587                    out_c_25083\n",
      "12603    0.000585                     out_c_2842\n",
      "13088    0.000583                    out_c_36650\n",
      "12335    0.000580                     out_c_2356\n",
      "12577    0.000579                     out_c_2808\n",
      "13623    0.000578                     out_c_5194\n",
      "13465    0.000574                    out_c_45933\n",
      "11535    0.000570                       in_c_151\n",
      "14412    0.000565                     out_c_7236\n",
      "13229    0.000564                     out_c_4267\n",
      "13592    0.000564                     out_c_5100\n",
      "13907    0.000563                    out_c_60010\n",
      "14378    0.000561                     out_c_7217\n",
      "13381    0.000561                     out_c_4461\n",
      "14458    0.000558                    out_c_72690\n",
      "13617    0.000556                     out_c_5184\n",
      "13255    0.000555                    out_c_42831\n",
      "12594    0.000554                     out_c_2830\n",
      "14441    0.000553                    out_c_72633\n",
      "15422    0.000552                     out_c_V717\n",
      "12300    0.000551                    out_c_20916\n",
      "12145    0.000551                       in_c_977\n",
      "12611    0.000547                    out_c_28529\n",
      "13912    0.000547                    out_c_60090\n",
      "14388    0.000546                    out_c_72232\n",
      "12952    0.000545                    out_c_34660\n",
      "12553    0.000545                     out_c_2763\n",
      "14068    0.000540                     out_c_7103\n",
      "13850    0.000540                    out_c_59010\n",
      "14083    0.000539                     out_c_7141\n",
      "12734    0.000536                    out_c_29606\n",
      "13929    0.000535                    out_c_61171\n",
      "14385    0.000534                     out_c_7222\n",
      "12530    0.000533                     out_c_2721\n",
      "13625    0.000532                     out_c_5199\n",
      "15314    0.000531                     out_c_V538\n",
      "13275    0.000526                      out_c_430\n",
      "13024    0.000525                     out_c_3571\n",
      "13627    0.000525                    out_c_53010\n",
      "14660    0.000523                    out_c_78601\n",
      "13890    0.000523                     out_c_5970\n",
      "15008    0.000521                    out_c_99669\n",
      "13240    0.000518                    out_c_42742\n",
      "13092    0.000517                     out_c_3668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# BEST MODEL FEATURE IMPORTANCES\n",
    "\n",
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "\n",
    "# training and testing sets\n",
    "X_sample, X_test, y_sample, y_test = \\\n",
    "train_test_split(X, y, random_state = 42, stratify=y)\n",
    "X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "\n",
    "\n",
    "bestmodel = LogisticRegression(C=3, class_weight='balanced', dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
    "                   random_state=30, solver='saga', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "bestmodel.fit(X_train, y_train)\n",
    "\n",
    "feature_frame = pd.DataFrame({'Importance':i for i in bestmodel.coef_})\n",
    "st_dv = [i for i in np.std(X_train, 0)]\n",
    "feature_array = feature_frame['Importance'].values * st_dv\n",
    "feature_frame['Importance'] = feature_array\n",
    "feature_frame['Variable'] = X.columns\n",
    "\n",
    "pd.set_option('display.max_rows', 1001)\n",
    "print(feature_frame.sort_values('Importance', ascending=False).head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1652a630>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(feature_frame['Importance'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
