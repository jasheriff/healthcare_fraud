{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest & XGB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_general(df, target, method):\n",
    "    # baseline logistic regression. \n",
    "    # penalty = 'l2', ridge. , solver = 'liblinear'\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    # training and testing sets\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42, stratify=y)\n",
    "\n",
    "    # Instantiate model\n",
    "    rf = RandomForestClassifier(oob_score=True, random_state=30)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Out of Bag Error\n",
    "    # oob_score = rf.oob_score_ # score  = 1- oob error\n",
    "    print('OOB Score: %.2f' % rf.oob_score_ )\n",
    "    \n",
    "    # Scores\n",
    "    y_predict_test = rf.predict(X_test)\n",
    "    print(\"Test accuracy score:\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "     \n",
    "def random_forest_tuned(df, target, method):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    # training and testing sets\n",
    "    X_sample, X_test, y_sample, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42, stratify=y)\n",
    "    \n",
    "    # resampling methods\n",
    "    # sampling strategy = (ratio : minority / majority) could be 'auto' or 1. could be something else.\n",
    "    if method == 'ros':\n",
    "        X_train, y_train = RandomOverSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'ADASYN':\n",
    "        X_train, y_train = ADASYN(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'SMOTE':\n",
    "        X_train, y_train = SMOTE(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'rus':\n",
    "        X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'cc':\n",
    "        X_train, y_train = ClusterCentroids(random_state=0).fit_resample(X_sample, y_sample)   \n",
    "    if method == 'NearMiss':\n",
    "        X_train, y_train = NearMiss(version=1, random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'none':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "    \n",
    "    # Instantiate model\n",
    "    rf = RandomForestClassifier(oob_score=True, random_state=30)\n",
    "\n",
    "    #Parameters\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 400, stop = 700, num = 4)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features} \n",
    "    \n",
    "    # search across different combinations of parameters, and use all available scores\n",
    "    rf_tuned = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 3, random_state=42)\n",
    "    \n",
    "    # Train the model on training data\n",
    "    rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "    # Scores\n",
    "    y_predict_test = rf_tuned.predict(X_test)\n",
    "    print(\"Test accuracy score:\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "    # Best Parameters:\n",
    "    print(\"Best Parameters:\", rf_tuned.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Models\n",
    "##### Key models  \n",
    "\n",
    "* Random Forest increased the best model's Class 1 Recall by .06 points to .81, with a marginal decrease in accuracy. Class 1 Precision dropped by .09, so with the gain in recall, we also had more observations incorrectly identified as fraud.\n",
    "\n",
    "type, undersampling method | Class 1 Precision | Class 1 Recall | Accuracy Score | \n",
    "------- | ------------ | -------------- | --------------- | \n",
    "BEST: random forest, random under sampling | 0.37 | .81 | .853 | \n",
    "general random forest | .77 | .32 | .928  | .928 |\n",
    "xg_boost |.70 | .45 | .93 |\n",
    "* lasso cv, random under sampling | 0.46 | 0.75 | 0.885\n",
    "* ridge cv, random under sampling | 0.46 | 0.75 | 0.885\n",
    "\n",
    "\n",
    "\n",
    "\"*\" = best prior model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_data = pd.read_csv('/Users/Julia/Documents/bootcamp/fraud_capstone/data_out/train_final_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChronicCond_Alzheimer</th>\n",
       "      <th>ChronicCond_Cancer</th>\n",
       "      <th>ChronicCond_Depression</th>\n",
       "      <th>ChronicCond_Diabetes</th>\n",
       "      <th>ChronicCond_Heartfailure</th>\n",
       "      <th>ChronicCond_IschemicHeart</th>\n",
       "      <th>ChronicCond_KidneyDisease</th>\n",
       "      <th>ChronicCond_ObstrPulmonary</th>\n",
       "      <th>ChronicCond_Osteoporasis</th>\n",
       "      <th>ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>ChronicCond_stroke</th>\n",
       "      <th>County_0</th>\n",
       "      <th>County_1</th>\n",
       "      <th>County_10</th>\n",
       "      <th>County_100</th>\n",
       "      <th>County_11</th>\n",
       "      <th>County_110</th>\n",
       "      <th>County_111</th>\n",
       "      <th>County_113</th>\n",
       "      <th>County_117</th>\n",
       "      <th>County_120</th>\n",
       "      <th>County_130</th>\n",
       "      <th>County_131</th>\n",
       "      <th>County_14</th>\n",
       "      <th>County_140</th>\n",
       "      <th>County_141</th>\n",
       "      <th>County_150</th>\n",
       "      <th>County_160</th>\n",
       "      <th>County_161</th>\n",
       "      <th>County_170</th>\n",
       "      <th>County_180</th>\n",
       "      <th>County_190</th>\n",
       "      <th>County_191</th>\n",
       "      <th>County_194</th>\n",
       "      <th>County_20</th>\n",
       "      <th>County_200</th>\n",
       "      <th>County_210</th>\n",
       "      <th>County_211</th>\n",
       "      <th>County_212</th>\n",
       "      <th>County_213</th>\n",
       "      <th>County_220</th>\n",
       "      <th>County_221</th>\n",
       "      <th>County_222</th>\n",
       "      <th>County_223</th>\n",
       "      <th>County_224</th>\n",
       "      <th>County_230</th>\n",
       "      <th>County_240</th>\n",
       "      <th>County_241</th>\n",
       "      <th>County_25</th>\n",
       "      <th>County_250</th>\n",
       "      <th>...</th>\n",
       "      <th>proc_9764.0</th>\n",
       "      <th>proc_9784.0</th>\n",
       "      <th>proc_9787.0</th>\n",
       "      <th>proc_9789.0</th>\n",
       "      <th>proc_9805.0</th>\n",
       "      <th>proc_9815.0</th>\n",
       "      <th>proc_9851.0</th>\n",
       "      <th>proc_9903.0</th>\n",
       "      <th>proc_9904.0</th>\n",
       "      <th>proc_9905.0</th>\n",
       "      <th>proc_9906.0</th>\n",
       "      <th>proc_9907.0</th>\n",
       "      <th>proc_9910.0</th>\n",
       "      <th>proc_9914.0</th>\n",
       "      <th>proc_9915.0</th>\n",
       "      <th>proc_9916.0</th>\n",
       "      <th>proc_9917.0</th>\n",
       "      <th>proc_9918.0</th>\n",
       "      <th>proc_9919.0</th>\n",
       "      <th>proc_9920.0</th>\n",
       "      <th>proc_9921.0</th>\n",
       "      <th>proc_9922.0</th>\n",
       "      <th>proc_9923.0</th>\n",
       "      <th>proc_9925.0</th>\n",
       "      <th>proc_9926.0</th>\n",
       "      <th>proc_9928.0</th>\n",
       "      <th>proc_9929.0</th>\n",
       "      <th>proc_9938.0</th>\n",
       "      <th>proc_9939.0</th>\n",
       "      <th>proc_9952.0</th>\n",
       "      <th>proc_9955.0</th>\n",
       "      <th>proc_9959.0</th>\n",
       "      <th>proc_9960.0</th>\n",
       "      <th>proc_9961.0</th>\n",
       "      <th>proc_9962.0</th>\n",
       "      <th>proc_9969.0</th>\n",
       "      <th>proc_9971.0</th>\n",
       "      <th>proc_9972.0</th>\n",
       "      <th>proc_9973.0</th>\n",
       "      <th>proc_9974.0</th>\n",
       "      <th>proc_9975.0</th>\n",
       "      <th>proc_9978.0</th>\n",
       "      <th>proc_9979.0</th>\n",
       "      <th>proc_9982.0</th>\n",
       "      <th>proc_9984.0</th>\n",
       "      <th>proc_9986.0</th>\n",
       "      <th>proc_9992.0</th>\n",
       "      <th>proc_9995.0</th>\n",
       "      <th>proc_9998.0</th>\n",
       "      <th>proc_9999.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.365759</td>\n",
       "      <td>0.233463</td>\n",
       "      <td>0.451362</td>\n",
       "      <td>0.754864</td>\n",
       "      <td>0.564202</td>\n",
       "      <td>0.762646</td>\n",
       "      <td>0.474708</td>\n",
       "      <td>0.400778</td>\n",
       "      <td>0.272374</td>\n",
       "      <td>0.330739</td>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.426901</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.730994</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.380117</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.345029</td>\n",
       "      <td>0.076023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429515</td>\n",
       "      <td>0.229075</td>\n",
       "      <td>0.451542</td>\n",
       "      <td>0.685022</td>\n",
       "      <td>0.596916</td>\n",
       "      <td>0.799559</td>\n",
       "      <td>0.398678</td>\n",
       "      <td>0.341410</td>\n",
       "      <td>0.370044</td>\n",
       "      <td>0.290749</td>\n",
       "      <td>0.063877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.496454</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.773050</td>\n",
       "      <td>0.624113</td>\n",
       "      <td>0.794326</td>\n",
       "      <td>0.460993</td>\n",
       "      <td>0.304965</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.099291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322917</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 16888 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ChronicCond_Alzheimer  ChronicCond_Cancer  ChronicCond_Depression  ChronicCond_Diabetes  ChronicCond_Heartfailure  ChronicCond_IschemicHeart  ChronicCond_KidneyDisease  ChronicCond_ObstrPulmonary  ChronicCond_Osteoporasis  ChronicCond_rheumatoidarthritis  ChronicCond_stroke  County_0  County_1  County_10  County_100  County_11  County_110  County_111  County_113  County_117  County_120  County_130  County_131  County_14  County_140  County_141  County_150  County_160  County_161  \\\n",
       "0               0.365759            0.233463                0.451362              0.754864                  0.564202                   0.762646                   0.474708                    0.400778                  0.272374                         0.330739            0.105058  0.011673       0.0   0.011673    0.011673        0.0         0.0         0.0         0.0         0.0         0.0    0.015564         0.0        0.0    0.003891         0.0     0.07393    0.000000         0.0   \n",
       "1               0.426901            0.175439                0.444444              0.730994                  0.649123                   0.807018                   0.473684                    0.380117                  0.280702                         0.345029            0.076023  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.023392         0.0        0.0    0.005848         0.0     0.00000    0.000000         0.0   \n",
       "2               0.429515            0.229075                0.451542              0.685022                  0.596916                   0.799559                   0.398678                    0.341410                  0.370044                         0.290749            0.063877  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.000000         0.0   \n",
       "3               0.496454            0.191489                0.446809              0.773050                  0.624113                   0.794326                   0.460993                    0.304965                  0.326241                         0.326241            0.099291  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.014184         0.0   \n",
       "4               0.322917            0.156250                0.385417              0.645833                  0.645833                   0.687500                   0.395833                    0.302083                  0.291667                         0.270833            0.104167  0.000000       0.0   0.031250    0.031250        0.0         0.0         0.0         0.0         0.0         0.0    0.020833         0.0        0.0    0.000000         0.0     0.00000    0.135417         0.0   \n",
       "\n",
       "   County_170  County_180  County_190  County_191  County_194  County_20  County_200  County_210  County_211  County_212  County_213  County_220  County_221  County_222  County_223  County_224  County_230  County_240  County_241  County_25  County_250     ...       proc_9764.0  proc_9784.0  proc_9787.0  proc_9789.0  proc_9805.0  proc_9815.0  proc_9851.0  proc_9903.0  proc_9904.0  proc_9905.0  proc_9906.0  proc_9907.0  proc_9910.0  proc_9914.0  proc_9915.0  proc_9916.0  proc_9917.0  \\\n",
       "0         0.0    0.003891    0.011673         0.0         0.0   0.003891         0.0    0.000000         0.0         0.0         0.0    0.011673         0.0         0.0         0.0         0.0    0.007782    0.011673         0.0        0.0    0.054475     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "1         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.070175         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.070175         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          2.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "2         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.000000         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.156388         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "3         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.014184         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "4         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.010417         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   proc_9918.0  proc_9919.0  proc_9920.0  proc_9921.0  proc_9922.0  proc_9923.0  proc_9925.0  proc_9926.0  proc_9928.0  proc_9929.0  proc_9938.0  proc_9939.0  proc_9952.0  proc_9955.0  proc_9959.0  proc_9960.0  proc_9961.0  proc_9962.0  proc_9969.0  proc_9971.0  proc_9972.0  proc_9973.0  proc_9974.0  proc_9975.0  proc_9978.0  proc_9979.0  proc_9982.0  proc_9984.0  proc_9986.0  proc_9992.0  proc_9995.0  proc_9998.0  proc_9999.0  \n",
       "0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "1          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "2          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "3          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "4          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "\n",
       "[5 rows x 16888 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.92\n",
      "Test accuracy score: 0.928\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      1226\n",
      "           1       0.77      0.32      0.46       127\n",
      "\n",
      "    accuracy                           0.93      1353\n",
      "   macro avg       0.85      0.66      0.71      1353\n",
      "weighted avg       0.92      0.93      0.91      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest_general(train_final_data, 'PotentialFraud', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.928\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      1216\n",
      "           1       0.79      0.39      0.53       137\n",
      "\n",
      "    accuracy                           0.93      1353\n",
      "   macro avg       0.86      0.69      0.74      1353\n",
      "weighted avg       0.92      0.93      0.92      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=700,\n",
      "                       n_jobs=None, oob_score=True, random_state=30, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.853\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91      1226\n",
      "           1       0.37      0.81      0.51       127\n",
      "\n",
      "    accuracy                           0.85      1353\n",
      "   macro avg       0.67      0.83      0.71      1353\n",
      "weighted avg       0.92      0.85      0.88      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=600,\n",
      "                       n_jobs=None, oob_score=True, random_state=30, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.872\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      1226\n",
      "           1       0.38      0.54      0.44       127\n",
      "\n",
      "    accuracy                           0.87      1353\n",
      "   macro avg       0.66      0.72      0.69      1353\n",
      "weighted avg       0.90      0.87      0.88      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=600,\n",
      "                       n_jobs=None, oob_score=True, random_state=30, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.635\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.60      0.75      1226\n",
      "           1       0.20      0.97      0.33       127\n",
      "\n",
      "    accuracy                           0.63      1353\n",
      "   macro avg       0.60      0.78      0.54      1353\n",
      "weighted avg       0.92      0.63      0.71      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
      "                       n_jobs=None, oob_score=True, random_state=30, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'NearMiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Author: Kian Ho <hui.kian.ho@gmail.com>\n",
    "#         Gilles Louppe <g.louppe@gmail.com>\n",
    "#         Andreas Mueller <amueller@ais.uni-bonn.de>\n",
    "#\n",
    "# License: BSD 3 Clause\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "# Generate a binary classification dataset.\n",
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "    \n",
    "# NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
    "# support for parallelized ensembles but is necessary for tracking the OOB\n",
    "# error trajectory during training.\n",
    "ensemble_clfs = [\n",
    "    (\"RandomForestClassifier, max_features='sqrt'\",\n",
    "        RandomForestClassifier(n_estimators=200,\n",
    "                               warm_start=True, max_features=\"sqrt\",\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features='log2'\",\n",
    "        RandomForestClassifier(n_estimators=200,\n",
    "                               warm_start=True, max_features='log2',\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features=None\",\n",
    "        RandomForestClassifier(n_estimators=200,\n",
    "                               warm_start=True, max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 10\n",
    "max_estimators = 400\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(min_estimators, max_estimators + 1, 5):\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))\n",
    "\n",
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "sns.set()\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model above: OOB error settles around 200:\n",
    "    * 150 for max_features = 'log2'\n",
    "    * 180 for max_features = 'sqrt'\n",
    "    * 300 for max_features = 'None'\n",
    "* Our best tuned models had 400-600 estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.853\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>diag_40390</td>\n",
       "      <td>0.011210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Mean_Duration</td>\n",
       "      <td>0.009751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5225</th>\n",
       "      <td>diag_53081</td>\n",
       "      <td>0.008454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>diag_2449</td>\n",
       "      <td>0.008196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>diag_2762</td>\n",
       "      <td>0.008081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>diag_41401</td>\n",
       "      <td>0.007904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Median_Duration</td>\n",
       "      <td>0.007530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>diag_2851</td>\n",
       "      <td>0.007385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4527</th>\n",
       "      <td>diag_4280</td>\n",
       "      <td>0.007379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>diag_2724</td>\n",
       "      <td>0.007350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>Mean_InscClaimAmtReimbursed</td>\n",
       "      <td>0.007181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Mean_In_Out</td>\n",
       "      <td>0.006942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>diag_42731</td>\n",
       "      <td>0.006856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5613</th>\n",
       "      <td>diag_5859</td>\n",
       "      <td>0.006832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5691</th>\n",
       "      <td>diag_5990</td>\n",
       "      <td>0.006572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>diag_486</td>\n",
       "      <td>0.006570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>diag_51881</td>\n",
       "      <td>0.006315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>diag_5180</td>\n",
       "      <td>0.006228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>diag_2768</td>\n",
       "      <td>0.006023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>diag_73300</td>\n",
       "      <td>0.005966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>diag_5849</td>\n",
       "      <td>0.005965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>diag_25000</td>\n",
       "      <td>0.005632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>diag_27651</td>\n",
       "      <td>0.005275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>diag_2639</td>\n",
       "      <td>0.005244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>diag_496</td>\n",
       "      <td>0.005164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>diag_41400</td>\n",
       "      <td>0.005081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>diag_40391</td>\n",
       "      <td>0.005024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>diag_3051</td>\n",
       "      <td>0.005012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>diag_2859</td>\n",
       "      <td>0.004803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>diag_41071</td>\n",
       "      <td>0.004794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6614</th>\n",
       "      <td>diag_71536</td>\n",
       "      <td>0.002520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>diag_2720</td>\n",
       "      <td>0.002477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16298</th>\n",
       "      <td>proc_66.0</td>\n",
       "      <td>0.002423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>diag_2767</td>\n",
       "      <td>0.002404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>diag_29680</td>\n",
       "      <td>0.002356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ChronicCond_KidneyDisease</td>\n",
       "      <td>0.002351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>diag_2760</td>\n",
       "      <td>0.002340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>diag_34590</td>\n",
       "      <td>0.002339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4517</th>\n",
       "      <td>diag_42732</td>\n",
       "      <td>0.002330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10907</th>\n",
       "      <td>diag_V4581</td>\n",
       "      <td>0.002323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11074</th>\n",
       "      <td>diag_V5867</td>\n",
       "      <td>0.002321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7664</th>\n",
       "      <td>diag_78057</td>\n",
       "      <td>0.002284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>diag_1629</td>\n",
       "      <td>0.002215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6970</th>\n",
       "      <td>diag_72402</td>\n",
       "      <td>0.002214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>diag_V4501</td>\n",
       "      <td>0.002202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>diag_28521</td>\n",
       "      <td>0.002201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4427</th>\n",
       "      <td>diag_412</td>\n",
       "      <td>0.002187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>diag_2749</td>\n",
       "      <td>0.002160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>RenalDiseaseIndicator</td>\n",
       "      <td>0.002140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10629</th>\n",
       "      <td>diag_V1254</td>\n",
       "      <td>0.002131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>diag_2948</td>\n",
       "      <td>0.002118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>diag_4439</td>\n",
       "      <td>0.002091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>diag_41519</td>\n",
       "      <td>0.002085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437</th>\n",
       "      <td>diag_56400</td>\n",
       "      <td>0.002073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>diag_43491</td>\n",
       "      <td>0.001992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6576</th>\n",
       "      <td>diag_7140</td>\n",
       "      <td>0.001988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10921</th>\n",
       "      <td>diag_V462</td>\n",
       "      <td>0.001971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7652</th>\n",
       "      <td>diag_7802</td>\n",
       "      <td>0.001962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>diag_4241</td>\n",
       "      <td>0.001943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7655</th>\n",
       "      <td>diag_78039</td>\n",
       "      <td>0.001931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Variable  Importance\n",
       "4365                    diag_40390    0.011210\n",
       "330                  Mean_Duration    0.009751\n",
       "5225                    diag_53081    0.008454\n",
       "2096                     diag_2449    0.008196\n",
       "2333                     diag_2762    0.008081\n",
       "4432                    diag_41401    0.007904\n",
       "339                Median_Duration    0.007530\n",
       "2439                     diag_2851    0.007385\n",
       "4527                     diag_4280    0.007379\n",
       "2299                     diag_2724    0.007350\n",
       "335    Mean_InscClaimAmtReimbursed    0.007181\n",
       "334                    Mean_In_Out    0.006942\n",
       "4516                    diag_42731    0.006856\n",
       "5613                     diag_5859    0.006832\n",
       "5691                     diag_5990    0.006572\n",
       "4891                      diag_486    0.006570\n",
       "4980                    diag_51881    0.006315\n",
       "4972                     diag_5180    0.006228\n",
       "2341                     diag_2768    0.006023\n",
       "7171                    diag_73300    0.005966\n",
       "5606                     diag_5849    0.005965\n",
       "2130                    diag_25000    0.005632\n",
       "2337                    diag_27651    0.005275\n",
       "2249                     diag_2639    0.005244\n",
       "4930                      diag_496    0.005164\n",
       "4431                    diag_41400    0.005081\n",
       "4366                    diag_40391    0.005024\n",
       "2814                     diag_3051    0.005012\n",
       "2444                     diag_2859    0.004803\n",
       "4414                    diag_41071    0.004794\n",
       "...                            ...         ...\n",
       "6614                    diag_71536    0.002520\n",
       "2295                     diag_2720    0.002477\n",
       "16298                    proc_66.0    0.002423\n",
       "2340                     diag_2767    0.002404\n",
       "2665                    diag_29680    0.002356\n",
       "6        ChronicCond_KidneyDisease    0.002351\n",
       "2331                     diag_2760    0.002340\n",
       "3212                    diag_34590    0.002339\n",
       "4517                    diag_42732    0.002330\n",
       "10907                   diag_V4581    0.002323\n",
       "11074                   diag_V5867    0.002321\n",
       "7664                    diag_78057    0.002284\n",
       "1298                     diag_1629    0.002215\n",
       "6970                    diag_72402    0.002214\n",
       "10885                   diag_V4501    0.002202\n",
       "2440                    diag_28521    0.002201\n",
       "4427                      diag_412    0.002187\n",
       "2319                     diag_2749    0.002160\n",
       "355          RenalDiseaseIndicator    0.002140\n",
       "10629                   diag_V1254    0.002131\n",
       "2553                     diag_2948    0.002118\n",
       "4669                     diag_4439    0.002091\n",
       "4450                    diag_41519    0.002085\n",
       "5437                    diag_56400    0.002073\n",
       "4578                    diag_43491    0.001992\n",
       "6576                     diag_7140    0.001988\n",
       "10921                    diag_V462    0.001971\n",
       "7652                     diag_7802    0.001962\n",
       "4479                     diag_4241    0.001943\n",
       "7655                    diag_78039    0.001931\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FEATURE IMPORTANCES FOR BEST RUS RANDOM FOREST MODEL \n",
    "\n",
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "\n",
    "X_sample, X_test, y_sample, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Instantiate\n",
    "model=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=600,\n",
    "                       n_jobs=None, oob_score=True, random_state=30, verbose=0,\n",
    "                       warm_start=False)\n",
    "\n",
    "# Undersampling \n",
    "X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "y_predict_test = model.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n Test Classification Report:\")\n",
    "print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "# Top feature Importances    \n",
    "pd.set_option('display.max_rows', 100)\n",
    "(pd.DataFrame({'Variable':X.columns, 'Importance':model.feature_importances_}).sort_values('Importance', ascending=False)).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING WITH XGB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy:\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1216\n",
      "           1       0.67      0.45      0.54       137\n",
      "\n",
      "    accuracy                           0.92      1353\n",
      "   macro avg       0.81      0.71      0.75      1353\n",
      "weighted avg       0.91      0.92      0.92      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BASELINE:\n",
    "\n",
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# fit model \n",
    "model = XGBClassifier(seed=30)\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "print(\"\\n Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNED WITHOUT RESAMPLING\n",
    "def xgb_tuned(df, target):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    # Instantiate model\n",
    "    model = XGBClassifier(random_state=30)\n",
    "\n",
    "    #Parameters\n",
    "    cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
    "    ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "    \n",
    "    # Optimize for accuracy since that is the metric used in the Adult Data Set notation\n",
    "    model_tuned = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                    cv_params, scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "    model_tuned.fit(X_train, y_train)\n",
    "                       \n",
    "    # make predictions for test data\n",
    "    y_pred = model_tuned.predict(X_test)\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Best Parameters:\n",
    "    print(\"Best Parameters\", model_tuned.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy:\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1216\n",
      "           1       0.70      0.45      0.55       137\n",
      "\n",
      "    accuracy                           0.93      1353\n",
      "   macro avg       0.82      0.72      0.76      1353\n",
      "weighted avg       0.92      0.93      0.92      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_tuned(train_final_data, \"PotentialFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNED WITH RANDOM UNDER SAMPLING \n",
    "def xgb_rus(df, target):\n",
    "    X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "    y = train_final_data[\"PotentialFraud\"]\n",
    "\n",
    "    X_sample, X_test, y_sample, y_test = \\\n",
    "        train_test_split(X, y, random_state = 42, stratify=y)\n",
    "    X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    X_train_df = pd.DataFrame(data=X_train, columns=X_test.columns)\n",
    "    \n",
    "    # Instantiate model\n",
    "    model = XGBClassifier(random_state=30)\n",
    "\n",
    "    #Parameters\n",
    "    cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
    "    ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "    \n",
    "    # Optimize for accuracy since that is the metric used in the Adult Data Set notation\n",
    "    model_tuned = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                    cv_params, scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "    model_tuned.fit(X_train_df, y_train)\n",
    "                       \n",
    "    # make predictions for test data\n",
    "    y_pred = model_tuned.predict(X_test)\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Best Parameters:\n",
    "    print(\"Best Parameters:\", model_tuned.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92      1226\n",
      "           1       0.39      0.87      0.54       127\n",
      "\n",
      "    accuracy                           0.86      1353\n",
      "   macro avg       0.69      0.86      0.73      1353\n",
      "weighted avg       0.93      0.86      0.88      1353\n",
      "\n",
      "Best Parameters: {'max_depth': 3, 'min_child_weight': 1}\n"
     ]
    }
   ],
   "source": [
    "xgb_rus(train_final_data, \"PotentialFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92      1226\n",
      "           1       0.39      0.87      0.54       127\n",
      "\n",
      "    accuracy                           0.86      1353\n",
      "   macro avg       0.69      0.86      0.73      1353\n",
      "weighted avg       0.93      0.86      0.88      1353\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>diag_41401</td>\n",
       "      <td>0.066810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>diag_40390</td>\n",
       "      <td>0.044962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5691</th>\n",
       "      <td>diag_5990</td>\n",
       "      <td>0.028702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4527</th>\n",
       "      <td>diag_4280</td>\n",
       "      <td>0.024929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4446</th>\n",
       "      <td>diag_4149</td>\n",
       "      <td>0.020869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11068</th>\n",
       "      <td>diag_V5861</td>\n",
       "      <td>0.017123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>diag_2768</td>\n",
       "      <td>0.016310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11316</th>\n",
       "      <td>diag_V7651</td>\n",
       "      <td>0.012111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10334</th>\n",
       "      <td>diag_E9342</td>\n",
       "      <td>0.011915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>diag_2762</td>\n",
       "      <td>0.011510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253</th>\n",
       "      <td>diag_V7231</td>\n",
       "      <td>0.011197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15306</th>\n",
       "      <td>out_c_V529</td>\n",
       "      <td>0.010556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>diag_4556</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>diag_73301</td>\n",
       "      <td>0.009221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>diag_5789</td>\n",
       "      <td>0.008829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4539</th>\n",
       "      <td>diag_42842</td>\n",
       "      <td>0.008461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983</th>\n",
       "      <td>diag_79902</td>\n",
       "      <td>0.008442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>diag_34831</td>\n",
       "      <td>0.008175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>diag_8082</td>\n",
       "      <td>0.008171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>diag_4580</td>\n",
       "      <td>0.008156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15315</th>\n",
       "      <td>out_c_V5390</td>\n",
       "      <td>0.007855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>diag_4263</td>\n",
       "      <td>0.007574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13835</th>\n",
       "      <td>out_c_5854</td>\n",
       "      <td>0.007334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11148</th>\n",
       "      <td>diag_V6284</td>\n",
       "      <td>0.007225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>diag_27651</td>\n",
       "      <td>0.007149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15376</th>\n",
       "      <td>out_c_V589</td>\n",
       "      <td>0.006997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7837</th>\n",
       "      <td>diag_78940</td>\n",
       "      <td>0.006701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>diag_2448</td>\n",
       "      <td>0.006699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>diag_28860</td>\n",
       "      <td>0.006323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11063</th>\n",
       "      <td>diag_V5842</td>\n",
       "      <td>0.006191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>County_991</td>\n",
       "      <td>0.006149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7681</th>\n",
       "      <td>diag_78097</td>\n",
       "      <td>0.006025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>diag_2761</td>\n",
       "      <td>0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4536</th>\n",
       "      <td>diag_42833</td>\n",
       "      <td>0.005937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14599</th>\n",
       "      <td>out_c_78060</td>\n",
       "      <td>0.005845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6936</th>\n",
       "      <td>diag_72211</td>\n",
       "      <td>0.005695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>County_340</td>\n",
       "      <td>0.005596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>County_70</td>\n",
       "      <td>0.005531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13563</th>\n",
       "      <td>out_c_486</td>\n",
       "      <td>0.005460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>diag_29620</td>\n",
       "      <td>0.005460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14527</th>\n",
       "      <td>out_c_72973</td>\n",
       "      <td>0.005396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13192</th>\n",
       "      <td>out_c_4142</td>\n",
       "      <td>0.005365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13588</th>\n",
       "      <td>out_c_4940</td>\n",
       "      <td>0.005355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>diag_5968</td>\n",
       "      <td>0.005354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>diag_2449</td>\n",
       "      <td>0.005315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>County_140</td>\n",
       "      <td>0.005299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12411</th>\n",
       "      <td>out_c_2449</td>\n",
       "      <td>0.005162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>diag_3004</td>\n",
       "      <td>0.005117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>diag_4241</td>\n",
       "      <td>0.005044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>County_770</td>\n",
       "      <td>0.004890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>State_45</td>\n",
       "      <td>0.004836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11674</th>\n",
       "      <td>in_c_329</td>\n",
       "      <td>0.004649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6984</th>\n",
       "      <td>diag_7260</td>\n",
       "      <td>0.004492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15893</th>\n",
       "      <td>proc_3995.0</td>\n",
       "      <td>0.004434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>diag_2760</td>\n",
       "      <td>0.004421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>diag_3659</td>\n",
       "      <td>0.004419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>diag_5693</td>\n",
       "      <td>0.004381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>County_640</td>\n",
       "      <td>0.004350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Mean_Duration</td>\n",
       "      <td>0.004350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>diag_41400</td>\n",
       "      <td>0.004342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391</th>\n",
       "      <td>diag_70722</td>\n",
       "      <td>0.004339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13929</th>\n",
       "      <td>out_c_61171</td>\n",
       "      <td>0.004331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15496</th>\n",
       "      <td>out_c_V772</td>\n",
       "      <td>0.004223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5618</th>\n",
       "      <td>diag_58881</td>\n",
       "      <td>0.004178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>diag_0413</td>\n",
       "      <td>0.004163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>diag_40391</td>\n",
       "      <td>0.004085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15371</th>\n",
       "      <td>out_c_V5878</td>\n",
       "      <td>0.004054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15361</th>\n",
       "      <td>out_c_V5866</td>\n",
       "      <td>0.004027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12228</th>\n",
       "      <td>out_c_1623</td>\n",
       "      <td>0.003997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12610</th>\n",
       "      <td>out_c_28522</td>\n",
       "      <td>0.003993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>diag_4372</td>\n",
       "      <td>0.003986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>diag_0417</td>\n",
       "      <td>0.003779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>diag_4571</td>\n",
       "      <td>0.003717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>diag_28409</td>\n",
       "      <td>0.003710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11823</th>\n",
       "      <td>in_c_508</td>\n",
       "      <td>0.003658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15508</th>\n",
       "      <td>out_c_V783</td>\n",
       "      <td>0.003553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14771</th>\n",
       "      <td>out_c_79029</td>\n",
       "      <td>0.003551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13262</th>\n",
       "      <td>out_c_4289</td>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11447</th>\n",
       "      <td>in_c_031</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13570</th>\n",
       "      <td>out_c_4918</td>\n",
       "      <td>0.003503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>County_510</td>\n",
       "      <td>0.003481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11502</th>\n",
       "      <td>in_c_095</td>\n",
       "      <td>0.003409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>diag_27652</td>\n",
       "      <td>0.003352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>County_260</td>\n",
       "      <td>0.003284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10212</th>\n",
       "      <td>diag_E887</td>\n",
       "      <td>0.003277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>diag_27541</td>\n",
       "      <td>0.003274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>out_c_4272</td>\n",
       "      <td>0.003272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>County_60</td>\n",
       "      <td>0.003165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>diag_3572</td>\n",
       "      <td>0.003142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>State_5</td>\n",
       "      <td>0.003136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>diag_V7282</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15477</th>\n",
       "      <td>out_c_V7619</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16264</th>\n",
       "      <td>proc_6029.0</td>\n",
       "      <td>0.003109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6273</th>\n",
       "      <td>diag_6929</td>\n",
       "      <td>0.003073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12254</th>\n",
       "      <td>out_c_1744</td>\n",
       "      <td>0.003011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>County_480</td>\n",
       "      <td>0.002992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15348</th>\n",
       "      <td>out_c_V5830</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12315</th>\n",
       "      <td>out_c_2312</td>\n",
       "      <td>0.002920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12607</th>\n",
       "      <td>out_c_2850</td>\n",
       "      <td>0.002904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>County_440</td>\n",
       "      <td>0.002842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Variable  Importance\n",
       "4432      diag_41401    0.066810\n",
       "4365      diag_40390    0.044962\n",
       "5691       diag_5990    0.028702\n",
       "4527       diag_4280    0.024929\n",
       "4446       diag_4149    0.020869\n",
       "11068     diag_V5861    0.017123\n",
       "2341       diag_2768    0.016310\n",
       "11316     diag_V7651    0.012111\n",
       "10334     diag_E9342    0.011915\n",
       "2333       diag_2762    0.011510\n",
       "11253     diag_V7231    0.011197\n",
       "15306     out_c_V529    0.010556\n",
       "4735       diag_4556    0.010300\n",
       "7172      diag_73301    0.009221\n",
       "5566       diag_5789    0.008829\n",
       "4539      diag_42842    0.008461\n",
       "7983      diag_79902    0.008442\n",
       "3262      diag_34831    0.008175\n",
       "8326       diag_8082    0.008171\n",
       "4753       diag_4580    0.008156\n",
       "15315    out_c_V5390    0.007855\n",
       "4500       diag_4263    0.007574\n",
       "13835     out_c_5854    0.007334\n",
       "11148     diag_V6284    0.007225\n",
       "2337      diag_27651    0.007149\n",
       "15376     out_c_V589    0.006997\n",
       "7837      diag_78940    0.006701\n",
       "2095       diag_2448    0.006699\n",
       "2479      diag_28860    0.006323\n",
       "11063     diag_V5842    0.006191\n",
       "319       County_991    0.006149\n",
       "7681      diag_78097    0.006025\n",
       "2332       diag_2761    0.005952\n",
       "4536      diag_42833    0.005937\n",
       "14599    out_c_78060    0.005845\n",
       "6936      diag_72211    0.005695\n",
       "72        County_340    0.005596\n",
       "164        County_70    0.005531\n",
       "13563      out_c_486    0.005460\n",
       "2629      diag_29620    0.005460\n",
       "14527    out_c_72973    0.005396\n",
       "13192     out_c_4142    0.005365\n",
       "13588     out_c_4940    0.005355\n",
       "5679       diag_5968    0.005354\n",
       "2096       diag_2449    0.005315\n",
       "24        County_140    0.005299\n",
       "12411     out_c_2449    0.005162\n",
       "2710       diag_3004    0.005117\n",
       "4479       diag_4241    0.005044\n",
       "193       County_770    0.004890\n",
       "394         State_45    0.004836\n",
       "11674       in_c_329    0.004649\n",
       "6984       diag_7260    0.004492\n",
       "15893    proc_3995.0    0.004434\n",
       "2331       diag_2760    0.004421\n",
       "3605       diag_3659    0.004419\n",
       "5475       diag_5693    0.004381\n",
       "147       County_640    0.004350\n",
       "330    Mean_Duration    0.004350\n",
       "4431      diag_41400    0.004342\n",
       "6391      diag_70722    0.004339\n",
       "13929    out_c_61171    0.004331\n",
       "15496     out_c_V772    0.004223\n",
       "5618      diag_58881    0.004178\n",
       "637        diag_0413    0.004163\n",
       "4366      diag_40391    0.004085\n",
       "15371    out_c_V5878    0.004054\n",
       "15361    out_c_V5866    0.004027\n",
       "12228     out_c_1623    0.003997\n",
       "12610    out_c_28522    0.003993\n",
       "4588       diag_4372    0.003986\n",
       "641        diag_0417    0.003779\n",
       "4749       diag_4571    0.003717\n",
       "2432      diag_28409    0.003710\n",
       "11823       in_c_508    0.003658\n",
       "15508     out_c_V783    0.003553\n",
       "14771    out_c_79029    0.003551\n",
       "13262     out_c_4289    0.003516\n",
       "11447       in_c_031    0.003510\n",
       "13570     out_c_4918    0.003503\n",
       "108       County_510    0.003481\n",
       "11502       in_c_095    0.003409\n",
       "2338      diag_27652    0.003352\n",
       "51        County_260    0.003284\n",
       "10212      diag_E887    0.003277\n",
       "2325      diag_27541    0.003274\n",
       "13236     out_c_4272    0.003272\n",
       "135        County_60    0.003165\n",
       "3329       diag_3572    0.003142\n",
       "398          State_5    0.003136\n",
       "11262     diag_V7282    0.003135\n",
       "15477    out_c_V7619    0.003110\n",
       "16264    proc_6029.0    0.003109\n",
       "6273       diag_6929    0.003073\n",
       "12254     out_c_1744    0.003011\n",
       "104       County_480    0.002992\n",
       "15348    out_c_V5830    0.002933\n",
       "12315     out_c_2312    0.002920\n",
       "12607     out_c_2850    0.002904\n",
       "95        County_440    0.002842"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Importances\n",
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "\n",
    "X_sample, X_test, y_sample, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42, stratify=y)\n",
    "X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "X_train_df = pd.DataFrame(data=X_train, columns=X_test.columns)\n",
    "        \n",
    "#fit model \n",
    "model = XGBClassifier(max_depth=3, min_child_weight=1, learning_rate=0.1, \\\n",
    "        n_estimators=1000, seed=0, subsample=0.8, colsample_bytree=0.8, \\\n",
    "        objective='binary:logistic')\n",
    "model.fit(X_train_df, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "    \n",
    "# Feature importances\n",
    "pd.set_option('display.max_rows', 100)\n",
    "(pd.DataFrame({'Variable':X.columns, 'Importance':model.feature_importances_}).sort_values('Importance', ascending=False)).head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
