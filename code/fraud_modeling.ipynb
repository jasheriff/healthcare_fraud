{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_general(df, target):\n",
    "    # baseline logistic regression. \n",
    "    # penalty = 'l2', ridge. , solver = 'liblinear'\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    # training and testing sets\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42, stratify=y)\n",
    "\n",
    "    # Instantiate model\n",
    "    lgr = LogisticRegression(class_weight='balanced', random_state=30)\n",
    "\n",
    "    # Train the model on training data\n",
    "    lgr.fit(X_train, y_train)\n",
    "\n",
    "    # Scores for training and testing\n",
    "    y_predict_train = lgr.predict(X_train)\n",
    "    print(\"Train accuracy score:\", round(accuracy_score(y_predict_train, y_train), 3))\n",
    "\n",
    "    y_predict_test = lgr.predict(X_test)\n",
    "    print(\"Test accuracy score\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "\n",
    "    # Classification reports\n",
    "    print(\"\\n Training Classification Report:\")\n",
    "    print(classification_report(y_train, y_predict_train))\n",
    "\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "    # Model with higher weight towards class 1.\n",
    "    for i in np.linspace(5, 6, 6):\n",
    "        weight_dict = {0: 1, 1:i}\n",
    "        lgr = LogisticRegression(class_weight= weight_dict, random_state=30)\n",
    "        lgr.fit(X_train, y_train)\n",
    "        y_predict_train = lgr.predict(X_train)\n",
    "        y_predict_test = lgr.predict(X_test)\n",
    "        print(\"Weighted Test accuracy score, w=\", i, \":\", accuracy_score(y_predict_test, y_test))\n",
    "        print(\"\\n Weighted Test Classification Report, w=\", i, \":\")\n",
    "        print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "def lassoreg_cv(df, target):\n",
    "    # Logistic regression with lasso 'l1' penalty. Hyperparameter tuning\n",
    "    \n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42, stratify=y)\n",
    "    \n",
    "    param_grid = {'solver': ['liblinear', 'saga'],\n",
    "              'C': [int(x) for x in np.logspace(0, 1, num = 10)]} \n",
    "    \n",
    "    # Instantiate model and grid search\n",
    "    lgr = LogisticRegression(penalty='l1', class_weight = \"balanced\", random_state=30)\n",
    "    gm_cv = RandomizedSearchCV(lgr, param_grid, n_iter = 15, cv = 3, random_state=30)\n",
    "    gm_cv.fit(X_train, y_train)\n",
    "\n",
    "    # Scores for training and testing\n",
    "    y_predict_train = gm_cv.predict(X_train)\n",
    "    print(\"Train accuracy score:\", round(accuracy_score(y_predict_train, y_train), 3))\n",
    "\n",
    "    y_predict_test = gm_cv.predict(X_test)\n",
    "    print(\"Test accuracy score\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "\n",
    "    # Classification reports\n",
    "    print(\"\\n Training Classification Report:\")\n",
    "    print(classification_report(y_train, y_predict_train))\n",
    "\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "    # Best estimator\n",
    "    print(gm_cv.best_estimator_)\n",
    "    \n",
    "    \n",
    "def ridgereg_cv(df, target):\n",
    "    # Logistic regression with ridge 'l2' penalty. Hyperparameter tuning.\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42, stratify=y)\n",
    "    \n",
    "    param_grid = {'solver': ['lbfgs', 'sag', 'saga'],\n",
    "                'C': [int(x) for x in np.logspace(0, 1, num = 10)]}\n",
    "    \n",
    "    # Instantiate model and grid search\n",
    "    lgr = LogisticRegression(penalty='l2', class_weight=\"balanced\", random_state=30)\n",
    "    gm_cv = RandomizedSearchCV(lgr, param_grid, n_iter = 15, cv = 3, random_state=30)\n",
    "    gm_cv.fit(X_train, y_train)\n",
    "\n",
    "    # Scores for training and testing\n",
    "    y_predict_train = gm_cv.predict(X_train)\n",
    "    print(\"Train accuracy score:\", round(accuracy_score(y_predict_train, y_train), 3))\n",
    "\n",
    "    y_predict_test = gm_cv.predict(X_test)\n",
    "    print(\"Test accuracy score\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "\n",
    "    # Classification reports\n",
    "    print(\"\\n Training Classification Report:\")\n",
    "    print(classification_report(y_train, y_predict_train))\n",
    "\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "    # Best Model\n",
    "    print(gm_cv.best_estimator_)\n",
    "\n",
    "def ridge_cv_weights(df, target):\n",
    "    # checking different weights with best cv ridge model\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    # training and testing sets\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Model with higher weight towards class 1.\n",
    "    for i in np.linspace(1, 10, 6):\n",
    "        weight_dict = {0: 1, 1:i}\n",
    "        lgr = LogisticRegression(C=7, class_weight=i, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='warn', n_jobs=None, penalty='l2', random_state=30,\n",
    "          solver='saga', tol=0.0001, verbose=0, warm_start=False)\n",
    "        lgr.fit(X_train, y_train)\n",
    "        y_predict_train = lgr.predict(X_train)\n",
    "        y_predict_test = lgr.predict(X_test)\n",
    "        print(\"Weighted Test accuracy score, w=\", i, \":\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "        print(\"\\n Weighted Test Classification Report, w=\", i)\n",
    "        print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_data = pd.read_csv('/Users/Julia/Documents/bootcamp/fraud_capstone/data_out/train_final_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChronicCond_Alzheimer</th>\n",
       "      <th>ChronicCond_Cancer</th>\n",
       "      <th>ChronicCond_Depression</th>\n",
       "      <th>ChronicCond_Diabetes</th>\n",
       "      <th>ChronicCond_Heartfailure</th>\n",
       "      <th>ChronicCond_IschemicHeart</th>\n",
       "      <th>ChronicCond_KidneyDisease</th>\n",
       "      <th>ChronicCond_ObstrPulmonary</th>\n",
       "      <th>ChronicCond_Osteoporasis</th>\n",
       "      <th>ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>ChronicCond_stroke</th>\n",
       "      <th>County_0</th>\n",
       "      <th>County_1</th>\n",
       "      <th>County_10</th>\n",
       "      <th>County_100</th>\n",
       "      <th>County_11</th>\n",
       "      <th>County_110</th>\n",
       "      <th>County_111</th>\n",
       "      <th>County_113</th>\n",
       "      <th>County_117</th>\n",
       "      <th>County_120</th>\n",
       "      <th>County_130</th>\n",
       "      <th>County_131</th>\n",
       "      <th>County_14</th>\n",
       "      <th>County_140</th>\n",
       "      <th>County_141</th>\n",
       "      <th>County_150</th>\n",
       "      <th>County_160</th>\n",
       "      <th>County_161</th>\n",
       "      <th>County_170</th>\n",
       "      <th>County_180</th>\n",
       "      <th>County_190</th>\n",
       "      <th>County_191</th>\n",
       "      <th>County_194</th>\n",
       "      <th>County_20</th>\n",
       "      <th>County_200</th>\n",
       "      <th>County_210</th>\n",
       "      <th>County_211</th>\n",
       "      <th>County_212</th>\n",
       "      <th>County_213</th>\n",
       "      <th>County_220</th>\n",
       "      <th>County_221</th>\n",
       "      <th>County_222</th>\n",
       "      <th>County_223</th>\n",
       "      <th>County_224</th>\n",
       "      <th>County_230</th>\n",
       "      <th>County_240</th>\n",
       "      <th>County_241</th>\n",
       "      <th>County_25</th>\n",
       "      <th>County_250</th>\n",
       "      <th>...</th>\n",
       "      <th>proc_9764.0</th>\n",
       "      <th>proc_9784.0</th>\n",
       "      <th>proc_9787.0</th>\n",
       "      <th>proc_9789.0</th>\n",
       "      <th>proc_9805.0</th>\n",
       "      <th>proc_9815.0</th>\n",
       "      <th>proc_9851.0</th>\n",
       "      <th>proc_9903.0</th>\n",
       "      <th>proc_9904.0</th>\n",
       "      <th>proc_9905.0</th>\n",
       "      <th>proc_9906.0</th>\n",
       "      <th>proc_9907.0</th>\n",
       "      <th>proc_9910.0</th>\n",
       "      <th>proc_9914.0</th>\n",
       "      <th>proc_9915.0</th>\n",
       "      <th>proc_9916.0</th>\n",
       "      <th>proc_9917.0</th>\n",
       "      <th>proc_9918.0</th>\n",
       "      <th>proc_9919.0</th>\n",
       "      <th>proc_9920.0</th>\n",
       "      <th>proc_9921.0</th>\n",
       "      <th>proc_9922.0</th>\n",
       "      <th>proc_9923.0</th>\n",
       "      <th>proc_9925.0</th>\n",
       "      <th>proc_9926.0</th>\n",
       "      <th>proc_9928.0</th>\n",
       "      <th>proc_9929.0</th>\n",
       "      <th>proc_9938.0</th>\n",
       "      <th>proc_9939.0</th>\n",
       "      <th>proc_9952.0</th>\n",
       "      <th>proc_9955.0</th>\n",
       "      <th>proc_9959.0</th>\n",
       "      <th>proc_9960.0</th>\n",
       "      <th>proc_9961.0</th>\n",
       "      <th>proc_9962.0</th>\n",
       "      <th>proc_9969.0</th>\n",
       "      <th>proc_9971.0</th>\n",
       "      <th>proc_9972.0</th>\n",
       "      <th>proc_9973.0</th>\n",
       "      <th>proc_9974.0</th>\n",
       "      <th>proc_9975.0</th>\n",
       "      <th>proc_9978.0</th>\n",
       "      <th>proc_9979.0</th>\n",
       "      <th>proc_9982.0</th>\n",
       "      <th>proc_9984.0</th>\n",
       "      <th>proc_9986.0</th>\n",
       "      <th>proc_9992.0</th>\n",
       "      <th>proc_9995.0</th>\n",
       "      <th>proc_9998.0</th>\n",
       "      <th>proc_9999.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.365759</td>\n",
       "      <td>0.233463</td>\n",
       "      <td>0.451362</td>\n",
       "      <td>0.754864</td>\n",
       "      <td>0.564202</td>\n",
       "      <td>0.762646</td>\n",
       "      <td>0.474708</td>\n",
       "      <td>0.400778</td>\n",
       "      <td>0.272374</td>\n",
       "      <td>0.330739</td>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.426901</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.730994</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.380117</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.345029</td>\n",
       "      <td>0.076023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429515</td>\n",
       "      <td>0.229075</td>\n",
       "      <td>0.451542</td>\n",
       "      <td>0.685022</td>\n",
       "      <td>0.596916</td>\n",
       "      <td>0.799559</td>\n",
       "      <td>0.398678</td>\n",
       "      <td>0.341410</td>\n",
       "      <td>0.370044</td>\n",
       "      <td>0.290749</td>\n",
       "      <td>0.063877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.496454</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.773050</td>\n",
       "      <td>0.624113</td>\n",
       "      <td>0.794326</td>\n",
       "      <td>0.460993</td>\n",
       "      <td>0.304965</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.099291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322917</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16888 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ChronicCond_Alzheimer  ChronicCond_Cancer  ChronicCond_Depression  ChronicCond_Diabetes  ChronicCond_Heartfailure  ChronicCond_IschemicHeart  ChronicCond_KidneyDisease  ChronicCond_ObstrPulmonary  ChronicCond_Osteoporasis  ChronicCond_rheumatoidarthritis  ChronicCond_stroke  County_0  County_1  County_10  County_100  County_11  County_110  County_111  County_113  County_117  County_120  County_130  County_131  County_14  County_140  County_141  County_150  County_160  County_161  \\\n",
       "0               0.365759            0.233463                0.451362              0.754864                  0.564202                   0.762646                   0.474708                    0.400778                  0.272374                         0.330739            0.105058  0.011673       0.0   0.011673    0.011673        0.0         0.0         0.0         0.0         0.0         0.0    0.015564         0.0        0.0    0.003891         0.0     0.07393    0.000000         0.0   \n",
       "1               0.426901            0.175439                0.444444              0.730994                  0.649123                   0.807018                   0.473684                    0.380117                  0.280702                         0.345029            0.076023  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.023392         0.0        0.0    0.005848         0.0     0.00000    0.000000         0.0   \n",
       "2               0.429515            0.229075                0.451542              0.685022                  0.596916                   0.799559                   0.398678                    0.341410                  0.370044                         0.290749            0.063877  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.000000         0.0   \n",
       "3               0.496454            0.191489                0.446809              0.773050                  0.624113                   0.794326                   0.460993                    0.304965                  0.326241                         0.326241            0.099291  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.014184         0.0   \n",
       "4               0.322917            0.156250                0.385417              0.645833                  0.645833                   0.687500                   0.395833                    0.302083                  0.291667                         0.270833            0.104167  0.000000       0.0   0.031250    0.031250        0.0         0.0         0.0         0.0         0.0         0.0    0.020833         0.0        0.0    0.000000         0.0     0.00000    0.135417         0.0   \n",
       "\n",
       "   County_170  County_180  County_190  County_191  County_194  County_20  County_200  County_210  County_211  County_212  County_213  County_220  County_221  County_222  County_223  County_224  County_230  County_240  County_241  County_25  County_250     ...       proc_9764.0  proc_9784.0  proc_9787.0  proc_9789.0  proc_9805.0  proc_9815.0  proc_9851.0  proc_9903.0  proc_9904.0  proc_9905.0  proc_9906.0  proc_9907.0  proc_9910.0  proc_9914.0  proc_9915.0  proc_9916.0  proc_9917.0  \\\n",
       "0         0.0    0.003891    0.011673         0.0         0.0   0.003891         0.0    0.000000         0.0         0.0         0.0    0.011673         0.0         0.0         0.0         0.0    0.007782    0.011673         0.0        0.0    0.054475     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "1         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.070175         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.070175         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          2.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "2         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.000000         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.156388         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "3         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.014184         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "4         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.010417         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   proc_9918.0  proc_9919.0  proc_9920.0  proc_9921.0  proc_9922.0  proc_9923.0  proc_9925.0  proc_9926.0  proc_9928.0  proc_9929.0  proc_9938.0  proc_9939.0  proc_9952.0  proc_9955.0  proc_9959.0  proc_9960.0  proc_9961.0  proc_9962.0  proc_9969.0  proc_9971.0  proc_9972.0  proc_9973.0  proc_9974.0  proc_9975.0  proc_9978.0  proc_9979.0  proc_9982.0  proc_9984.0  proc_9986.0  proc_9992.0  proc_9995.0  proc_9998.0  proc_9999.0  \n",
       "0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "1          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "2          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "3          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "4          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "\n",
       "[5 rows x 16888 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Provider</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PotentialFraud</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Provider\n",
       "PotentialFraud          \n",
       "0                   4904\n",
       "1                    506"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are dealing with an imbalanced data set.\n",
    "train_final_data[['PotentialFraud', 'Provider']].groupby('PotentialFraud').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION RESULTS\n",
    "* Precision: TP / (TP+FP)\n",
    "    * high precision: few false positives\n",
    "    * high precision for class 1: few providers we identify as fraudulent are not fraudulent\n",
    "    * if we have high precision, few providers are being falsely accused of fraud\n",
    "* Recall: TP / (TP+FN)\n",
    "    * high recall: few false negatives\n",
    "    * high recall for class 1: few providers we identify as not fraudulent are fraudulent\n",
    "    * if we have high recall, few providers are getting away with fraud\n",
    "* F1-score: 2*(precision * recall)/(precision+recall)\n",
    "* Support: number of occurrences per class\n",
    "* Averages\n",
    "    * micro average - sum of (TP) / (TP + FN) across all classes\n",
    "    * macro average - average of each class' recall\n",
    "    * weighted average \n",
    "\n",
    "#### We will choose \"Recall\" of Class 1, then \"Precision\" as key important metrics\n",
    "False accuastions of fraud are less concerning than missed accusations.\n",
    "\n",
    "###### Key models:  \n",
    "\n",
    "type    | class weight | Class 1 Precision | Class 1 Recall | Accuracy Score |\n",
    "------- | ------------ | -------------- | -------------- | --------------- |\n",
    "BEST: lasso logistic tuned | balanced | .46   | .55 | .897\n",
    "BEST: ridge logistic tuned | balanced | .46 | .55 | .897\n",
    "BASELINE: general logistic regression | 1:5.8 | .35 | .35   | .878\n",
    "\n",
    "##### Feature Importances: \n",
    "* Mean Insurance Claim Amount Reimbursed was the most predictive feature of potential fraud. \n",
    "* For the top 50 influential features, all others were Mean Maximum Insurance Claim Amount Reimbursed Amongst Attending Physicians per Provider for various DRGs, similar to our 2nd, 3rd, and 4th top influences. \n",
    "* Inpatient DRG reimbursement rates were of higher importance than outpatient DRG reimbursement rates\n",
    "* Other notable importances (top 1000) were Mean_IPAnnualDeductibleAmt, Median_IPAnnualDeductibleAmt, and Mean_OPAnnualDeductibleAmt, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 1.0\n",
      "Test accuracy score 0.876\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3678\n",
      "           1       1.00      1.00      1.00       379\n",
      "\n",
      "    accuracy                           1.00      4057\n",
      "   macro avg       1.00      1.00      1.00      4057\n",
      "weighted avg       1.00      1.00      1.00      4057\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      1226\n",
      "           1       0.34      0.33      0.33       127\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.63      0.63      0.63      1353\n",
      "weighted avg       0.87      0.88      0.88      1353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 5.0 : 0.8787878787878788\n",
      "\n",
      " Weighted Test Classification Report, w= 5.0 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      1226\n",
      "           1       0.35      0.34      0.34       127\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.64      0.64      0.64      1353\n",
      "weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 5.2 : 0.8780487804878049\n",
      "\n",
      " Weighted Test Classification Report, w= 5.2 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      1226\n",
      "           1       0.35      0.34      0.34       127\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.64      0.64      0.64      1353\n",
      "weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 5.4 : 0.8780487804878049\n",
      "\n",
      " Weighted Test Classification Report, w= 5.4 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      1226\n",
      "           1       0.34      0.33      0.34       127\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.64      0.63      0.64      1353\n",
      "weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 5.6 : 0.8780487804878049\n",
      "\n",
      " Weighted Test Classification Report, w= 5.6 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      1226\n",
      "           1       0.34      0.33      0.34       127\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.64      0.63      0.64      1353\n",
      "weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 5.8 : 0.8780487804878049\n",
      "\n",
      " Weighted Test Classification Report, w= 5.8 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      1226\n",
      "           1       0.35      0.35      0.35       127\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.64      0.64      0.64      1353\n",
      "weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 6.0 : 0.8780487804878049\n",
      "\n",
      " Weighted Test Classification Report, w= 6.0 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      1226\n",
      "           1       0.35      0.34      0.34       127\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.64      0.64      0.64      1353\n",
      "weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit_general(train_final_data, 'PotentialFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 0.952\n",
      "Test accuracy score 0.897\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      3433\n",
      "           1       0.68      0.94      0.79       354\n",
      "\n",
      "    accuracy                           0.95      3787\n",
      "   macro avg       0.84      0.94      0.88      3787\n",
      "weighted avg       0.96      0.95      0.96      3787\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      1471\n",
      "           1       0.46      0.55      0.50       152\n",
      "\n",
      "    accuracy                           0.90      1623\n",
      "   macro avg       0.71      0.74      0.72      1623\n",
      "weighted avg       0.91      0.90      0.90      1623\n",
      "\n",
      "LogisticRegression(C=3, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=30, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lassoreg_cv(train_final_data, 'PotentialFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score: 0.952\n",
      "Test accuracy score 0.897\n",
      "\n",
      " Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      3433\n",
      "           1       0.68      0.94      0.79       354\n",
      "\n",
      "    accuracy                           0.95      3787\n",
      "   macro avg       0.84      0.94      0.88      3787\n",
      "weighted avg       0.96      0.95      0.96      3787\n",
      "\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      1471\n",
      "           1       0.46      0.55      0.50       152\n",
      "\n",
      "    accuracy                           0.90      1623\n",
      "   macro avg       0.71      0.74      0.72      1623\n",
      "weighted avg       0.91      0.90      0.90      1623\n",
      "\n",
      "LogisticRegression(C=7, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=30, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "ridgereg_cv(train_final_data, 'PotentialFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 1.0 : 0.914\n",
      "\n",
      " Weighted Test Classification Report, w= 1.0 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      1471\n",
      "           1       0.57      0.34      0.42       152\n",
      "\n",
      "    accuracy                           0.91      1623\n",
      "   macro avg       0.75      0.65      0.69      1623\n",
      "weighted avg       0.90      0.91      0.90      1623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 2.8 : 0.914\n",
      "\n",
      " Weighted Test Classification Report, w= 2.8 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      1471\n",
      "           1       0.57      0.34      0.42       152\n",
      "\n",
      "    accuracy                           0.91      1623\n",
      "   macro avg       0.75      0.65      0.69      1623\n",
      "weighted avg       0.90      0.91      0.90      1623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 4.6 : 0.914\n",
      "\n",
      " Weighted Test Classification Report, w= 4.6 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      1471\n",
      "           1       0.57      0.34      0.42       152\n",
      "\n",
      "    accuracy                           0.91      1623\n",
      "   macro avg       0.75      0.65      0.69      1623\n",
      "weighted avg       0.90      0.91      0.90      1623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 6.4 : 0.914\n",
      "\n",
      " Weighted Test Classification Report, w= 6.4 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      1471\n",
      "           1       0.57      0.34      0.42       152\n",
      "\n",
      "    accuracy                           0.91      1623\n",
      "   macro avg       0.75      0.65      0.69      1623\n",
      "weighted avg       0.90      0.91      0.90      1623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 8.2 : 0.914\n",
      "\n",
      " Weighted Test Classification Report, w= 8.2 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      1471\n",
      "           1       0.57      0.34      0.42       152\n",
      "\n",
      "    accuracy                           0.91      1623\n",
      "   macro avg       0.75      0.65      0.69      1623\n",
      "weighted avg       0.90      0.91      0.90      1623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Test accuracy score, w= 10.0 : 0.914\n",
      "\n",
      " Weighted Test Classification Report, w= 10.0 :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      1471\n",
      "           1       0.57      0.34      0.42       152\n",
      "\n",
      "    accuracy                           0.91      1623\n",
      "   macro avg       0.75      0.65      0.69      1623\n",
      "weighted avg       0.90      0.91      0.90      1623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no improvement with various weights\n",
    "ridge_cv_weights(train_final_data, \"PotentialFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Importance                       Variable\n",
      "335      0.334664    Mean_InscClaimAmtReimbursed\n",
      "12067    0.265656                       in_c_854\n",
      "11644    0.231837                       in_c_290\n",
      "11594    0.187301                       in_c_225\n",
      "12148    0.186465                       in_c_983\n",
      "11561    0.186066                       in_c_186\n",
      "11577    0.183288                       in_c_202\n",
      "11620    0.179496                       in_c_251\n",
      "11567    0.175714                       in_c_192\n",
      "12155    0.172016                       in_c_998\n",
      "11612    0.164618                       in_c_243\n",
      "11588    0.164567                       in_c_219\n",
      "11818    0.161766                       in_c_503\n",
      "11663    0.158654                       in_c_309\n",
      "333      0.156940  Mean_IPAnnualReimbursementAmt\n",
      "11731    0.148089                       in_c_395\n",
      "12130    0.147991                       in_c_950\n",
      "12125    0.147156                       in_c_945\n",
      "12088    0.144454                       in_c_884\n",
      "11653    0.141045                       in_c_299\n",
      "11857    0.140677                       in_c_557\n",
      "11576    0.140381                       in_c_201\n",
      "12077    0.137672                       in_c_867\n",
      "12034    0.133273                       in_c_812\n",
      "11603    0.131799                       in_c_234\n",
      "11628    0.125465                       in_c_259\n",
      "11843    0.124903                       in_c_543\n",
      "11578    0.123885                       in_c_203\n",
      "11568    0.122353                       in_c_193\n",
      "11425    0.122201                       in_c_003\n",
      "11429    0.119852                       in_c_007\n",
      "11609    0.119512                       in_c_240\n",
      "11605    0.119052                       in_c_236\n",
      "11824    0.118373                       in_c_509\n",
      "11558    0.118268                       in_c_183\n",
      "12151    0.118204                       in_c_986\n",
      "11570    0.117725                       in_c_195\n",
      "11606    0.117467                       in_c_237\n",
      "11582    0.115190                       in_c_207\n",
      "11571    0.114137                       in_c_196\n",
      "11423    0.112677                       in_c_001\n",
      "11667    0.110775                       in_c_313\n",
      "11611    0.110434                       in_c_242\n",
      "11435    0.110433                       in_c_013\n",
      "12070    0.109324                       in_c_857\n",
      "12153    0.108774                       in_c_988\n",
      "11430    0.107937                       in_c_008\n",
      "12080    0.107327                       in_c_870\n",
      "11853    0.106746                       in_c_553\n",
      "11712    0.106490                       in_c_376\n",
      "11591    0.106114                       in_c_222\n",
      "11934    0.105212                       in_c_664\n",
      "11732    0.105070                       in_c_405\n",
      "11633    0.103280                       in_c_264\n",
      "11544    0.103095                       in_c_163\n",
      "11657    0.102036                       in_c_303\n",
      "11802    0.101843                       in_c_487\n",
      "11649    0.101835                       in_c_295\n",
      "11768    0.101289                       in_c_453\n",
      "11585    0.100239                       in_c_216\n",
      "11639    0.099241                       in_c_285\n",
      "11810    0.098603                       in_c_495\n",
      "12150    0.097966                       in_c_985\n",
      "11566    0.097055                       in_c_191\n",
      "11549    0.095485                       in_c_168\n",
      "11627    0.094712                       in_c_258\n",
      "11794    0.094411                       in_c_479\n",
      "11679    0.094247                       in_c_334\n",
      "11626    0.089932                       in_c_257\n",
      "15332    0.087665                     out_c_V562\n",
      "11587    0.087537                       in_c_218\n",
      "11656    0.087460                       in_c_302\n",
      "11813    0.086427                       in_c_498\n",
      "11490    0.086328                       in_c_083\n",
      "11746    0.084110                       in_c_419\n",
      "11932    0.082914                       in_c_662\n",
      "12072    0.078991                       in_c_862\n",
      "11851    0.078790                       in_c_551\n",
      "15331    0.078680                     out_c_V561\n",
      "11670    0.078379                       in_c_316\n",
      "11960    0.077357                       in_c_696\n",
      "12069    0.076559                       in_c_856\n",
      "11584    0.075525                       in_c_215\n",
      "13838    0.075313                     out_c_5859\n",
      "11607    0.074186                       in_c_238\n",
      "11859    0.074092                       in_c_559\n",
      "11952    0.074038                       in_c_688\n",
      "11668    0.072384                       in_c_314\n",
      "11630    0.072234                       in_c_261\n",
      "12138    0.069972                       in_c_964\n",
      "11687    0.069756                       in_c_342\n",
      "11883    0.069466                       in_c_595\n",
      "11632    0.068290                       in_c_263\n",
      "11909    0.068286                       in_c_627\n",
      "12073    0.068162                       in_c_863\n",
      "12087    0.067697                       in_c_883\n",
      "11743    0.067378                       in_c_416\n",
      "12127    0.067185                       in_c_947\n",
      "11688    0.066772                       in_c_343\n",
      "11681    0.066571                       in_c_336\n",
      "11722    0.066350                       in_c_386\n",
      "11770    0.065590                       in_c_455\n",
      "12149    0.065551                       in_c_984\n",
      "11826    0.065390                       in_c_511\n",
      "11569    0.064519                       in_c_194\n",
      "11718    0.064514                       in_c_382\n",
      "11867    0.064368                       in_c_573\n",
      "12091    0.064064                       in_c_887\n",
      "11827    0.063289                       in_c_512\n",
      "11646    0.062858                       in_c_292\n",
      "11496    0.062376                       in_c_089\n",
      "12113    0.062170                       in_c_921\n",
      "13618    0.061525                    out_c_51884\n",
      "15330    0.061383                     out_c_V560\n",
      "11642    0.061047                       in_c_288\n",
      "11477    0.060637                       in_c_070\n",
      "11621    0.060617                       in_c_252\n",
      "11492    0.060594                       in_c_085\n",
      "11426    0.059585                       in_c_004\n",
      "11651    0.058962                       in_c_297\n",
      "11658    0.058870                       in_c_304\n",
      "15333    0.058195                    out_c_V5631\n",
      "11669    0.058157                       in_c_315\n",
      "13834    0.058145                     out_c_5853\n",
      "11640    0.058090                       in_c_286\n",
      "11550    0.057665                       in_c_175\n",
      "15195    0.057471                     out_c_V420\n",
      "11602    0.057406                       in_c_233\n",
      "11808    0.057238                       in_c_493\n",
      "11911    0.056626                       in_c_629\n",
      "11631    0.056568                       in_c_262\n",
      "13835    0.055618                     out_c_5854\n",
      "11779    0.055212                       in_c_464\n",
      "11950    0.054453                       in_c_686\n",
      "11915    0.054445                       in_c_639\n",
      "11698    0.054011                       in_c_353\n",
      "15334    0.053944                    out_c_V5632\n",
      "14789    0.053902                     out_c_7925\n",
      "11562    0.053681                       in_c_187\n",
      "11752    0.053439                       in_c_425\n",
      "11683    0.053357                       in_c_338\n",
      "13836    0.052805                     out_c_5855\n",
      "11913    0.052409                       in_c_637\n",
      "11787    0.051449                       in_c_472\n",
      "12075    0.051353                       in_c_865\n",
      "11951    0.051341                       in_c_687\n",
      "12097    0.050119                       in_c_902\n",
      "11855    0.050026                       in_c_555\n",
      "12122    0.049948                       in_c_939\n",
      "11940    0.049648                       in_c_670\n",
      "11760    0.049623                       in_c_439\n",
      "11654    0.049561                       in_c_300\n",
      "11672    0.047958                       in_c_327\n",
      "11729    0.047619                       in_c_393\n",
      "11459    0.047508                       in_c_052\n",
      "11460    0.047320                       in_c_053\n",
      "11589    0.046983                       in_c_220\n",
      "337      0.046802  Mean_OPAnnualReimbursementAmt\n",
      "11650    0.046252                       in_c_296\n",
      "11801    0.046228                       in_c_486\n",
      "11554    0.045999                       in_c_179\n",
      "12046    0.045493                       in_c_827\n",
      "12089    0.044688                       in_c_885\n",
      "15238    0.044555                    out_c_V4512\n",
      "11784    0.044511                       in_c_469\n",
      "11579    0.044086                       in_c_204\n",
      "13842    0.043976                    out_c_58881\n",
      "11485    0.043805                       in_c_078\n",
      "11864    0.043749                       in_c_564\n",
      "11741    0.042662                       in_c_414\n",
      "11937    0.042585                       in_c_667\n",
      "11600    0.042164                       in_c_231\n",
      "15346    0.041837                    out_c_V5811\n",
      "11434    0.040784                       in_c_012\n",
      "11636    0.040476                       in_c_282\n",
      "11563    0.040378                       in_c_188\n",
      "11608    0.040043                       in_c_239\n",
      "11444    0.039817                       in_c_028\n",
      "11699    0.039554                       in_c_354\n",
      "15347    0.039096                    out_c_V5812\n",
      "11685    0.038146                       in_c_340\n",
      "11845    0.036935                       in_c_545\n",
      "11671    0.036377                       in_c_326\n",
      "11753    0.036078                       in_c_432\n",
      "11580    0.036060                       in_c_205\n",
      "11761    0.035665                       in_c_440\n",
      "11797    0.035613                       in_c_482\n",
      "11958    0.035271                       in_c_694\n",
      "15345    0.035210                     out_c_V580\n",
      "11879    0.035074                       in_c_585\n",
      "12083    0.034484                       in_c_876\n",
      "11869    0.034445                       in_c_575\n",
      "11903    0.033256                       in_c_621\n",
      "11923    0.033163                       in_c_653\n",
      "12609    0.032904                    out_c_28521\n",
      "15393    0.032896                     out_c_V662\n",
      "15237    0.032503                    out_c_V4511\n",
      "15251    0.032326                    out_c_V4582\n",
      "12090    0.032029                       in_c_886\n",
      "11816    0.031470                       in_c_501\n",
      "11865    0.031400                       in_c_565\n",
      "11443    0.031289                       in_c_027\n",
      "11455    0.030602                       in_c_039\n",
      "11546    0.030203                       in_c_165\n",
      "15335    0.030154                     out_c_V568\n",
      "11559    0.030147                       in_c_184\n",
      "12074    0.030067                       in_c_864\n",
      "11781    0.029954                       in_c_466\n",
      "11871    0.029931                       in_c_577\n",
      "11819    0.029798                       in_c_504\n",
      "11828    0.029587                       in_c_513\n",
      "11648    0.029388                       in_c_294\n",
      "12049    0.029019                       in_c_830\n",
      "12098    0.028831                       in_c_903\n",
      "11439    0.028757                       in_c_023\n",
      "12036    0.028698                       in_c_814\n",
      "14675    0.028664                    out_c_78659\n",
      "11503    0.028038                       in_c_096\n",
      "11772    0.027978                       in_c_457\n",
      "12081    0.027866                       in_c_871\n",
      "11502    0.027859                       in_c_095\n",
      "11723    0.027598                       in_c_387\n",
      "11936    0.027228                       in_c_666\n",
      "11815    0.026909                       in_c_500\n",
      "11943    0.026886                       in_c_673\n",
      "11664    0.026840                       in_c_310\n",
      "11870    0.026248                       in_c_576\n",
      "11693    0.025688                       in_c_348\n",
      "11849    0.025234                       in_c_549\n",
      "12137    0.024382                       in_c_963\n",
      "12029    0.024083                       in_c_804\n",
      "11560    0.023979                       in_c_185\n",
      "11811    0.023841                       in_c_496\n",
      "11450    0.023514                       in_c_034\n",
      "11724    0.023421                       in_c_388\n",
      "11481    0.023168                       in_c_074\n",
      "15402    0.023086                     out_c_V672\n",
      "11858    0.023064                       in_c_558\n",
      "12047    0.022867                       in_c_828\n",
      "12043    0.022839                       in_c_824\n",
      "15098    0.022780                     out_c_V103\n",
      "11993    0.022601                       in_c_741\n",
      "11901    0.022502                       in_c_619\n",
      "12059    0.022177                       in_c_843\n",
      "11834    0.022155                       in_c_534\n",
      "11623    0.021878                       in_c_254\n",
      "11618    0.021289                       in_c_249\n",
      "11661    0.021060                       in_c_307\n",
      "11948    0.020662                       in_c_684\n",
      "11836    0.020621                       in_c_536\n",
      "12105    0.020175                       in_c_913\n",
      "11437    0.020035                       in_c_021\n",
      "11907    0.019880                       in_c_625\n",
      "11765    0.019795                       in_c_444\n",
      "11438    0.019605                       in_c_022\n",
      "11702    0.019522                       in_c_357\n",
      "11660    0.019286                       in_c_306\n",
      "11673    0.019221                       in_c_328\n",
      "11841    0.019110                       in_c_541\n",
      "11551    0.019103                       in_c_176\n",
      "11734    0.019003                       in_c_407\n",
      "11898    0.018930                       in_c_616\n",
      "11938    0.018641                       in_c_668\n",
      "11893    0.018338                       in_c_605\n",
      "11994    0.018307                       in_c_742\n",
      "12102    0.018132                       in_c_907\n",
      "11719    0.018049                       in_c_383\n",
      "11475    0.017981                       in_c_068\n",
      "11441    0.017921                       in_c_025\n",
      "15401    0.017729                     out_c_V671\n",
      "12062    0.017530                       in_c_846\n",
      "11812    0.017318                       in_c_497\n",
      "11975    0.017084                       in_c_717\n",
      "11479    0.017067                       in_c_072\n",
      "11613    0.017009                       in_c_244\n",
      "11647    0.016870                       in_c_293\n",
      "11887    0.016763                       in_c_599\n",
      "13246    0.016670                    out_c_42789\n",
      "12050    0.016669                       in_c_834\n",
      "11695    0.016615                       in_c_350\n",
      "11999    0.016556                       in_c_747\n",
      "11881    0.015993                       in_c_593\n",
      "11474    0.015988                       in_c_067\n",
      "11916    0.015863                       in_c_640\n",
      "11498    0.015795                       in_c_091\n",
      "11856    0.015764                       in_c_556\n",
      "11804    0.015673                       in_c_489\n",
      "11675    0.015492                       in_c_330\n",
      "11520    0.015476                       in_c_130\n",
      "11545    0.015425                       in_c_164\n",
      "11918    0.015287                       in_c_642\n",
      "14673    0.015223                    out_c_78651\n",
      "11933    0.015031                       in_c_663\n",
      "11788    0.015017                       in_c_473\n",
      "11747    0.014959                       in_c_420\n",
      "12126    0.014792                       in_c_946\n",
      "12066    0.014679                       in_c_853\n",
      "11895    0.014633                       in_c_607\n",
      "11838    0.014492                       in_c_538\n",
      "15099    0.014245                    out_c_V1042\n",
      "11786    0.014172                       in_c_471\n",
      "14587    0.014073                     out_c_7802\n",
      "11921    0.013746                       in_c_645\n",
      "11592    0.013679                       in_c_223\n",
      "11852    0.013360                       in_c_552\n",
      "11962    0.013281                       in_c_698\n",
      "11574    0.013205                       in_c_199\n",
      "11488    0.012925                       in_c_081\n",
      "11866    0.012694                       in_c_566\n",
      "11912    0.012600                       in_c_630\n",
      "11846    0.012535                       in_c_546\n",
      "11957    0.012307                       in_c_693\n",
      "15392    0.012212                     out_c_V661\n",
      "13665    0.012153                    out_c_53361\n",
      "12104    0.012003                       in_c_909\n",
      "11678    0.011882                       in_c_333\n",
      "13184    0.011873                     out_c_4139\n",
      "12009    0.011437                       in_c_760\n",
      "13247    0.011190                     out_c_4279\n",
      "11947    0.011169                       in_c_683\n",
      "13180    0.011042                    out_c_41189\n",
      "12044    0.011024                       in_c_825\n",
      "13186    0.010874                    out_c_41401\n",
      "13837    0.010820                     out_c_5856\n",
      "11506    0.010768                       in_c_099\n",
      "11717    0.010712                       in_c_381\n",
      "11751    0.010699                       in_c_424\n",
      "11525    0.010616                       in_c_135\n",
      "11929    0.010589                       in_c_659\n",
      "332      0.010568     Mean_IPAnnualDeductibleAmt\n",
      "11800    0.010160                       in_c_485\n",
      "11462    0.010151                       in_c_055\n",
      "11842    0.010073                       in_c_542\n",
      "13193    0.009742                     out_c_4143\n",
      "12328    0.009674                     out_c_2334\n",
      "11776    0.009527                       in_c_461\n",
      "12082    0.009434                       in_c_872\n",
      "12326    0.009289                     out_c_2330\n",
      "11917    0.009211                       in_c_641\n",
      "15101    0.009206                    out_c_V1046\n",
      "11888    0.008997                       in_c_600\n",
      "11726    0.008964                       in_c_390\n",
      "12103    0.008801                       in_c_908\n",
      "12086    0.008771                       in_c_882\n",
      "13339    0.008732                    out_c_44020\n",
      "13194    0.008624                     out_c_4148\n",
      "11777    0.008582                       in_c_462\n",
      "11581    0.008533                       in_c_206\n",
      "12078    0.008224                       in_c_868\n",
      "13182    0.008129                     out_c_4130\n",
      "12108    0.008066                       in_c_916\n",
      "13178    0.008015                     out_c_4111\n",
      "11448    0.007855                       in_c_032\n",
      "13183    0.007824                     out_c_4131\n",
      "13181    0.007740                      out_c_412\n",
      "11528    0.007713                       in_c_138\n",
      "12578    0.007697                     out_c_2809\n",
      "12232    0.007600                     out_c_1629\n",
      "11860    0.007523                       in_c_560\n",
      "12251    0.007477                     out_c_1741\n",
      "11748    0.007452                       in_c_421\n",
      "12230    0.007408                     out_c_1625\n",
      "11736    0.007399                       in_c_409\n",
      "12092    0.007294                       in_c_894\n",
      "11510    0.007278                       in_c_103\n",
      "12053    0.007142                       in_c_837\n",
      "11440    0.007095                       in_c_024\n",
      "11686    0.006965                       in_c_341\n",
      "14672    0.006908                    out_c_78650\n",
      "13179    0.006891                    out_c_41181\n",
      "340      0.006837   Median_IPAnnualDeductibleAmt\n",
      "11890    0.006791                       in_c_602\n",
      "11902    0.006790                       in_c_620\n",
      "11986    0.006764                       in_c_734\n",
      "12027    0.006569                       in_c_802\n",
      "11629    0.006487                       in_c_260\n",
      "13219    0.006479                    out_c_42613\n",
      "12033    0.006466                       in_c_811\n",
      "11452    0.006462                       in_c_036\n",
      "12231    0.006375                     out_c_1628\n",
      "11471    0.006350                       in_c_064\n",
      "11495    0.006139                       in_c_088\n",
      "13069    0.006124                    out_c_36616\n",
      "11982    0.006105                       in_c_727\n",
      "11590    0.006081                       in_c_221\n",
      "11465    0.005884                       in_c_058\n",
      "15306    0.005878                     out_c_V529\n",
      "11745    0.005825                       in_c_418\n",
      "11487    0.005799                       in_c_080\n",
      "13244    0.005793                    out_c_42769\n",
      "14089    0.005771                     out_c_7144\n",
      "12052    0.005711                       in_c_836\n",
      "12256    0.005627                     out_c_1746\n",
      "12266    0.005330                      out_c_185\n",
      "13145    0.005313                     out_c_4019\n",
      "11689    0.005288                       in_c_344\n",
      "11995    0.005259                       in_c_743\n",
      "12613    0.005235                     out_c_2859\n",
      "12030    0.005228                       in_c_808\n",
      "13013    0.005161                     out_c_3555\n",
      "14088    0.004992                    out_c_71433\n",
      "12000    0.004977                       in_c_748\n",
      "11971    0.004927                       in_c_713\n",
      "11451    0.004844                       in_c_035\n",
      "14697    0.004818                     out_c_7880\n",
      "12265    0.004807                     out_c_1830\n",
      "13070    0.004755                    out_c_36617\n",
      "12302    0.004599                    out_c_20921\n",
      "13877    0.004594                     out_c_5941\n",
      "12124    0.004582                       in_c_941\n",
      "13880    0.004577                     out_c_5949\n",
      "13379    0.004546                     out_c_4439\n",
      "13346    0.004542                     out_c_4404\n",
      "11573    0.004414                       in_c_198\n",
      "14731    0.004391                    out_c_78903\n",
      "12445    0.004342                    out_c_25000\n",
      "12558    0.004305                     out_c_2766\n",
      "13564    0.004219                      out_c_490\n",
      "12055    0.004192                       in_c_839\n",
      "13192    0.004158                     out_c_4142\n",
      "11478    0.004050                       in_c_071\n",
      "12254    0.004042                     out_c_1744\n",
      "13237    0.004034                    out_c_42731\n",
      "12281    0.004004                     out_c_1970\n",
      "11920    0.003999                       in_c_644\n",
      "15303    0.003975                     out_c_V521\n",
      "11884    0.003972                       in_c_596\n",
      "14653    0.003938                     out_c_7851\n",
      "11877    0.003932                       in_c_583\n",
      "15304    0.003883                     out_c_V524\n",
      "15363    0.003874                    out_c_V5869\n",
      "13185    0.003847                    out_c_41400\n",
      "13796    0.003828                    out_c_57420\n",
      "15337    0.003819                     out_c_V571\n",
      "11456    0.003805                       in_c_040\n",
      "12115    0.003783                       in_c_923\n",
      "12008    0.003755                       in_c_759\n",
      "11472    0.003735                       in_c_065\n",
      "12135    0.003715                       in_c_958\n",
      "13093    0.003666                     out_c_3669\n",
      "13212    0.003603                    out_c_42491\n",
      "14604    0.003585                    out_c_78079\n",
      "13195    0.003568                     out_c_4149\n",
      "13245    0.003565                    out_c_42781\n",
      "11442    0.003523                       in_c_026\n",
      "11892    0.003518                       in_c_604\n",
      "13860    0.003493                     out_c_5929\n",
      "12253    0.003462                     out_c_1743\n",
      "12575    0.003455                     out_c_2800\n",
      "13061    0.003387                    out_c_36604\n",
      "11987    0.003350                       in_c_735\n",
      "13243    0.003345                    out_c_42761\n",
      "12260    0.003323                     out_c_1759\n",
      "14664    0.003193                    out_c_78605\n",
      "11894    0.003185                       in_c_606\n",
      "14425    0.003180                     out_c_7246\n",
      "14390    0.003176                     out_c_7224\n",
      "15310    0.003174                    out_c_V5339\n",
      "12252    0.003157                     out_c_1742\n",
      "13694    0.003135                    out_c_55010\n",
      "13878    0.003132                     out_c_5942\n",
      "13057    0.003043                    out_c_36600\n",
      "11754    0.003042                       in_c_433\n",
      "13075    0.003029                    out_c_36622\n",
      "11521    0.003026                       in_c_131\n",
      "14426    0.003001                    out_c_72470\n",
      "336      0.002993     Mean_OPAnnualDeductibleAmt\n",
      "11533    0.002872                       in_c_149\n",
      "13858    0.002870                     out_c_5920\n",
      "13718    0.002861                    out_c_55321\n",
      "11998    0.002847                       in_c_746\n",
      "11997    0.002841                       in_c_745\n",
      "14551    0.002830                    out_c_73303\n",
      "13733    0.002794                     out_c_5571\n",
      "11878    0.002745                       in_c_584\n",
      "13732    0.002745                     out_c_5570\n",
      "12004    0.002723                       in_c_755\n",
      "12288    0.002681                    out_c_20300\n",
      "13073    0.002663                    out_c_36620\n",
      "14730    0.002661                    out_c_78902\n",
      "12284    0.002646                     out_c_1983\n",
      "13276    0.002635                      out_c_431\n",
      "12533    0.002623                     out_c_2724\n",
      "13738    0.002613                    out_c_55842\n",
      "13340    0.002605                    out_c_44021\n",
      "14393    0.002578                     out_c_7226\n",
      "11832    0.002572                       in_c_517\n",
      "15068    0.002570                    out_c_V0481\n",
      "12255    0.002566                     out_c_1745\n",
      "13910    0.002533                    out_c_60021\n",
      "14130    0.002487                    out_c_71595\n",
      "14735    0.002486                    out_c_78907\n",
      "13218    0.002466                    out_c_42612\n",
      "15305    0.002448                     out_c_V528\n",
      "14816    0.002425                    out_c_79439\n",
      "12041    0.002384                       in_c_822\n",
      "12599    0.002362                     out_c_2839\n",
      "14092    0.002357                     out_c_7149\n",
      "14399    0.002312                    out_c_72281\n",
      "14764    0.002307                    out_c_78966\n",
      "12259    0.002300                     out_c_1750\n",
      "15308    0.002288                    out_c_V5331\n",
      "12003    0.002288                       in_c_754\n",
      "12114    0.002245                       in_c_922\n",
      "11531    0.002232                       in_c_147\n",
      "15373    0.002215                    out_c_V5882\n",
      "14549    0.002210                    out_c_73301\n",
      "12058    0.002179                       in_c_842\n",
      "13091    0.002178                    out_c_36653\n",
      "14734    0.002152                    out_c_78906\n",
      "13063    0.002147                    out_c_36610\n",
      "14400    0.002145                    out_c_72282\n",
      "14383    0.002131                    out_c_72210\n",
      "12306    0.002130                     out_c_2113\n",
      "14729    0.002119                    out_c_78901\n",
      "13144    0.002081                     out_c_4011\n",
      "12228    0.002078                     out_c_1623\n",
      "12321    0.002078                     out_c_2325\n",
      "14499    0.002074                     out_c_7282\n",
      "14851    0.002066                    out_c_82021\n",
      "12610    0.002066                    out_c_28522\n",
      "13691    0.002061                    out_c_55001\n",
      "13992    0.002058                     out_c_7012\n",
      "11530    0.002051                       in_c_146\n",
      "13915    0.002043                     out_c_6100\n",
      "14759    0.002036                    out_c_78961\n",
      "15236    0.002024                    out_c_V4509\n",
      "15057    0.002020                    out_c_V0382\n",
      "15010    0.002019                    out_c_99674\n",
      "12276    0.002017                     out_c_1889\n",
      "13699    0.002015                    out_c_55102\n",
      "13280    0.001990                    out_c_43300\n",
      "11466    0.001988                       in_c_059\n",
      "12905    0.001923                      out_c_340\n",
      "15344    0.001922                     out_c_V579\n",
      "14921    0.001922                     out_c_8489\n",
      "14663    0.001915                    out_c_78604\n",
      "13077    0.001890                    out_c_36630\n",
      "13298    0.001887                     out_c_4350\n",
      "14396    0.001880                    out_c_72272\n",
      "12559    0.001874                     out_c_2767\n",
      "12866    0.001874                     out_c_3320\n",
      "13253    0.001871                    out_c_42823\n",
      "13719    0.001823                     out_c_5533\n",
      "13220    0.001820                     out_c_4262\n",
      "13705    0.001816                    out_c_55202\n",
      "14429    0.001815                     out_c_7248\n",
      "14379    0.001808                     out_c_7218\n",
      "12293    0.001802                    out_c_20382\n",
      "13301    0.001795                     out_c_4353\n",
      "13602    0.001795                      out_c_514\n",
      "14387    0.001790                    out_c_72231\n",
      "13670    0.001788                    out_c_53441\n",
      "12246    0.001786                     out_c_1736\n",
      "15340    0.001781                     out_c_V573\n",
      "13605    0.001763                     out_c_5161\n",
      "14763    0.001750                    out_c_78965\n",
      "11537    0.001749                       in_c_153\n",
      "13242    0.001748                    out_c_42760\n",
      "13238    0.001744                    out_c_42732\n",
      "14550    0.001733                    out_c_73302\n",
      "11755    0.001702                       in_c_434\n",
      "14652    0.001694                     out_c_7850\n",
      "15336    0.001685                     out_c_V570\n",
      "14758    0.001681                    out_c_78960\n",
      "15097    0.001674                    out_c_V1011\n",
      "13908    0.001673                    out_c_60011\n",
      "12014    0.001671                       in_c_769\n",
      "13863    0.001667                     out_c_5932\n",
      "13341    0.001666                    out_c_44022\n",
      "14760    0.001661                    out_c_78962\n",
      "12580    0.001655                     out_c_2811\n",
      "13258    0.001653                    out_c_42840\n",
      "11977    0.001642                       in_c_722\n",
      "12063    0.001635                       in_c_847\n",
      "13809    0.001633                     out_c_5763\n",
      "13303    0.001627                     out_c_4359\n",
      "14408    0.001626                     out_c_7232\n",
      "13748    0.001626                    out_c_56203\n",
      "12110    0.001618                       in_c_918\n",
      "15342    0.001612                    out_c_V5781\n",
      "12112    0.001608                       in_c_920\n",
      "11486    0.001596                       in_c_079\n",
      "15250    0.001593                    out_c_V4581\n",
      "13223    0.001589                    out_c_42650\n",
      "13722    0.001587                     out_c_5551\n",
      "13575    0.001580                    out_c_49301\n",
      "14813    0.001575                     out_c_7942\n",
      "11991    0.001564                       in_c_739\n",
      "13752    0.001548                    out_c_56213\n",
      "13425    0.001547                     out_c_4548\n",
      "13248    0.001542                     out_c_4280\n",
      "14534    0.001541                    out_c_72992\n",
      "15074    0.001540                     out_c_V053\n",
      "14391    0.001536                    out_c_72251\n",
      "13840    0.001536                     out_c_5880\n",
      "13485    0.001531                    out_c_46431\n",
      "13089    0.001510                    out_c_36651\n",
      "12275    0.001506                     out_c_1888\n",
      "13066    0.001491                    out_c_36613\n",
      "13249    0.001484                     out_c_4281\n",
      "12739    0.001480                    out_c_29614\n",
      "13590    0.001478                      out_c_496\n",
      "13221    0.001474                     out_c_4263\n",
      "12747    0.001472                    out_c_29625\n",
      "13566    0.001469                     out_c_4911\n",
      "13749    0.001465                    out_c_56210\n",
      "13079    0.001463                    out_c_36632\n",
      "12955    0.001460                    out_c_34663\n",
      "14395    0.001459                    out_c_72271\n",
      "13696    0.001449                    out_c_55013\n",
      "13328    0.001449                     out_c_4386\n",
      "13086    0.001444                    out_c_36645\n",
      "14085    0.001440                    out_c_71430\n",
      "13275    0.001432                      out_c_430\n",
      "14728    0.001426                    out_c_78900\n",
      "12411    0.001424                     out_c_2449\n",
      "13345    0.001423                    out_c_44031\n",
      "14398    0.001414                    out_c_72280\n",
      "14413    0.001413                     out_c_7237\n",
      "12851    0.001411                    out_c_32735\n",
      "12576    0.001405                     out_c_2801\n",
      "12601    0.001394                    out_c_28409\n",
      "13644    0.001390                     out_c_5309\n",
      "13068    0.001375                    out_c_36615\n",
      "12274    0.001372                     out_c_1887\n",
      "14397    0.001369                    out_c_72273\n",
      "14526    0.001358                    out_c_72972\n",
      "13239    0.001354                    out_c_42741\n",
      "13571    0.001353                     out_c_4919\n",
      "14532    0.001353                    out_c_72990\n",
      "12649    0.001348                    out_c_29020\n",
      "12335    0.001338                     out_c_2356\n",
      "14603    0.001338                    out_c_78071\n",
      "15356    0.001329                    out_c_V5861\n",
      "11536    0.001327                       in_c_152\n",
      "15009    0.001322                    out_c_99673\n",
      "12341    0.001315                     out_c_2362\n",
      "13342    0.001313                    out_c_44023\n",
      "15012    0.001311                    out_c_99682\n",
      "15339    0.001298                    out_c_V5722\n",
      "15302    0.001287                     out_c_V520\n",
      "14368    0.001276                    out_c_72089\n",
      "11572    0.001275                       in_c_197\n",
      "13625    0.001269                     out_c_5199\n",
      "15002    0.001268                     out_c_9961\n",
      "12480    0.001268                    out_c_25083\n",
      "13234    0.001265                     out_c_4270\n",
      "14414    0.001262                     out_c_7238\n",
      "12771    0.001255                    out_c_29661\n",
      "12443    0.001254                    out_c_24990\n",
      "15003    0.001250                    out_c_99630\n",
      "12201    0.001238                     out_c_1530\n",
      "14424    0.001237                     out_c_7245\n",
      "14378    0.001233                     out_c_7217\n",
      "13078    0.001222                    out_c_36631\n",
      "12869    0.001213                    out_c_33721\n",
      "14319    0.001213                    out_c_71946\n",
      "14871    0.001208                    out_c_84200\n",
      "14519    0.001192                    out_c_72930\n",
      "13060    0.001190                    out_c_36603\n",
      "13612    0.001190                     out_c_5178\n",
      "14599    0.001188                    out_c_78060\n",
      "14761    0.001188                    out_c_78963\n",
      "12729    0.001186                    out_c_29601\n",
      "12290    0.001184                    out_c_20302\n",
      "13080    0.001175                    out_c_36633\n",
      "13136    0.001175                     out_c_3963\n",
      "13589    0.001170                     out_c_4941\n",
      "12271    0.001170                     out_c_1884\n",
      "14406    0.001160                     out_c_7230\n",
      "14548    0.001159                    out_c_73300\n",
      "13852    0.001158                     out_c_5902\n",
      "12972    0.001156                     out_c_3481\n",
      "13810    0.001154                     out_c_5764\n",
      "14086    0.001144                    out_c_71431\n",
      "13138    0.001143                     out_c_3969\n",
      "13882    0.001143                     out_c_5951\n",
      "13092    0.001139                     out_c_3668\n",
      "13913    0.001138                    out_c_60091\n",
      "14736    0.001107                    out_c_78909\n",
      "11447    0.001106                       in_c_031\n",
      "13623    0.001088                     out_c_5194\n",
      "13082    0.001081                    out_c_36641\n",
      "13299    0.001076                     out_c_4351\n",
      "13064    0.001067                    out_c_36611\n",
      "13317    0.001062                    out_c_43820\n",
      "13285    0.001059                    out_c_43321\n",
      "14620    0.001054                    out_c_78191\n",
      "12222    0.001053                     out_c_1573\n",
      "15200    0.001052                     out_c_V426\n",
      "13256    0.001049                    out_c_42832\n",
      "14125    0.001045                    out_c_71590\n",
      "13634    0.001034                     out_c_5303\n",
      "13330    0.001032                    out_c_43881\n",
      "13697    0.001032                    out_c_55090\n",
      "12611    0.001031                    out_c_28529\n",
      "14457    0.001028                     out_c_7268\n",
      "12248    0.001026                     out_c_1738\n",
      "14381    0.001026                    out_c_72191\n",
      "13157    0.001024                    out_c_40391\n",
      "14450    0.001013                    out_c_72665\n",
      "12298    0.001010                    out_c_20914\n",
      "12701    0.001008                    out_c_29553\n",
      "14416    0.001007                    out_c_72400\n",
      "12591    0.001007                     out_c_2827\n",
      "13746    0.001003                    out_c_56201\n",
      "14404    0.001003                    out_c_72292\n",
      "14864    0.001003                     out_c_8409\n",
      "13909    0.001003                    out_c_60020\n",
      "14872    0.000998                    out_c_84201\n",
      "13601    0.000995                     out_c_5131\n",
      "12825    0.000986                      out_c_311\n",
      "13723    0.000984                     out_c_5552\n",
      "14750    0.000981                    out_c_78942\n",
      "12250    0.000980                     out_c_1740\n",
      "13824    0.000973                     out_c_5794\n",
      "14695    0.000962                    out_c_78791\n",
      "14878    0.000950                    out_c_84213\n",
      "14428    0.000948                    out_c_72479\n",
      "13604    0.000947                     out_c_5160\n",
      "12257    0.000943                     out_c_1748\n",
      "13654    0.000942                    out_c_53221\n",
      "12587    0.000941                     out_c_2821\n",
      "13347    0.000939                     out_c_4408\n",
      "12001    0.000935                       in_c_749\n",
      "12133    0.000934                       in_c_956\n",
      "15376    0.000922                     out_c_V589\n",
      "13081    0.000921                    out_c_36634\n",
      "14032    0.000920                    out_c_70701\n",
      "13058    0.000918                    out_c_36601\n",
      "13160    0.000917                    out_c_40402\n",
      "12589    0.000915                     out_c_2823\n",
      "13344    0.000913                    out_c_44029\n",
      "14470    0.000913                    out_c_72740\n",
      "12376    0.000911                     out_c_2390\n",
      "14370    0.000909                     out_c_7210\n",
      "13062    0.000909                    out_c_36609\n",
      "12516    0.000905                     out_c_2682\n",
      "13241    0.000903                     out_c_4275\n",
      "15341    0.000897                     out_c_V574\n",
      "15013    0.000895                    out_c_99685\n",
      "13565    0.000895                     out_c_4910\n",
      "14097    0.000894                    out_c_71511\n",
      "12588    0.000890                     out_c_2822\n",
      "14409    0.000890                     out_c_7233\n",
      "14384    0.000878                    out_c_72211\n",
      "14600    0.000877                    out_c_78061\n",
      "15120    0.000875                    out_c_V1269\n",
      "13292    0.000874                    out_c_43400\n",
      "12362    0.000874                     out_c_2382\n",
      "13353    0.000870                     out_c_4411\n",
      "12721    0.000868                    out_c_29585\n",
      "13065    0.000867                    out_c_36612\n",
      "12669    0.000866                    out_c_29500\n",
      "13750    0.000865                    out_c_56211\n",
      "15483    0.000865                    out_c_V7644\n",
      "13866    0.000860                     out_c_5935\n",
      "14375    0.000859                    out_c_72142\n",
      "12608    0.000853                     out_c_2851\n",
      "13028    0.000849                     out_c_3576\n",
      "12532    0.000848                     out_c_2723\n",
      "13703    0.000847                    out_c_55200\n",
      "13386    0.000847                     out_c_4464\n",
      "13362    0.000843                     out_c_4422\n",
      "13923    0.000833                     out_c_6111\n",
      "13712    0.000832                    out_c_55300\n",
      "12584    0.000828                     out_c_2818\n",
      "13259    0.000828                    out_c_42841\n",
      "14401    0.000828                    out_c_72283\n",
      "14762    0.000827                    out_c_78964\n",
      "13465    0.000824                    out_c_45933\n",
      "14068    0.000823                     out_c_7103\n",
      "14669    0.000822                     out_c_7862\n",
      "14733    0.000821                    out_c_78905\n",
      "14771    0.000820                    out_c_79029\n",
      "12249    0.000820                     out_c_1739\n",
      "13187    0.000819                    out_c_41402\n",
      "13804    0.000819                     out_c_5752\n",
      "13698    0.000813                    out_c_55091\n",
      "14377    0.000813                     out_c_7216\n",
      "12160    0.000813                     out_c_0204\n",
      "12229    0.000803                     out_c_1624\n",
      "13800    0.000801                    out_c_57471\n",
      "12023    0.000798                       in_c_782\n",
      "13003    0.000798                     out_c_3538\n",
      "15328    0.000794                     out_c_V558\n",
      "13354    0.000791                     out_c_4412\n",
      "12597    0.000791                    out_c_28319\n",
      "14371    0.000791                     out_c_7211\n",
      "14516    0.000791                     out_c_7290\n",
      "14419    0.000788                    out_c_72409\n",
      "15390    0.000786                    out_c_V6546\n",
      "14441    0.000785                    out_c_72633\n",
      "13627    0.000784                    out_c_53010\n",
      "14808    0.000783                    out_c_79412\n",
      "12530    0.000780                     out_c_2721\n",
      "14619    0.000779                     out_c_7819\n",
      "13233    0.000777                     out_c_4269\n",
      "14412    0.000777                     out_c_7236\n",
      "14034    0.000776                    out_c_70703\n",
      "14388    0.000776                    out_c_72232\n",
      "14659    0.000775                    out_c_78600\n",
      "12778    0.000774                    out_c_29680\n",
      "12859    0.000773                     out_c_3310\n",
      "13607    0.000766                     out_c_5163\n",
      "14552    0.000766                    out_c_73309\n",
      "15008    0.000763                    out_c_99669\n",
      "12603    0.000758                     out_c_2842\n",
      "15227    0.000750                    out_c_V4450\n",
      "13278    0.000749                     out_c_4321\n",
      "14084    0.000747                     out_c_7142\n",
      "14365    0.000746                     out_c_7201\n",
      "15213    0.000745                    out_c_V4362\n",
      "13592    0.000743                     out_c_5100\n",
      "14794    0.000736                     out_c_7934\n",
      "13801    0.000726                    out_c_57480\n",
      "15343    0.000726                    out_c_V5789\n",
      "15004    0.000726                    out_c_99645\n",
      "14458    0.000717                    out_c_72690\n",
      "14680    0.000717                    out_c_78701\n",
      "15102    0.000714                    out_c_V1051\n",
      "13102    0.000713                    out_c_38619\n",
      "14033    0.000706                    out_c_70702\n",
      "15374    0.000704                    out_c_V5883\n",
      "14411    0.000692                     out_c_7235\n",
      "14660    0.000692                    out_c_78601\n",
      "12555    0.000687                    out_c_27650\n",
      "12485    0.000685                    out_c_25200\n",
      "13936    0.000683                     out_c_6119\n",
      "12264    0.000681                     out_c_1828\n",
      "14178    0.000679                    out_c_71654\n",
      "12300    0.000678                    out_c_20916\n",
      "14591    0.000678                     out_c_7804\n",
      "15130    0.000677                     out_c_V134\n",
      "13807    0.000672                     out_c_5758\n",
      "14766    0.000669                    out_c_78969\n",
      "13734    0.000669                     out_c_5579\n",
      "14374    0.000666                    out_c_72141\n",
      "12211    0.000664                     out_c_1540\n",
      "12952    0.000664                    out_c_34660\n",
      "12594    0.000659                     out_c_2830\n",
      "13208    0.000659                     out_c_4241\n",
      "13171    0.000658                    out_c_40501\n",
      "14791    0.000657                     out_c_7931\n",
      "12553    0.000653                     out_c_2763\n",
      "14343    0.000649                     out_c_7197\n",
      "13424    0.000647                     out_c_4542\n",
      "14696    0.000646                    out_c_78799\n",
      "13930    0.000643                    out_c_61172\n",
      "14510    0.000642                    out_c_72885\n",
      "13929    0.000641                    out_c_61171\n",
      "13263    0.000636                     out_c_4291\n",
      "15126    0.000627                    out_c_V1302\n",
      "13850    0.000625                    out_c_59010\n",
      "13701    0.000624                    out_c_55120\n",
      "14773    0.000619                     out_c_7905\n",
      "11535    0.000616                       in_c_151\n",
      "12577    0.000615                     out_c_2808\n",
      "12656    0.000615                     out_c_2908\n",
      "15493    0.000614                     out_c_V769\n",
      "13941    0.000614                     out_c_6202\n",
      "13255    0.000614                    out_c_42831\n",
      "13240    0.000613                    out_c_42742\n",
      "12208    0.000613                     out_c_1537\n",
      "13167    0.000608                    out_c_40490\n",
      "13747    0.000605                    out_c_56202\n",
      "13839    0.000599                      out_c_586\n",
      "13151    0.000599                    out_c_40291\n",
      "13907    0.000598                    out_c_60010\n",
      "15119    0.000598                    out_c_V1261\n",
      "12561    0.000595                     out_c_2769\n",
      "12439    0.000593                    out_c_24970\n",
      "14833    0.000593                    out_c_79673\n",
      "13568    0.000589                    out_c_49121\n",
      "12768    0.000588                    out_c_29655\n",
      "14405    0.000588                    out_c_72293\n",
      "14394    0.000588                    out_c_72270\n",
      "15285    0.000586                    out_c_V4967\n",
      "13004    0.000586                     out_c_3539\n",
      "12738    0.000584                    out_c_29613\n",
      "12225    0.000582                     out_c_1579\n",
      "13827    0.000579                     out_c_5845\n",
      "13494    0.000574                    out_c_46619\n",
      "13146    0.000573                    out_c_40200\n",
      "12303    0.000571                    out_c_20924\n",
      "13520    0.000570                    out_c_47826\n",
      "12317    0.000569                     out_c_2321\n",
      "13381    0.000568                     out_c_4461\n",
      "12529    0.000568                     out_c_2720\n",
      "12592    0.000567                     out_c_2828\n",
      "14105    0.000566                    out_c_71520\n",
      "15434    0.000565                    out_c_V7281\n",
      "13739    0.000564                     out_c_5589\n",
      "14618    0.000562                     out_c_7818\n",
      "14417    0.000561                    out_c_72401\n",
      "12772    0.000556                    out_c_29662\n",
      "15209    0.000554                     out_c_V434\n",
      "14703    0.000554                    out_c_78830\n",
      "14403    0.000552                    out_c_72291\n",
      "12808    0.000552                     out_c_3004\n",
      "14121    0.000550                    out_c_71537\n",
      "12291    0.000550                    out_c_20380\n",
      "13024    0.000550                     out_c_3571\n",
      "11513    0.000549                       in_c_115\n",
      "14369    0.000548                     out_c_7209\n",
      "13097    0.000547                    out_c_38603\n",
      "14091    0.000547                    out_c_71489\n",
      "13110    0.000546                    out_c_38640\n",
      "12726    0.000546                    out_c_29594\n",
      "14828    0.000545                     out_c_7961\n",
      "13808    0.000544                     out_c_5761\n",
      "14857    0.000544                     out_c_8402\n",
      "13872    0.000542                    out_c_59381\n",
      "14373    0.000541                     out_c_7213\n",
      "12452    0.000540                    out_c_25013\n",
      "15283    0.000537                    out_c_V4965\n",
      "12631    0.000534                     out_c_2874\n",
      "13452    0.000534                     out_c_4588\n",
      "14104    0.000532                    out_c_71518\n",
      "12871    0.000532                     out_c_3380\n",
      "13572    0.000531                     out_c_4920\n",
      "13051    0.000530                    out_c_36524\n",
      "13906    0.000528                    out_c_60001\n",
      "13039    0.000526                    out_c_35922\n",
      "13642    0.000526                    out_c_53085\n",
      "12750    0.000526                    out_c_29631\n",
      "13912    0.000523                    out_c_60090\n",
      "13851    0.000520                    out_c_59011\n",
      "13859    0.000519                     out_c_5921\n",
      "13730    0.000518                     out_c_5566\n",
      "14506    0.000517                    out_c_72881\n",
      "12862    0.000516                     out_c_3312\n",
      "15490    0.000516                    out_c_V7652\n",
      "14512    0.000515                    out_c_72887\n",
      "11491    0.000515                       in_c_084\n",
      "12287    0.000515                    out_c_20280\n",
      "14783    0.000512                     out_c_7915\n",
      "14083    0.000512                     out_c_7141\n",
      "14422    0.000511                     out_c_7243\n",
      "14509    0.000511                    out_c_72884\n",
      "12748    0.000511                    out_c_29626\n",
      "14037    0.000508                    out_c_70706\n",
      "12544    0.000506                     out_c_2752\n",
      "13329    0.000505                     out_c_4387\n",
      "13945    0.000504                     out_c_6271\n",
      "14490    0.000502                    out_c_72783\n",
      "14047    0.000502                    out_c_70719\n",
      "15405    0.000501                     out_c_V676\n",
      "14366    0.000500                     out_c_7202\n",
      "13312    0.000500                     out_c_4379\n",
      "12551    0.000499                     out_c_2761\n",
      "13708    0.000499                    out_c_55229\n",
      "13262    0.000497                     out_c_4289\n",
      "15350    0.000495                    out_c_V5832\n",
      "12662    0.000495                    out_c_29383\n",
      "14093    0.000493                    out_c_71500\n",
      "12586    0.000492                     out_c_2820\n",
      "12278    0.000490                     out_c_1891\n",
      "12579    0.000490                     out_c_2810\n",
      "13751    0.000490                    out_c_56212\n",
      "13306    0.000489                     out_c_4371\n",
      "13875    0.000487                     out_c_5939\n",
      "13300    0.000487                     out_c_4352\n",
      "13475    0.000486                     out_c_4618\n",
      "12612    0.000481                     out_c_2858\n",
      "13650    0.000478                    out_c_53141\n",
      "14514    0.000476                    out_c_72889\n",
      "13512    0.000476                     out_c_4772\n",
      "13617    0.000474                     out_c_5184\n",
      "12552    0.000472                     out_c_2762\n",
      "12861    0.000472                    out_c_33119\n",
      "13281    0.000471                    out_c_43301\n",
      "12355    0.000470                     out_c_2376\n",
      "13295    0.000469                    out_c_43411\n",
      "14128    0.000468                    out_c_71593\n",
      "12476    0.000468                    out_c_25073\n",
      "14111    0.000468                    out_c_71526\n",
      "13760    0.000466                    out_c_56481\n",
      "12556    0.000465                    out_c_27651\n",
      "15338    0.000464                    out_c_V5721\n",
      "14732    0.000464                    out_c_78904\n",
      "15422    0.000463                     out_c_V717\n",
      "15266    0.000462                     out_c_V481\n",
      "14774    0.000461                     out_c_7906\n",
      "12279    0.000458                      out_c_193\n",
      "15229    0.000457                    out_c_V4452\n",
      "13682    0.000456                    out_c_53551\n",
      "13848    0.000455                    out_c_59000\n",
      "13647    0.000455                    out_c_53120\n",
      "12319    0.000449                     out_c_2323\n",
      "12471    0.000448                    out_c_25062\n",
      "12746    0.000447                    out_c_29624\n",
      "13855    0.000446                    out_c_59081\n",
      "14769    0.000446                    out_c_79021\n",
      "13100    0.000445                    out_c_38611\n",
      "14517    0.000443                     out_c_7291\n",
      "14784    0.000443                     out_c_7916\n",
      "14805    0.000443                    out_c_79409\n",
      "15217    0.000441                    out_c_V4366\n"
     ]
    }
   ],
   "source": [
    "#feature importances\n",
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "\n",
    "# training and testing sets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, random_state = 42, stratify=y)\n",
    "\n",
    "bestmodel = LogisticRegression(C=7, class_weight='balanced', dual=False, fit_intercept=True,\n",
    "intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='warn', n_jobs=None, penalty='l2', \n",
    "random_state=30, solver='saga', tol=0.0001, verbose=0,\n",
    "warm_start=False)\n",
    "\n",
    "bestmodel.fit(X_train, y_train)\n",
    "\n",
    "feature_frame = pd.DataFrame({'Importance':i for i in bestmodel.coef_})\n",
    "feature_array = feature_frame['Importance'].values * np.std(X_train, 0).values\n",
    "feature_frame['Importance'] = feature_array\n",
    "feature_frame['Variable'] = X.columns\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "print(feature_frame.sort_values('Importance', ascending=False).head(100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
