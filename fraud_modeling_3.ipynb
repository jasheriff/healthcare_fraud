{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Models\n",
    "##### Key models  \n",
    "\n",
    "* Random Forest increased the best model's Class 1 Recall by .06 points to .81, with a marginal decrease in accuracy. Class 1 Precision dropped by .09, so with the gain in recall, we also had more observations incorrectly identified as fraud.\n",
    "\n",
    "type, undersampling method | Class 1 Precision | Class 1 Recall | Accuracy Score | \n",
    "------- | ------------ | -------------- | --------------- | \n",
    "BEST: random forest, random under sampling | 0.37 | .81 | .853 | \n",
    "general random forest | .77 | .32 | .928  | .928 |\n",
    "xg_boost |.70 | .45 | .93 |\n",
    "* lasso cv, random under sampling | 0.46 | 0.75 | 0.885\n",
    "* ridge cv, random under sampling | 0.46 | 0.75 | 0.885\n",
    "\n",
    "\n",
    "\n",
    "\"*\" = best prior model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try resampling with auto and 1 for ross. see which is better and proceed \n",
    "# choose resampling methods from previous that worked better. \n",
    "# figure out xd boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_general(df, target, method):\n",
    "    # baseline logistic regression. \n",
    "    # penalty = 'l2', ridge. , solver = 'liblinear'\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    # training and testing sets\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42, stratify=y)\n",
    "\n",
    "    # Instantiate model\n",
    "    rf = RandomForestClassifier(oob_score=True, random_state=30)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Out of Bag Error\n",
    "    # oob_score = rf.oob_score_ # score  = 1- oob error\n",
    "    print('OOB Score: %.2f' % rf.oob_score_ )\n",
    "    \n",
    "    # Scores\n",
    "    y_predict_test = rf.predict(X_test)\n",
    "    print(\"Test accuracy score:\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "     \n",
    "def random_forest_tuned(df, target, method):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    # training and testing sets\n",
    "    X_sample, X_test, y_sample, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42, stratify=y)\n",
    "    \n",
    "    # resampling methods\n",
    "    # sampling strategy = (ratio : minority / majority) could be 'auto' or 1. could be something else.\n",
    "    if method == 'ros':\n",
    "        X_train, y_train = RandomOverSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'ADASYN':\n",
    "        X_train, y_train = ADASYN(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'SMOTE':\n",
    "        X_train, y_train = SMOTE(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'rus':\n",
    "        X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'cc':\n",
    "        X_train, y_train = ClusterCentroids(random_state=0).fit_resample(X_sample, y_sample)   \n",
    "    if method == 'NearMiss':\n",
    "        X_train, y_train = NearMiss(version=1, random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'none':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "    \n",
    "    # Instantiate model\n",
    "    rf = RandomForestClassifier(oob_score=True, random_state=30)\n",
    "\n",
    "    #Parameters\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 400, stop = 700, num = 4)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features} \n",
    "    \n",
    "    # search across different combinations of parameters, and use all available scores\n",
    "    rf_tuned = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 3, random_state=42)\n",
    "    \n",
    "    # Train the model on training data\n",
    "    rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "    # Scores\n",
    "    y_predict_test = rf_tuned.predict(X_test)\n",
    "    print(\"Test accuracy score:\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "    # Best Parameters:\n",
    "    print(\"Best Parameters\", rf_tuned.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_data = pd.read_csv('/Users/Julia/Documents/bootcamp/fraud_capstone/data_out/train_final_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChronicCond_Alzheimer</th>\n",
       "      <th>ChronicCond_Cancer</th>\n",
       "      <th>ChronicCond_Depression</th>\n",
       "      <th>ChronicCond_Diabetes</th>\n",
       "      <th>ChronicCond_Heartfailure</th>\n",
       "      <th>ChronicCond_IschemicHeart</th>\n",
       "      <th>ChronicCond_KidneyDisease</th>\n",
       "      <th>ChronicCond_ObstrPulmonary</th>\n",
       "      <th>ChronicCond_Osteoporasis</th>\n",
       "      <th>ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>ChronicCond_stroke</th>\n",
       "      <th>County_0</th>\n",
       "      <th>County_1</th>\n",
       "      <th>County_10</th>\n",
       "      <th>County_100</th>\n",
       "      <th>County_11</th>\n",
       "      <th>County_110</th>\n",
       "      <th>County_111</th>\n",
       "      <th>County_113</th>\n",
       "      <th>County_117</th>\n",
       "      <th>County_120</th>\n",
       "      <th>County_130</th>\n",
       "      <th>County_131</th>\n",
       "      <th>County_14</th>\n",
       "      <th>County_140</th>\n",
       "      <th>County_141</th>\n",
       "      <th>County_150</th>\n",
       "      <th>County_160</th>\n",
       "      <th>County_161</th>\n",
       "      <th>County_170</th>\n",
       "      <th>County_180</th>\n",
       "      <th>County_190</th>\n",
       "      <th>County_191</th>\n",
       "      <th>County_194</th>\n",
       "      <th>County_20</th>\n",
       "      <th>County_200</th>\n",
       "      <th>County_210</th>\n",
       "      <th>County_211</th>\n",
       "      <th>County_212</th>\n",
       "      <th>County_213</th>\n",
       "      <th>County_220</th>\n",
       "      <th>County_221</th>\n",
       "      <th>County_222</th>\n",
       "      <th>County_223</th>\n",
       "      <th>County_224</th>\n",
       "      <th>County_230</th>\n",
       "      <th>County_240</th>\n",
       "      <th>County_241</th>\n",
       "      <th>County_25</th>\n",
       "      <th>County_250</th>\n",
       "      <th>...</th>\n",
       "      <th>proc_9764.0</th>\n",
       "      <th>proc_9784.0</th>\n",
       "      <th>proc_9787.0</th>\n",
       "      <th>proc_9789.0</th>\n",
       "      <th>proc_9805.0</th>\n",
       "      <th>proc_9815.0</th>\n",
       "      <th>proc_9851.0</th>\n",
       "      <th>proc_9903.0</th>\n",
       "      <th>proc_9904.0</th>\n",
       "      <th>proc_9905.0</th>\n",
       "      <th>proc_9906.0</th>\n",
       "      <th>proc_9907.0</th>\n",
       "      <th>proc_9910.0</th>\n",
       "      <th>proc_9914.0</th>\n",
       "      <th>proc_9915.0</th>\n",
       "      <th>proc_9916.0</th>\n",
       "      <th>proc_9917.0</th>\n",
       "      <th>proc_9918.0</th>\n",
       "      <th>proc_9919.0</th>\n",
       "      <th>proc_9920.0</th>\n",
       "      <th>proc_9921.0</th>\n",
       "      <th>proc_9922.0</th>\n",
       "      <th>proc_9923.0</th>\n",
       "      <th>proc_9925.0</th>\n",
       "      <th>proc_9926.0</th>\n",
       "      <th>proc_9928.0</th>\n",
       "      <th>proc_9929.0</th>\n",
       "      <th>proc_9938.0</th>\n",
       "      <th>proc_9939.0</th>\n",
       "      <th>proc_9952.0</th>\n",
       "      <th>proc_9955.0</th>\n",
       "      <th>proc_9959.0</th>\n",
       "      <th>proc_9960.0</th>\n",
       "      <th>proc_9961.0</th>\n",
       "      <th>proc_9962.0</th>\n",
       "      <th>proc_9969.0</th>\n",
       "      <th>proc_9971.0</th>\n",
       "      <th>proc_9972.0</th>\n",
       "      <th>proc_9973.0</th>\n",
       "      <th>proc_9974.0</th>\n",
       "      <th>proc_9975.0</th>\n",
       "      <th>proc_9978.0</th>\n",
       "      <th>proc_9979.0</th>\n",
       "      <th>proc_9982.0</th>\n",
       "      <th>proc_9984.0</th>\n",
       "      <th>proc_9986.0</th>\n",
       "      <th>proc_9992.0</th>\n",
       "      <th>proc_9995.0</th>\n",
       "      <th>proc_9998.0</th>\n",
       "      <th>proc_9999.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.365759</td>\n",
       "      <td>0.233463</td>\n",
       "      <td>0.451362</td>\n",
       "      <td>0.754864</td>\n",
       "      <td>0.564202</td>\n",
       "      <td>0.762646</td>\n",
       "      <td>0.474708</td>\n",
       "      <td>0.400778</td>\n",
       "      <td>0.272374</td>\n",
       "      <td>0.330739</td>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.426901</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.730994</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.380117</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.345029</td>\n",
       "      <td>0.076023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429515</td>\n",
       "      <td>0.229075</td>\n",
       "      <td>0.451542</td>\n",
       "      <td>0.685022</td>\n",
       "      <td>0.596916</td>\n",
       "      <td>0.799559</td>\n",
       "      <td>0.398678</td>\n",
       "      <td>0.341410</td>\n",
       "      <td>0.370044</td>\n",
       "      <td>0.290749</td>\n",
       "      <td>0.063877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.496454</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.773050</td>\n",
       "      <td>0.624113</td>\n",
       "      <td>0.794326</td>\n",
       "      <td>0.460993</td>\n",
       "      <td>0.304965</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.099291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322917</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 16888 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ChronicCond_Alzheimer  ChronicCond_Cancer  ChronicCond_Depression  ChronicCond_Diabetes  ChronicCond_Heartfailure  ChronicCond_IschemicHeart  ChronicCond_KidneyDisease  ChronicCond_ObstrPulmonary  ChronicCond_Osteoporasis  ChronicCond_rheumatoidarthritis  ChronicCond_stroke  County_0  County_1  County_10  County_100  County_11  County_110  County_111  County_113  County_117  County_120  County_130  County_131  County_14  County_140  County_141  County_150  County_160  County_161  \\\n",
       "0               0.365759            0.233463                0.451362              0.754864                  0.564202                   0.762646                   0.474708                    0.400778                  0.272374                         0.330739            0.105058  0.011673       0.0   0.011673    0.011673        0.0         0.0         0.0         0.0         0.0         0.0    0.015564         0.0        0.0    0.003891         0.0     0.07393    0.000000         0.0   \n",
       "1               0.426901            0.175439                0.444444              0.730994                  0.649123                   0.807018                   0.473684                    0.380117                  0.280702                         0.345029            0.076023  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.023392         0.0        0.0    0.005848         0.0     0.00000    0.000000         0.0   \n",
       "2               0.429515            0.229075                0.451542              0.685022                  0.596916                   0.799559                   0.398678                    0.341410                  0.370044                         0.290749            0.063877  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.000000         0.0   \n",
       "3               0.496454            0.191489                0.446809              0.773050                  0.624113                   0.794326                   0.460993                    0.304965                  0.326241                         0.326241            0.099291  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.014184         0.0   \n",
       "4               0.322917            0.156250                0.385417              0.645833                  0.645833                   0.687500                   0.395833                    0.302083                  0.291667                         0.270833            0.104167  0.000000       0.0   0.031250    0.031250        0.0         0.0         0.0         0.0         0.0         0.0    0.020833         0.0        0.0    0.000000         0.0     0.00000    0.135417         0.0   \n",
       "\n",
       "   County_170  County_180  County_190  County_191  County_194  County_20  County_200  County_210  County_211  County_212  County_213  County_220  County_221  County_222  County_223  County_224  County_230  County_240  County_241  County_25  County_250     ...       proc_9764.0  proc_9784.0  proc_9787.0  proc_9789.0  proc_9805.0  proc_9815.0  proc_9851.0  proc_9903.0  proc_9904.0  proc_9905.0  proc_9906.0  proc_9907.0  proc_9910.0  proc_9914.0  proc_9915.0  proc_9916.0  proc_9917.0  \\\n",
       "0         0.0    0.003891    0.011673         0.0         0.0   0.003891         0.0    0.000000         0.0         0.0         0.0    0.011673         0.0         0.0         0.0         0.0    0.007782    0.011673         0.0        0.0    0.054475     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "1         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.070175         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.070175         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          2.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "2         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.000000         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.156388         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "3         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.014184         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "4         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.010417         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   proc_9918.0  proc_9919.0  proc_9920.0  proc_9921.0  proc_9922.0  proc_9923.0  proc_9925.0  proc_9926.0  proc_9928.0  proc_9929.0  proc_9938.0  proc_9939.0  proc_9952.0  proc_9955.0  proc_9959.0  proc_9960.0  proc_9961.0  proc_9962.0  proc_9969.0  proc_9971.0  proc_9972.0  proc_9973.0  proc_9974.0  proc_9975.0  proc_9978.0  proc_9979.0  proc_9982.0  proc_9984.0  proc_9986.0  proc_9992.0  proc_9995.0  proc_9998.0  proc_9999.0  \n",
       "0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "1          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "2          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "3          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "4          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "\n",
       "[5 rows x 16888 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.92\n",
      "Test accuracy score: 0.928\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      1226\n",
      "           1       0.77      0.32      0.46       127\n",
      "\n",
      "    accuracy                           0.93      1353\n",
      "   macro avg       0.85      0.66      0.71      1353\n",
      "weighted avg       0.92      0.93      0.91      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest_general(train_final_data, 'PotentialFraud', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.853\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91      1226\n",
      "           1       0.37      0.81      0.51       127\n",
      "\n",
      "    accuracy                           0.85      1353\n",
      "   macro avg       0.67      0.83      0.71      1353\n",
      "weighted avg       0.92      0.85      0.88      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=600,\n",
      "                       n_jobs=None, oob_score=True, random_state=30, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.872\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      1226\n",
      "           1       0.38      0.54      0.44       127\n",
      "\n",
      "    accuracy                           0.87      1353\n",
      "   macro avg       0.66      0.72      0.69      1353\n",
      "weighted avg       0.90      0.87      0.88      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=600,\n",
      "                       n_jobs=None, oob_score=True, random_state=30, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.635\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.60      0.75      1226\n",
      "           1       0.20      0.97      0.33       127\n",
      "\n",
      "    accuracy                           0.63      1353\n",
      "   macro avg       0.60      0.78      0.54      1353\n",
      "weighted avg       0.92      0.63      0.71      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
      "                       n_jobs=None, oob_score=True, random_state=30, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'NearMiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Author: Kian Ho <hui.kian.ho@gmail.com>\n",
    "#         Gilles Louppe <g.louppe@gmail.com>\n",
    "#         Andreas Mueller <amueller@ais.uni-bonn.de>\n",
    "#\n",
    "# License: BSD 3 Clause\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "# Generate a binary classification dataset.\n",
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "    \n",
    "# NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
    "# support for parallelized ensembles but is necessary for tracking the OOB\n",
    "# error trajectory during training.\n",
    "ensemble_clfs = [\n",
    "    (\"RandomForestClassifier, max_features='sqrt'\",\n",
    "        RandomForestClassifier(n_estimators=200,\n",
    "                               warm_start=True, max_features=\"sqrt\",\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features='log2'\",\n",
    "        RandomForestClassifier(n_estimators=200,\n",
    "                               warm_start=True, max_features='log2',\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features=None\",\n",
    "        RandomForestClassifier(n_estimators=200,\n",
    "                               warm_start=True, max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 10\n",
    "max_estimators = 400\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(min_estimators, max_estimators + 1, 5):\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))\n",
    "\n",
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "sns.set()\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model above: OOB error settles around _____ . \n",
    "* Our best tuned models had 400-600 estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELING WITH XGB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy:\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1216\n",
      "           1       0.67      0.45      0.54       137\n",
      "\n",
      "    accuracy                           0.92      1353\n",
      "   macro avg       0.81      0.71      0.75      1353\n",
      "weighted avg       0.91      0.92      0.92      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline:\n",
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# fit model \n",
    "model = XGBClassifier(seed=30)\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\n Test Accuracy:\" % (accuracy * 100.0))\n",
    "print(\"\\n Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_tuned(df, target):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    # Instantiate model\n",
    "    model = XGBClassifier(random_state=30)\n",
    "\n",
    "    #Parameters\n",
    "    cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
    "    ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "    \n",
    "    # Optimize for accuracy since that is the metric used in the Adult Data Set notation\n",
    "    model_tuned = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                    cv_params, scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "    model_tuned.fit(X_train, y_train)\n",
    "                       \n",
    "    # make predictions for test data\n",
    "    y_pred = model_tuned.predict(X_test)\n",
    "\n",
    "    # evaluate predictions\n",
    "    print(\"\\n Test Accuracy:\", (accuracy_score(y_test, y_pred) * 100.0))\n",
    "   \n",
    "    # Classification report\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Best Parameters:\n",
    "    print(\"Best Parameters\", model_tuned.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy:\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1216\n",
      "           1       0.70      0.45      0.55       137\n",
      "\n",
      "    accuracy                           0.93      1353\n",
      "   macro avg       0.82      0.72      0.76      1353\n",
      "weighted avg       0.92      0.93      0.92      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_tuned(train_final_data, \"PotentialFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.853\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>diag_40390</td>\n",
       "      <td>0.011210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Mean_Duration</td>\n",
       "      <td>0.009751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5225</th>\n",
       "      <td>diag_53081</td>\n",
       "      <td>0.008454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>diag_2449</td>\n",
       "      <td>0.008196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>diag_2762</td>\n",
       "      <td>0.008081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>diag_41401</td>\n",
       "      <td>0.007904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Median_Duration</td>\n",
       "      <td>0.007530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>diag_2851</td>\n",
       "      <td>0.007385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4527</th>\n",
       "      <td>diag_4280</td>\n",
       "      <td>0.007379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>diag_2724</td>\n",
       "      <td>0.007350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>Mean_InscClaimAmtReimbursed</td>\n",
       "      <td>0.007181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Mean_In_Out</td>\n",
       "      <td>0.006942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>diag_42731</td>\n",
       "      <td>0.006856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5613</th>\n",
       "      <td>diag_5859</td>\n",
       "      <td>0.006832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5691</th>\n",
       "      <td>diag_5990</td>\n",
       "      <td>0.006572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>diag_486</td>\n",
       "      <td>0.006570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>diag_51881</td>\n",
       "      <td>0.006315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>diag_5180</td>\n",
       "      <td>0.006228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>diag_2768</td>\n",
       "      <td>0.006023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>diag_73300</td>\n",
       "      <td>0.005966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>diag_5849</td>\n",
       "      <td>0.005965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>diag_25000</td>\n",
       "      <td>0.005632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>diag_27651</td>\n",
       "      <td>0.005275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>diag_2639</td>\n",
       "      <td>0.005244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>diag_496</td>\n",
       "      <td>0.005164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>diag_41400</td>\n",
       "      <td>0.005081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>diag_40391</td>\n",
       "      <td>0.005024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>diag_3051</td>\n",
       "      <td>0.005012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>diag_2859</td>\n",
       "      <td>0.004803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>diag_41071</td>\n",
       "      <td>0.004794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6614</th>\n",
       "      <td>diag_71536</td>\n",
       "      <td>0.002520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>diag_2720</td>\n",
       "      <td>0.002477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16298</th>\n",
       "      <td>proc_66.0</td>\n",
       "      <td>0.002423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>diag_2767</td>\n",
       "      <td>0.002404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>diag_29680</td>\n",
       "      <td>0.002356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ChronicCond_KidneyDisease</td>\n",
       "      <td>0.002351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>diag_2760</td>\n",
       "      <td>0.002340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>diag_34590</td>\n",
       "      <td>0.002339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4517</th>\n",
       "      <td>diag_42732</td>\n",
       "      <td>0.002330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10907</th>\n",
       "      <td>diag_V4581</td>\n",
       "      <td>0.002323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11074</th>\n",
       "      <td>diag_V5867</td>\n",
       "      <td>0.002321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7664</th>\n",
       "      <td>diag_78057</td>\n",
       "      <td>0.002284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>diag_1629</td>\n",
       "      <td>0.002215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6970</th>\n",
       "      <td>diag_72402</td>\n",
       "      <td>0.002214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>diag_V4501</td>\n",
       "      <td>0.002202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>diag_28521</td>\n",
       "      <td>0.002201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4427</th>\n",
       "      <td>diag_412</td>\n",
       "      <td>0.002187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>diag_2749</td>\n",
       "      <td>0.002160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>RenalDiseaseIndicator</td>\n",
       "      <td>0.002140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10629</th>\n",
       "      <td>diag_V1254</td>\n",
       "      <td>0.002131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>diag_2948</td>\n",
       "      <td>0.002118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>diag_4439</td>\n",
       "      <td>0.002091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>diag_41519</td>\n",
       "      <td>0.002085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437</th>\n",
       "      <td>diag_56400</td>\n",
       "      <td>0.002073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>diag_43491</td>\n",
       "      <td>0.001992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6576</th>\n",
       "      <td>diag_7140</td>\n",
       "      <td>0.001988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10921</th>\n",
       "      <td>diag_V462</td>\n",
       "      <td>0.001971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7652</th>\n",
       "      <td>diag_7802</td>\n",
       "      <td>0.001962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>diag_4241</td>\n",
       "      <td>0.001943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7655</th>\n",
       "      <td>diag_78039</td>\n",
       "      <td>0.001931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Variable  Importance\n",
       "4365                    diag_40390    0.011210\n",
       "330                  Mean_Duration    0.009751\n",
       "5225                    diag_53081    0.008454\n",
       "2096                     diag_2449    0.008196\n",
       "2333                     diag_2762    0.008081\n",
       "4432                    diag_41401    0.007904\n",
       "339                Median_Duration    0.007530\n",
       "2439                     diag_2851    0.007385\n",
       "4527                     diag_4280    0.007379\n",
       "2299                     diag_2724    0.007350\n",
       "335    Mean_InscClaimAmtReimbursed    0.007181\n",
       "334                    Mean_In_Out    0.006942\n",
       "4516                    diag_42731    0.006856\n",
       "5613                     diag_5859    0.006832\n",
       "5691                     diag_5990    0.006572\n",
       "4891                      diag_486    0.006570\n",
       "4980                    diag_51881    0.006315\n",
       "4972                     diag_5180    0.006228\n",
       "2341                     diag_2768    0.006023\n",
       "7171                    diag_73300    0.005966\n",
       "5606                     diag_5849    0.005965\n",
       "2130                    diag_25000    0.005632\n",
       "2337                    diag_27651    0.005275\n",
       "2249                     diag_2639    0.005244\n",
       "4930                      diag_496    0.005164\n",
       "4431                    diag_41400    0.005081\n",
       "4366                    diag_40391    0.005024\n",
       "2814                     diag_3051    0.005012\n",
       "2444                     diag_2859    0.004803\n",
       "4414                    diag_41071    0.004794\n",
       "...                            ...         ...\n",
       "6614                    diag_71536    0.002520\n",
       "2295                     diag_2720    0.002477\n",
       "16298                    proc_66.0    0.002423\n",
       "2340                     diag_2767    0.002404\n",
       "2665                    diag_29680    0.002356\n",
       "6        ChronicCond_KidneyDisease    0.002351\n",
       "2331                     diag_2760    0.002340\n",
       "3212                    diag_34590    0.002339\n",
       "4517                    diag_42732    0.002330\n",
       "10907                   diag_V4581    0.002323\n",
       "11074                   diag_V5867    0.002321\n",
       "7664                    diag_78057    0.002284\n",
       "1298                     diag_1629    0.002215\n",
       "6970                    diag_72402    0.002214\n",
       "10885                   diag_V4501    0.002202\n",
       "2440                    diag_28521    0.002201\n",
       "4427                      diag_412    0.002187\n",
       "2319                     diag_2749    0.002160\n",
       "355          RenalDiseaseIndicator    0.002140\n",
       "10629                   diag_V1254    0.002131\n",
       "2553                     diag_2948    0.002118\n",
       "4669                     diag_4439    0.002091\n",
       "4450                    diag_41519    0.002085\n",
       "5437                    diag_56400    0.002073\n",
       "4578                    diag_43491    0.001992\n",
       "6576                     diag_7140    0.001988\n",
       "10921                    diag_V462    0.001971\n",
       "7652                     diag_7802    0.001962\n",
       "4479                     diag_4241    0.001943\n",
       "7655                    diag_78039    0.001931\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "\n",
    "X_sample, X_test, y_sample, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Instantiate\n",
    "model=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=600,\n",
    "                       n_jobs=None, oob_score=True, random_state=30, verbose=0,\n",
    "                       warm_start=False)\n",
    "\n",
    "# Undersampling \n",
    "X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "y_predict_test = model.predict(X_test)\n",
    "print(\"Test accuracy score:\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "    \n",
    "# Top feature Importances    \n",
    "pd.set_option('display.max_rows', 1001)\n",
    "(pd.DataFrame({'Variable':X.columns, 'Importance':model.feature_importances_}).sort_values('Importance', ascending=False)).head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
