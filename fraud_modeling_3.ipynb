{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest models\n",
    "##### Key models  \n",
    "\n",
    "\n",
    "type    | class weight | accuracy score | recall, class 1 | oob error | n_estimators | max_features \n",
    "------- | ------------ | -------------- | --------------- | --------- | ------------ | ---------- |\n",
    "* BEST: ridge logistic tuned | balanced | .55 | .897\n",
    "random forest | \"balanced\" | .92 | .35 | .92 | \n",
    "random forest, ADASYN | \"balanced\" | .914 | .42\n",
    "BASELINE: general logistic regression | balanced |  .33   | .876\n",
    "\n",
    "\n",
    "\"*\" = best prior model \n",
    "\n",
    "### Logistic Regression with varied resampling methods\n",
    "##### Key models  \n",
    "\n",
    "\n",
    "type    | class weight | Class 1 Recall | Accuracy Score \n",
    "------- | ------------ | -------------- | ---------------\n",
    "* BEST: ridge logistic tuned | balanced | .55 | .897      \n",
    "ridge, random over sampling | balanced | .914 | .42\n",
    "ridge, ADASYN | balanced | .914 | .42\n",
    "ridge, SMOTE | balanced | .914 | .42\n",
    "ridge, random under sampling | balanced | .914 | .42\n",
    "ridge, cluster centroids | balanced | .914 | .42\n",
    "ridge, near miss | balanced | balanced | .42\n",
    "BASELINE: general logistic regression | balanced |  .33   | .876 \n",
    "\n",
    "\"*\" = best prior model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try resampling with auto and 1 for ross. see which is better and proceed \n",
    "# choose resampling methods from previous that worked better. \n",
    "# figure out xd boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_general(df, target, method):\n",
    "    # baseline logistic regression. \n",
    "    # penalty = 'l2', ridge. , solver = 'liblinear'\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    # training and testing sets\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42, stratify=y)\n",
    "\n",
    "    # Instantiate model\n",
    "    rf = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Out of Bag Error\n",
    "    # oob_score = rf.oob_score_ # score  = 1- oob error\n",
    "    print('OOB Score: %.2f' % rf.oob_score_ )\n",
    "    \n",
    "    # Scores\n",
    "    y_predict_test = rf.predict(X_test)\n",
    "    print(\"Test accuracy score:\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "     \n",
    "def random_forest_tuned(df, target, method):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    # training and testing sets\n",
    "    X_sample, X_test, y_sample, y_test = \\\n",
    "    train_test_split(X, y, random_state = 42, stratify=y)\n",
    "    \n",
    "    # resampling methods\n",
    "    # sampling strategy = (ratio : minority / majority) could be 'auto' or 1. could be something else.\n",
    "    if method == 'ros':\n",
    "        X_train, y_train = RandomOverSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'ADASYN':\n",
    "        X_train, y_train = ADASYN(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'SMOTE':\n",
    "        X_train, y_train = SMOTE(random_state=0).fit_resample(X_sample, y_sample)        \n",
    "    if method == 'rus':\n",
    "        X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'cc':\n",
    "        X_train, y_train = ClusterCentroids(random_state=0).fit_resample(X_sample, y_sample)   \n",
    "    if method == 'NearMiss':\n",
    "        X_train, y_train = NearMiss(version=1, random_state=0).fit_resample(X_sample, y_sample)\n",
    "    if method == 'none':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "    \n",
    "    # Instantiate model\n",
    "    rf = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "    #Parameters\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 400, stop = 700, num = 4)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features} \n",
    "    \n",
    "    # search across different combinations of parameters, and use all available scores\n",
    "    rf_tuned = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 3, random_state=42)\n",
    "    \n",
    "    # Train the model on training data\n",
    "    rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "    # Scores\n",
    "    y_predict_test = rf_tuned.predict(X_test)\n",
    "    print(\"Test accuracy score:\", round(accuracy_score(y_predict_test, y_test), 3))\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_predict_test))\n",
    "    \n",
    "    # Best Parameters:\n",
    "    print(\"Best Parameters\", rf_tuned.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_data = pd.read_csv('/Users/Julia/Documents/bootcamp/fraud_capstone/data_out/train_final_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChronicCond_Alzheimer</th>\n",
       "      <th>ChronicCond_Cancer</th>\n",
       "      <th>ChronicCond_Depression</th>\n",
       "      <th>ChronicCond_Diabetes</th>\n",
       "      <th>ChronicCond_Heartfailure</th>\n",
       "      <th>ChronicCond_IschemicHeart</th>\n",
       "      <th>ChronicCond_KidneyDisease</th>\n",
       "      <th>ChronicCond_ObstrPulmonary</th>\n",
       "      <th>ChronicCond_Osteoporasis</th>\n",
       "      <th>ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>ChronicCond_stroke</th>\n",
       "      <th>County_0</th>\n",
       "      <th>County_1</th>\n",
       "      <th>County_10</th>\n",
       "      <th>County_100</th>\n",
       "      <th>County_11</th>\n",
       "      <th>County_110</th>\n",
       "      <th>County_111</th>\n",
       "      <th>County_113</th>\n",
       "      <th>County_117</th>\n",
       "      <th>County_120</th>\n",
       "      <th>County_130</th>\n",
       "      <th>County_131</th>\n",
       "      <th>County_14</th>\n",
       "      <th>County_140</th>\n",
       "      <th>County_141</th>\n",
       "      <th>County_150</th>\n",
       "      <th>County_160</th>\n",
       "      <th>County_161</th>\n",
       "      <th>County_170</th>\n",
       "      <th>County_180</th>\n",
       "      <th>County_190</th>\n",
       "      <th>County_191</th>\n",
       "      <th>County_194</th>\n",
       "      <th>County_20</th>\n",
       "      <th>County_200</th>\n",
       "      <th>County_210</th>\n",
       "      <th>County_211</th>\n",
       "      <th>County_212</th>\n",
       "      <th>County_213</th>\n",
       "      <th>County_220</th>\n",
       "      <th>County_221</th>\n",
       "      <th>County_222</th>\n",
       "      <th>County_223</th>\n",
       "      <th>County_224</th>\n",
       "      <th>County_230</th>\n",
       "      <th>County_240</th>\n",
       "      <th>County_241</th>\n",
       "      <th>County_25</th>\n",
       "      <th>County_250</th>\n",
       "      <th>...</th>\n",
       "      <th>proc_9764.0</th>\n",
       "      <th>proc_9784.0</th>\n",
       "      <th>proc_9787.0</th>\n",
       "      <th>proc_9789.0</th>\n",
       "      <th>proc_9805.0</th>\n",
       "      <th>proc_9815.0</th>\n",
       "      <th>proc_9851.0</th>\n",
       "      <th>proc_9903.0</th>\n",
       "      <th>proc_9904.0</th>\n",
       "      <th>proc_9905.0</th>\n",
       "      <th>proc_9906.0</th>\n",
       "      <th>proc_9907.0</th>\n",
       "      <th>proc_9910.0</th>\n",
       "      <th>proc_9914.0</th>\n",
       "      <th>proc_9915.0</th>\n",
       "      <th>proc_9916.0</th>\n",
       "      <th>proc_9917.0</th>\n",
       "      <th>proc_9918.0</th>\n",
       "      <th>proc_9919.0</th>\n",
       "      <th>proc_9920.0</th>\n",
       "      <th>proc_9921.0</th>\n",
       "      <th>proc_9922.0</th>\n",
       "      <th>proc_9923.0</th>\n",
       "      <th>proc_9925.0</th>\n",
       "      <th>proc_9926.0</th>\n",
       "      <th>proc_9928.0</th>\n",
       "      <th>proc_9929.0</th>\n",
       "      <th>proc_9938.0</th>\n",
       "      <th>proc_9939.0</th>\n",
       "      <th>proc_9952.0</th>\n",
       "      <th>proc_9955.0</th>\n",
       "      <th>proc_9959.0</th>\n",
       "      <th>proc_9960.0</th>\n",
       "      <th>proc_9961.0</th>\n",
       "      <th>proc_9962.0</th>\n",
       "      <th>proc_9969.0</th>\n",
       "      <th>proc_9971.0</th>\n",
       "      <th>proc_9972.0</th>\n",
       "      <th>proc_9973.0</th>\n",
       "      <th>proc_9974.0</th>\n",
       "      <th>proc_9975.0</th>\n",
       "      <th>proc_9978.0</th>\n",
       "      <th>proc_9979.0</th>\n",
       "      <th>proc_9982.0</th>\n",
       "      <th>proc_9984.0</th>\n",
       "      <th>proc_9986.0</th>\n",
       "      <th>proc_9992.0</th>\n",
       "      <th>proc_9995.0</th>\n",
       "      <th>proc_9998.0</th>\n",
       "      <th>proc_9999.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.365759</td>\n",
       "      <td>0.233463</td>\n",
       "      <td>0.451362</td>\n",
       "      <td>0.754864</td>\n",
       "      <td>0.564202</td>\n",
       "      <td>0.762646</td>\n",
       "      <td>0.474708</td>\n",
       "      <td>0.400778</td>\n",
       "      <td>0.272374</td>\n",
       "      <td>0.330739</td>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.426901</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.730994</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.380117</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.345029</td>\n",
       "      <td>0.076023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429515</td>\n",
       "      <td>0.229075</td>\n",
       "      <td>0.451542</td>\n",
       "      <td>0.685022</td>\n",
       "      <td>0.596916</td>\n",
       "      <td>0.799559</td>\n",
       "      <td>0.398678</td>\n",
       "      <td>0.341410</td>\n",
       "      <td>0.370044</td>\n",
       "      <td>0.290749</td>\n",
       "      <td>0.063877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.496454</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.773050</td>\n",
       "      <td>0.624113</td>\n",
       "      <td>0.794326</td>\n",
       "      <td>0.460993</td>\n",
       "      <td>0.304965</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.099291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322917</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16888 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ChronicCond_Alzheimer  ChronicCond_Cancer  ChronicCond_Depression  ChronicCond_Diabetes  ChronicCond_Heartfailure  ChronicCond_IschemicHeart  ChronicCond_KidneyDisease  ChronicCond_ObstrPulmonary  ChronicCond_Osteoporasis  ChronicCond_rheumatoidarthritis  ChronicCond_stroke  County_0  County_1  County_10  County_100  County_11  County_110  County_111  County_113  County_117  County_120  County_130  County_131  County_14  County_140  County_141  County_150  County_160  County_161  \\\n",
       "0               0.365759            0.233463                0.451362              0.754864                  0.564202                   0.762646                   0.474708                    0.400778                  0.272374                         0.330739            0.105058  0.011673       0.0   0.011673    0.011673        0.0         0.0         0.0         0.0         0.0         0.0    0.015564         0.0        0.0    0.003891         0.0     0.07393    0.000000         0.0   \n",
       "1               0.426901            0.175439                0.444444              0.730994                  0.649123                   0.807018                   0.473684                    0.380117                  0.280702                         0.345029            0.076023  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.023392         0.0        0.0    0.005848         0.0     0.00000    0.000000         0.0   \n",
       "2               0.429515            0.229075                0.451542              0.685022                  0.596916                   0.799559                   0.398678                    0.341410                  0.370044                         0.290749            0.063877  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.000000         0.0   \n",
       "3               0.496454            0.191489                0.446809              0.773050                  0.624113                   0.794326                   0.460993                    0.304965                  0.326241                         0.326241            0.099291  0.000000       0.0   0.000000    0.000000        0.0         0.0         0.0         0.0         0.0         0.0    0.000000         0.0        0.0    0.000000         0.0     0.00000    0.014184         0.0   \n",
       "4               0.322917            0.156250                0.385417              0.645833                  0.645833                   0.687500                   0.395833                    0.302083                  0.291667                         0.270833            0.104167  0.000000       0.0   0.031250    0.031250        0.0         0.0         0.0         0.0         0.0         0.0    0.020833         0.0        0.0    0.000000         0.0     0.00000    0.135417         0.0   \n",
       "\n",
       "   County_170  County_180  County_190  County_191  County_194  County_20  County_200  County_210  County_211  County_212  County_213  County_220  County_221  County_222  County_223  County_224  County_230  County_240  County_241  County_25  County_250     ...       proc_9764.0  proc_9784.0  proc_9787.0  proc_9789.0  proc_9805.0  proc_9815.0  proc_9851.0  proc_9903.0  proc_9904.0  proc_9905.0  proc_9906.0  proc_9907.0  proc_9910.0  proc_9914.0  proc_9915.0  proc_9916.0  proc_9917.0  \\\n",
       "0         0.0    0.003891    0.011673         0.0         0.0   0.003891         0.0    0.000000         0.0         0.0         0.0    0.011673         0.0         0.0         0.0         0.0    0.007782    0.011673         0.0        0.0    0.054475     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "1         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.070175         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.070175         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          2.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "2         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.000000         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.156388         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "3         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.014184         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "4         0.0    0.000000    0.000000         0.0         0.0   0.000000         0.0    0.010417         0.0         0.0         0.0    0.000000         0.0         0.0         0.0         0.0    0.000000    0.000000         0.0        0.0    0.000000     ...               0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   proc_9918.0  proc_9919.0  proc_9920.0  proc_9921.0  proc_9922.0  proc_9923.0  proc_9925.0  proc_9926.0  proc_9928.0  proc_9929.0  proc_9938.0  proc_9939.0  proc_9952.0  proc_9955.0  proc_9959.0  proc_9960.0  proc_9961.0  proc_9962.0  proc_9969.0  proc_9971.0  proc_9972.0  proc_9973.0  proc_9974.0  proc_9975.0  proc_9978.0  proc_9979.0  proc_9982.0  proc_9984.0  proc_9986.0  proc_9992.0  proc_9995.0  proc_9998.0  proc_9999.0  \n",
       "0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "1          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "2          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "3          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "4          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0  \n",
       "\n",
       "[5 rows x 16888 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.92\n",
      "Test accuracy score: 0.925\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      1226\n",
      "           1       0.71      0.33      0.45       127\n",
      "\n",
      "    accuracy                           0.92      1353\n",
      "   macro avg       0.82      0.66      0.71      1353\n",
      "weighted avg       0.91      0.92      0.91      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest_general(train_final_data, 'PotentialFraud', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.854\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91      1226\n",
      "           1       0.37      0.79      0.50       127\n",
      "\n",
      "    accuracy                           0.85      1353\n",
      "   macro avg       0.67      0.82      0.71      1353\n",
      "weighted avg       0.92      0.85      0.88      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
      "                       n_jobs=None, oob_score=True, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Julia/miniconda3/envs/advanced-pip-example/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.877\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      1226\n",
      "           1       0.39      0.56      0.46       127\n",
      "\n",
      "    accuracy                           0.88      1353\n",
      "   macro avg       0.67      0.73      0.70      1353\n",
      "weighted avg       0.90      0.88      0.89      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=600,\n",
      "                       n_jobs=None, oob_score=True, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.634\n",
      "\n",
      " Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.60      0.75      1226\n",
      "           1       0.20      0.97      0.33       127\n",
      "\n",
      "    accuracy                           0.63      1353\n",
      "   macro avg       0.60      0.78      0.54      1353\n",
      "weighted avg       0.92      0.63      0.71      1353\n",
      "\n",
      "Best Parameters RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
      "                       n_jobs=None, oob_score=True, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "random_forest_tuned(train_final_data, 'PotentialFraud', 'NearMiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Author: Kian Ho <hui.kian.ho@gmail.com>\n",
    "#         Gilles Louppe <g.louppe@gmail.com>\n",
    "#         Andreas Mueller <amueller@ais.uni-bonn.de>\n",
    "#\n",
    "# License: BSD 3 Clause\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "# Generate a binary classification dataset.\n",
    "X = train_final_data.drop(\"PotentialFraud\", axis=1)\n",
    "y = train_final_data[\"PotentialFraud\"]\n",
    "    \n",
    "# NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
    "# support for parallelized ensembles but is necessary for tracking the OOB\n",
    "# error trajectory during training.\n",
    "ensemble_clfs = [\n",
    "    (\"RandomForestClassifier, max_features='sqrt'\",\n",
    "        RandomForestClassifier(n_estimators=200,\n",
    "                               warm_start=True, max_features=\"sqrt\",\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features='log2'\",\n",
    "        RandomForestClassifier(n_estimators=200,\n",
    "                               warm_start=True, max_features='log2',\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features=None\",\n",
    "        RandomForestClassifier(n_estimators=200,\n",
    "                               warm_start=True, max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 50\n",
    "max_estimators = 400\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(min_estimators, max_estimators + 1):\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))\n",
    "\n",
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "sns.set()\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
